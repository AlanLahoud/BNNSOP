{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import data_generator\n",
    "import params_newsvendor as params\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from model import VariationalLayer, VariationalNet, StandardNet, VariationalNet2\n",
    "\n",
    "from train import TrainDecoupled\n",
    "\n",
    "from train_normflow import TrainFlowDecoupled\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 2\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters (change if necessary)\n",
    "N = 8000 # Total data size\n",
    "N_train = 5000 # Training data size\n",
    "N_SAMPLES = 16 # Sampling size while training\n",
    "BATCH_SIZE_LOADER = 32 # Standard batch size\n",
    "EPOCHS = 80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "N_valid = N - N_train\n",
    "X, Y_original = data_generator.data_4to8(N_train, noise_level=nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_multi.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output normalization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Y_original)\n",
    "tmean = torch.tensor(scaler.mean_)\n",
    "tstd = torch.tensor(scaler.scale_)\n",
    "joblib.dump(scaler, 'scaler_multi.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(yy):\n",
    "    return yy*tstd + tmean\n",
    "\n",
    "Y = scaler.transform(Y_original).copy()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_generator.ArtificialDataset(X, Y)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "X_val, Y_val_original = data_generator.data_4to8(N_valid, noise_level=nl)\n",
    "Y_val = scaler.transform(Y_val_original).copy()\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_original = torch.tensor(Y_val_original, dtype=torch.float32)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "data_valid = data_generator.ArtificialDataset(X_val, Y_val)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    data_valid, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t, n_samples):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "            \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items  \n",
    "        self.n_samples = n_samples\n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident_samples = torch.eye(n_items*n_samples)\n",
    "        ident3 = torch.eye(n_items + 2*n_items*n_samples)\n",
    "        zeros_matrix = torch.zeros((n_items*n_samples, n_items*n_samples))\n",
    "        zeros_array = torch.zeros(n_items*n_samples)\n",
    "        ones_array = torch.ones(n_items*n_samples)\n",
    "             \n",
    "        self.Q = torch.diag(\n",
    "            torch.hstack(\n",
    "                (\n",
    "                    params_t['q'], \n",
    "                    (1/n_samples)*params_t['qs'].repeat_interleave(n_samples), \n",
    "                    (1/n_samples)*params_t['qw'].repeat_interleave(n_samples)\n",
    "                )\n",
    "            )).to(dev)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.hstack(\n",
    "                                (\n",
    "                                    params_t['c'], \n",
    "                                    (1/n_samples)*params_t['cs'].repeat_interleave(n_samples), \n",
    "                                    (1/n_samples)*params_t['cw'].repeat_interleave(n_samples)\n",
    "                                )).to(dev)\n",
    "             \n",
    "            \n",
    "        shortage_ineq = torch.hstack(\n",
    "            (\n",
    "                -ident.repeat_interleave(n_samples, 0), \n",
    "                -ident_samples, \n",
    "                zeros_matrix\n",
    "            )\n",
    "        )  \n",
    "        \n",
    "        \n",
    "        excess_ineq = torch.hstack(\n",
    "            (\n",
    "                ident.repeat_interleave(n_samples, 0), \n",
    "                zeros_matrix, \n",
    "                -ident_samples\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        price_ineq = torch.hstack(\n",
    "            (\n",
    "                params_t['pr'], \n",
    "                zeros_array, \n",
    "                zeros_array\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        \n",
    "        self.ineqs = torch.vstack(\n",
    "            (\n",
    "                shortage_ineq, \n",
    "                excess_ineq, \n",
    "                price_ineq, \n",
    "                positive_ineq\n",
    "            )\n",
    "        ).to(dev)\n",
    " \n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        \n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items*n_samples), \n",
    "                                          torch.zeros(n_items*n_samples))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, n_samples_items = y.size()\n",
    "                \n",
    "        assert self.n_samples*self.n_items == n_samples_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(batch_size, Q.size(0), Q.size(1))\n",
    "        \n",
    "        lin = self.lin\n",
    "        lin = lin.expand(batch_size, lin.size(0))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(batch_size, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            batch_size, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_item = lambda Z, Y : params_t['q'].to(dev)*Z.to(dev)**2 \\\n",
    "                            + params_t['qs'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)))**2 \\\n",
    "                            + params_t['qw'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev)))**2 \\\n",
    "                            + params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "\n",
    "def reshape_outcomes(y_pred):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    batch_size = y_pred.shape[1]\n",
    "    n_items = y_pred.shape[2]\n",
    "\n",
    "    y_pred = y_pred.permute((1, 2, 0)).reshape((batch_size, n_samples*n_items))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred = reshape_outcomes(y_pred)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f\n",
    "\n",
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ann = StandardNet(input_size, output_size, 0).to(dev)\n",
    "h_bnn = VariationalNet(N_SAMPLES, input_size, output_size, 1.0).to(dev)\n",
    "\n",
    "opt_h_ann = torch.optim.Adam(h_ann.parameters(), lr=0.0010)\n",
    "opt_h_bnn = torch.optim.Adam(h_bnn.parameters(), lr=0.0015)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 0.496 valid 0.337\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.5 valid 0.34\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 0.306 valid 0.283\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.31 valid 0.28\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.26 valid 0.261\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.26 valid 0.26\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.237 valid 0.247\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.24 valid 0.25\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.224 valid 0.236\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.22 valid 0.24\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.215 valid 0.226\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.21 valid 0.23\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.206 valid 0.217\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.21 valid 0.22\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.198 valid 0.208\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.2 valid 0.21\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.189 valid 0.202\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.19 valid 0.2\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.18 valid 0.194\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.18 valid 0.19\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.17 valid 0.184\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.17 valid 0.18\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.16 valid 0.175\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.16 valid 0.18\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.149 valid 0.168\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.15 valid 0.17\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.14 valid 0.162\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.14 valid 0.16\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.132 valid 0.155\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.13 valid 0.16\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.125 valid 0.15\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.13 valid 0.15\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.12 valid 0.146\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.15\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.116 valid 0.144\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.14\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.112 valid 0.139\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.14\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.108 valid 0.137\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.14\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.105 valid 0.135\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.101 valid 0.131\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.098 valid 0.128\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.095 valid 0.123\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.12\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.092 valid 0.12\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.089 valid 0.117\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.086 valid 0.114\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.11\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.084 valid 0.111\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.082 valid 0.11\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.081 valid 0.109\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.079 valid 0.107\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.078 valid 0.106\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.077 valid 0.105\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.1\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.075 valid 0.104\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.1\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.075 valid 0.102\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.073 valid 0.102\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.073 valid 0.101\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.072 valid 0.1\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.071 valid 0.099\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.071 valid 0.099\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.07 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.07 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.069 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.068 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.068 valid 0.097\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.067 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.067 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.066 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.066 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.066 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.065 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.065 valid 0.095\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.065 valid 0.095\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.064 valid 0.095\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.065 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.064 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 57------------------\n",
      "DATA LOSS \t train 0.064 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 58------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.063 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.062 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.062 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.061 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.06 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.06 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.059 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.059 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.059 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.058 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.058 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.057 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.057 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.057 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.056 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.056 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.056 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.055 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.055 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.055 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.055 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.055 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.054 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.053 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.053 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.053 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.053 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.053 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.053 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.053 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.052 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.052 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.052 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.052 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.052 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.052 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.051 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.051 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.052 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.051 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.052 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.051 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.051 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.051 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.051 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.051 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.05 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.05 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.05 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 113------------------\n",
      "DATA LOSS \t train 0.05 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.05 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 115------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.05 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.05 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.05 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.049 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.049 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.049 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n"
     ]
    }
   ],
   "source": [
    "train_ANN = TrainDecoupled(\n",
    "                    bnn = False,\n",
    "                    model=h_ann,\n",
    "                    opt=opt_h_ann,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=0.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader\n",
    "                )\n",
    "\n",
    "train_ANN.train(EPOCHS=120)\n",
    "model_ann = train_ANN.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 0.893 valid 0.628\n",
      "KL LOSS \t train 3.31 valid 3.31\n",
      "ELBO LOSS \t train 4.21 valid 3.94\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 0.546 valid 0.481\n",
      "KL LOSS \t train 3.32 valid 3.33\n",
      "ELBO LOSS \t train 3.87 valid 3.81\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.435 valid 0.404\n",
      "KL LOSS \t train 3.33 valid 3.35\n",
      "ELBO LOSS \t train 3.77 valid 3.75\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.372 valid 0.357\n",
      "KL LOSS \t train 3.35 valid 3.36\n",
      "ELBO LOSS \t train 3.72 valid 3.72\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.333 valid 0.326\n",
      "KL LOSS \t train 3.37 valid 3.38\n",
      "ELBO LOSS \t train 3.7 valid 3.7\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.304 valid 0.303\n",
      "KL LOSS \t train 3.39 valid 3.4\n",
      "ELBO LOSS \t train 3.69 valid 3.7\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.283 valid 0.285\n",
      "KL LOSS \t train 3.4 valid 3.41\n",
      "ELBO LOSS \t train 3.69 valid 3.7\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.268 valid 0.273\n",
      "KL LOSS \t train 3.42 valid 3.43\n",
      "ELBO LOSS \t train 3.69 valid 3.7\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.256 valid 0.262\n",
      "KL LOSS \t train 3.44 valid 3.44\n",
      "ELBO LOSS \t train 3.69 valid 3.7\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.246 valid 0.251\n",
      "KL LOSS \t train 3.45 valid 3.45\n",
      "ELBO LOSS \t train 3.7 valid 3.71\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.236 valid 0.243\n",
      "KL LOSS \t train 3.47 valid 3.48\n",
      "ELBO LOSS \t train 3.7 valid 3.72\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.228 valid 0.236\n",
      "KL LOSS \t train 3.48 valid 3.49\n",
      "ELBO LOSS \t train 3.71 valid 3.72\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.221 valid 0.23\n",
      "KL LOSS \t train 3.5 valid 3.5\n",
      "ELBO LOSS \t train 3.72 valid 3.73\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.214 valid 0.223\n",
      "KL LOSS \t train 3.51 valid 3.52\n",
      "ELBO LOSS \t train 3.72 valid 3.74\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.206 valid 0.216\n",
      "KL LOSS \t train 3.53 valid 3.53\n",
      "ELBO LOSS \t train 3.73 valid 3.75\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.198 valid 0.208\n",
      "KL LOSS \t train 3.54 valid 3.55\n",
      "ELBO LOSS \t train 3.74 valid 3.76\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.19 valid 0.2\n",
      "KL LOSS \t train 3.55 valid 3.56\n",
      "ELBO LOSS \t train 3.74 valid 3.76\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.182 valid 0.193\n",
      "KL LOSS \t train 3.57 valid 3.58\n",
      "ELBO LOSS \t train 3.75 valid 3.77\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.175 valid 0.187\n",
      "KL LOSS \t train 3.58 valid 3.59\n",
      "ELBO LOSS \t train 3.76 valid 3.78\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.168 valid 0.183\n",
      "KL LOSS \t train 3.6 valid 3.61\n",
      "ELBO LOSS \t train 3.77 valid 3.79\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.163 valid 0.177\n",
      "KL LOSS \t train 3.61 valid 3.62\n",
      "ELBO LOSS \t train 3.78 valid 3.8\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.157 valid 0.174\n",
      "KL LOSS \t train 3.63 valid 3.64\n",
      "ELBO LOSS \t train 3.79 valid 3.81\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.153 valid 0.17\n",
      "KL LOSS \t train 3.64 valid 3.65\n",
      "ELBO LOSS \t train 3.8 valid 3.82\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.149 valid 0.169\n",
      "KL LOSS \t train 3.66 valid 3.66\n",
      "ELBO LOSS \t train 3.81 valid 3.83\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.145 valid 0.165\n",
      "KL LOSS \t train 3.67 valid 3.68\n",
      "ELBO LOSS \t train 3.82 valid 3.84\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.142 valid 0.162\n",
      "KL LOSS \t train 3.68 valid 3.7\n",
      "ELBO LOSS \t train 3.83 valid 3.86\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.138 valid 0.159\n",
      "KL LOSS \t train 3.7 valid 3.71\n",
      "ELBO LOSS \t train 3.84 valid 3.86\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.136 valid 0.157\n",
      "KL LOSS \t train 3.71 valid 3.72\n",
      "ELBO LOSS \t train 3.85 valid 3.87\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.132 valid 0.156\n",
      "KL LOSS \t train 3.72 valid 3.73\n",
      "ELBO LOSS \t train 3.85 valid 3.89\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.13 valid 0.153\n",
      "KL LOSS \t train 3.73 valid 3.74\n",
      "ELBO LOSS \t train 3.86 valid 3.89\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.127 valid 0.152\n",
      "KL LOSS \t train 3.75 valid 3.75\n",
      "ELBO LOSS \t train 3.87 valid 3.9\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.125 valid 0.149\n",
      "KL LOSS \t train 3.76 valid 3.77\n",
      "ELBO LOSS \t train 3.88 valid 3.92\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.123 valid 0.148\n",
      "KL LOSS \t train 3.77 valid 3.78\n",
      "ELBO LOSS \t train 3.89 valid 3.93\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.121 valid 0.146\n",
      "KL LOSS \t train 3.78 valid 3.79\n",
      "ELBO LOSS \t train 3.9 valid 3.94\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.119 valid 0.144\n",
      "KL LOSS \t train 3.79 valid 3.8\n",
      "ELBO LOSS \t train 3.91 valid 3.95\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.117 valid 0.146\n",
      "KL LOSS \t train 3.8 valid 3.81\n",
      "ELBO LOSS \t train 3.92 valid 3.95\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.115 valid 0.143\n",
      "KL LOSS \t train 3.82 valid 3.83\n",
      "ELBO LOSS \t train 3.93 valid 3.97\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.114 valid 0.142\n",
      "KL LOSS \t train 3.83 valid 3.84\n",
      "ELBO LOSS \t train 3.94 valid 3.98\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.112 valid 0.141\n",
      "KL LOSS \t train 3.84 valid 3.84\n",
      "ELBO LOSS \t train 3.95 valid 3.98\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.11 valid 0.139\n",
      "KL LOSS \t train 3.85 valid 3.85\n",
      "ELBO LOSS \t train 3.96 valid 3.99\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.11 valid 0.138\n",
      "KL LOSS \t train 3.86 valid 3.86\n",
      "ELBO LOSS \t train 3.97 valid 4.0\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.108 valid 0.136\n",
      "KL LOSS \t train 3.87 valid 3.87\n",
      "ELBO LOSS \t train 3.97 valid 4.01\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.107 valid 0.135\n",
      "KL LOSS \t train 3.88 valid 3.88\n",
      "ELBO LOSS \t train 3.98 valid 4.01\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.106 valid 0.134\n",
      "KL LOSS \t train 3.89 valid 3.89\n",
      "ELBO LOSS \t train 3.99 valid 4.03\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.104 valid 0.133\n",
      "KL LOSS \t train 3.89 valid 3.9\n",
      "ELBO LOSS \t train 4.0 valid 4.03\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.104 valid 0.133\n",
      "KL LOSS \t train 3.9 valid 3.9\n",
      "ELBO LOSS \t train 4.01 valid 4.04\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.102 valid 0.132\n",
      "KL LOSS \t train 3.91 valid 3.91\n",
      "ELBO LOSS \t train 4.01 valid 4.05\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.101 valid 0.131\n",
      "KL LOSS \t train 3.92 valid 3.93\n",
      "ELBO LOSS \t train 4.02 valid 4.06\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.1 valid 0.13\n",
      "KL LOSS \t train 3.93 valid 3.93\n",
      "ELBO LOSS \t train 4.03 valid 4.06\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.099 valid 0.129\n",
      "KL LOSS \t train 3.94 valid 3.94\n",
      "ELBO LOSS \t train 4.04 valid 4.07\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.098 valid 0.129\n",
      "KL LOSS \t train 3.95 valid 3.95\n",
      "ELBO LOSS \t train 4.04 valid 4.08\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.097 valid 0.127\n",
      "KL LOSS \t train 3.95 valid 3.96\n",
      "ELBO LOSS \t train 4.05 valid 4.09\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.096 valid 0.127\n",
      "KL LOSS \t train 3.96 valid 3.97\n",
      "ELBO LOSS \t train 4.06 valid 4.09\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.096 valid 0.126\n",
      "KL LOSS \t train 3.97 valid 3.97\n",
      "ELBO LOSS \t train 4.07 valid 4.1\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.095 valid 0.126\n",
      "KL LOSS \t train 3.98 valid 3.98\n",
      "ELBO LOSS \t train 4.07 valid 4.11\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.094 valid 0.126\n",
      "KL LOSS \t train 3.99 valid 3.99\n",
      "ELBO LOSS \t train 4.08 valid 4.11\n",
      "------------------EPOCH 57------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.093 valid 0.126\n",
      "KL LOSS \t train 3.99 valid 3.99\n",
      "ELBO LOSS \t train 4.09 valid 4.12\n",
      "------------------EPOCH 58------------------\n",
      "DATA LOSS \t train 0.093 valid 0.125\n",
      "KL LOSS \t train 4.0 valid 4.0\n",
      "ELBO LOSS \t train 4.09 valid 4.13\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.092 valid 0.125\n",
      "KL LOSS \t train 4.01 valid 4.01\n",
      "ELBO LOSS \t train 4.1 valid 4.14\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.092 valid 0.125\n",
      "KL LOSS \t train 4.01 valid 4.02\n",
      "ELBO LOSS \t train 4.11 valid 4.14\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.091 valid 0.125\n",
      "KL LOSS \t train 4.02 valid 4.02\n",
      "ELBO LOSS \t train 4.11 valid 4.15\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.09 valid 0.124\n",
      "KL LOSS \t train 4.03 valid 4.03\n",
      "ELBO LOSS \t train 4.12 valid 4.16\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.089 valid 0.123\n",
      "KL LOSS \t train 4.03 valid 4.04\n",
      "ELBO LOSS \t train 4.12 valid 4.16\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.089 valid 0.122\n",
      "KL LOSS \t train 4.04 valid 4.05\n",
      "ELBO LOSS \t train 4.13 valid 4.17\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.089 valid 0.123\n",
      "KL LOSS \t train 4.04 valid 4.05\n",
      "ELBO LOSS \t train 4.13 valid 4.17\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.088 valid 0.121\n",
      "KL LOSS \t train 4.05 valid 4.05\n",
      "ELBO LOSS \t train 4.14 valid 4.18\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.087 valid 0.121\n",
      "KL LOSS \t train 4.06 valid 4.06\n",
      "ELBO LOSS \t train 4.14 valid 4.18\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.087 valid 0.121\n",
      "KL LOSS \t train 4.06 valid 4.06\n",
      "ELBO LOSS \t train 4.15 valid 4.18\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.087 valid 0.121\n",
      "KL LOSS \t train 4.07 valid 4.07\n",
      "ELBO LOSS \t train 4.15 valid 4.19\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.086 valid 0.121\n",
      "KL LOSS \t train 4.07 valid 4.08\n",
      "ELBO LOSS \t train 4.16 valid 4.2\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.086 valid 0.12\n",
      "KL LOSS \t train 4.08 valid 4.08\n",
      "ELBO LOSS \t train 4.16 valid 4.2\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.085 valid 0.119\n",
      "KL LOSS \t train 4.08 valid 4.08\n",
      "ELBO LOSS \t train 4.17 valid 4.2\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.085 valid 0.12\n",
      "KL LOSS \t train 4.09 valid 4.09\n",
      "ELBO LOSS \t train 4.17 valid 4.21\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.085 valid 0.12\n",
      "KL LOSS \t train 4.09 valid 4.09\n",
      "ELBO LOSS \t train 4.17 valid 4.21\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.084 valid 0.12\n",
      "KL LOSS \t train 4.09 valid 4.1\n",
      "ELBO LOSS \t train 4.18 valid 4.22\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.084 valid 0.119\n",
      "KL LOSS \t train 4.1 valid 4.1\n",
      "ELBO LOSS \t train 4.18 valid 4.22\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.083 valid 0.119\n",
      "KL LOSS \t train 4.1 valid 4.1\n",
      "ELBO LOSS \t train 4.19 valid 4.22\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.083 valid 0.119\n",
      "KL LOSS \t train 4.11 valid 4.11\n",
      "ELBO LOSS \t train 4.19 valid 4.23\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.083 valid 0.119\n",
      "KL LOSS \t train 4.11 valid 4.11\n",
      "ELBO LOSS \t train 4.19 valid 4.23\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.082 valid 0.118\n",
      "KL LOSS \t train 4.11 valid 4.12\n",
      "ELBO LOSS \t train 4.2 valid 4.24\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.082 valid 0.118\n",
      "KL LOSS \t train 4.12 valid 4.12\n",
      "ELBO LOSS \t train 4.2 valid 4.24\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.082 valid 0.119\n",
      "KL LOSS \t train 4.12 valid 4.12\n",
      "ELBO LOSS \t train 4.2 valid 4.24\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.081 valid 0.117\n",
      "KL LOSS \t train 4.12 valid 4.12\n",
      "ELBO LOSS \t train 4.2 valid 4.24\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.081 valid 0.117\n",
      "KL LOSS \t train 4.13 valid 4.13\n",
      "ELBO LOSS \t train 4.21 valid 4.24\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.081 valid 0.116\n",
      "KL LOSS \t train 4.13 valid 4.13\n",
      "ELBO LOSS \t train 4.21 valid 4.25\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.08 valid 0.116\n",
      "KL LOSS \t train 4.13 valid 4.13\n",
      "ELBO LOSS \t train 4.21 valid 4.25\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.08 valid 0.116\n",
      "KL LOSS \t train 4.14 valid 4.14\n",
      "ELBO LOSS \t train 4.22 valid 4.25\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.08 valid 0.116\n",
      "KL LOSS \t train 4.14 valid 4.14\n",
      "ELBO LOSS \t train 4.22 valid 4.26\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.08 valid 0.116\n",
      "KL LOSS \t train 4.14 valid 4.14\n",
      "ELBO LOSS \t train 4.22 valid 4.26\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.079 valid 0.117\n",
      "KL LOSS \t train 4.14 valid 4.15\n",
      "ELBO LOSS \t train 4.22 valid 4.26\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.079 valid 0.116\n",
      "KL LOSS \t train 4.15 valid 4.15\n",
      "ELBO LOSS \t train 4.22 valid 4.26\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.079 valid 0.116\n",
      "KL LOSS \t train 4.15 valid 4.15\n",
      "ELBO LOSS \t train 4.23 valid 4.26\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.079 valid 0.115\n",
      "KL LOSS \t train 4.15 valid 4.15\n",
      "ELBO LOSS \t train 4.23 valid 4.27\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.078 valid 0.115\n",
      "KL LOSS \t train 4.15 valid 4.15\n",
      "ELBO LOSS \t train 4.23 valid 4.27\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.078 valid 0.115\n",
      "KL LOSS \t train 4.15 valid 4.15\n",
      "ELBO LOSS \t train 4.23 valid 4.27\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.078 valid 0.115\n",
      "KL LOSS \t train 4.16 valid 4.16\n",
      "ELBO LOSS \t train 4.23 valid 4.27\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.078 valid 0.116\n",
      "KL LOSS \t train 4.16 valid 4.16\n",
      "ELBO LOSS \t train 4.24 valid 4.28\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.077 valid 0.114\n",
      "KL LOSS \t train 4.16 valid 4.16\n",
      "ELBO LOSS \t train 4.24 valid 4.28\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.077 valid 0.115\n",
      "KL LOSS \t train 4.16 valid 4.16\n",
      "ELBO LOSS \t train 4.24 valid 4.27\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.077 valid 0.115\n",
      "KL LOSS \t train 4.16 valid 4.16\n",
      "ELBO LOSS \t train 4.24 valid 4.28\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.077 valid 0.114\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.24 valid 4.28\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.077 valid 0.115\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.24 valid 4.28\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.077 valid 0.115\n",
      "KL LOSS \t train 4.17 valid 4.16\n",
      "ELBO LOSS \t train 4.25 valid 4.28\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.077 valid 0.114\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.076 valid 0.114\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.25 valid 4.28\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.076 valid 0.114\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.25 valid 4.28\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.076 valid 0.114\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.076 valid 0.114\n",
      "KL LOSS \t train 4.17 valid 4.17\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.075 valid 0.114\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.076 valid 0.115\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.075 valid 0.114\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.25 valid 4.3\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.075 valid 0.114\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 113------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.075 valid 0.114\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.25 valid 4.29\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.075 valid 0.113\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.25 valid 4.3\n",
      "------------------EPOCH 115------------------\n",
      "DATA LOSS \t train 0.075 valid 0.113\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.075 valid 0.113\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.074 valid 0.113\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.074 valid 0.113\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.074 valid 0.113\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.074 valid 0.112\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 121------------------\n",
      "DATA LOSS \t train 0.074 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 122------------------\n",
      "DATA LOSS \t train 0.074 valid 0.112\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 123------------------\n",
      "DATA LOSS \t train 0.074 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 124------------------\n",
      "DATA LOSS \t train 0.074 valid 0.112\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 125------------------\n",
      "DATA LOSS \t train 0.074 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 126------------------\n",
      "DATA LOSS \t train 0.073 valid 0.114\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.31\n",
      "------------------EPOCH 127------------------\n",
      "DATA LOSS \t train 0.073 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 128------------------\n",
      "DATA LOSS \t train 0.073 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 129------------------\n",
      "DATA LOSS \t train 0.073 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 130------------------\n",
      "DATA LOSS \t train 0.073 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 131------------------\n",
      "DATA LOSS \t train 0.073 valid 0.111\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 132------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 133------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 134------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.19 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 135------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 136------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 137------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 138------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 139------------------\n",
      "DATA LOSS \t train 0.072 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 140------------------\n",
      "DATA LOSS \t train 0.072 valid 0.109\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 141------------------\n",
      "DATA LOSS \t train 0.071 valid 0.11\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 142------------------\n",
      "DATA LOSS \t train 0.072 valid 0.108\n",
      "KL LOSS \t train 4.18 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 143------------------\n",
      "DATA LOSS \t train 0.071 valid 0.108\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 144------------------\n",
      "DATA LOSS \t train 0.071 valid 0.108\n",
      "KL LOSS \t train 4.19 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 145------------------\n",
      "DATA LOSS \t train 0.071 valid 0.108\n",
      "KL LOSS \t train 4.19 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 146------------------\n",
      "DATA LOSS \t train 0.071 valid 0.108\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.3\n",
      "------------------EPOCH 147------------------\n",
      "DATA LOSS \t train 0.071 valid 0.108\n",
      "KL LOSS \t train 4.19 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 148------------------\n",
      "DATA LOSS \t train 0.07 valid 0.108\n",
      "KL LOSS \t train 4.18 valid 4.19\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 149------------------\n",
      "DATA LOSS \t train 0.071 valid 0.107\n",
      "KL LOSS \t train 4.19 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n",
      "------------------EPOCH 150------------------\n",
      "DATA LOSS \t train 0.071 valid 0.107\n",
      "KL LOSS \t train 4.19 valid 4.18\n",
      "ELBO LOSS \t train 4.26 valid 4.29\n"
     ]
    }
   ],
   "source": [
    "train_BNN = TrainDecoupled(\n",
    "                    bnn = True,\n",
    "                    model=h_bnn,\n",
    "                    opt=opt_h_bnn,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=1.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader\n",
    "                )\n",
    "\n",
    "train_BNN.train(EPOCHS=150)\n",
    "model_bnn = train_BNN.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 5.79808, val loss: 5.70293\n",
      "step: 50, train loss: 4.07737, val loss: 4.09179\n",
      "step: 100, train loss: 3.27692, val loss: 3.3427\n",
      "step: 150, train loss: 2.8138, val loss: 2.90492\n",
      "step: 200, train loss: 2.49744, val loss: 2.60201\n",
      "step: 250, train loss: 2.2451, val loss: 2.3652\n",
      "step: 300, train loss: 2.04975, val loss: 2.17902\n",
      "step: 350, train loss: 1.91018, val loss: 2.04559\n",
      "step: 400, train loss: 1.79934, val loss: 1.95387\n",
      "step: 450, train loss: 1.74476, val loss: 1.90066\n",
      "step: 500, train loss: 1.67851, val loss: 1.85055\n",
      "step: 550, train loss: 1.62338, val loss: 1.81214\n",
      "step: 600, train loss: 1.59577, val loss: 1.7853\n",
      "step: 650, train loss: 1.57174, val loss: 1.77225\n",
      "step: 700, train loss: 1.5458, val loss: 1.75836\n",
      "step: 750, train loss: 1.52965, val loss: 1.74595\n",
      "step: 800, train loss: 1.5259, val loss: 1.73437\n",
      "step: 850, train loss: 1.51237, val loss: 1.73382\n",
      "step: 900, train loss: 1.48909, val loss: 1.7271\n",
      "step: 950, train loss: 1.51302, val loss: 1.72441\n",
      "step: 1000, train loss: 1.46229, val loss: 1.714\n",
      "step: 1050, train loss: 1.47201, val loss: 1.70457\n",
      "step: 1100, train loss: 1.44299, val loss: 1.70512\n",
      "step: 1150, train loss: 1.45512, val loss: 1.7237\n",
      "step: 1200, train loss: 1.44473, val loss: 1.70115\n",
      "step: 1250, train loss: 1.4533, val loss: 1.69761\n",
      "step: 1300, train loss: 1.40472, val loss: 1.69884\n",
      "step: 1350, train loss: 1.45788, val loss: 1.70698\n",
      "step: 1400, train loss: 1.42154, val loss: 1.69487\n",
      "step: 1450, train loss: 1.40618, val loss: 1.69628\n",
      "step: 1500, train loss: 1.4362, val loss: 1.71368\n",
      "step: 1550, train loss: 1.39572, val loss: 1.69919\n",
      "step: 1600, train loss: 1.39845, val loss: 1.69286\n",
      "step: 1650, train loss: 1.39159, val loss: 1.69038\n",
      "step: 1700, train loss: 1.37877, val loss: 1.7086\n",
      "step: 1750, train loss: 1.37846, val loss: 1.69038\n",
      "step: 1800, train loss: 1.39449, val loss: 1.69174\n",
      "step: 1850, train loss: 1.3731, val loss: 1.70197\n",
      "step: 1900, train loss: 1.37434, val loss: 1.69616\n",
      "step: 1950, train loss: 1.39167, val loss: 1.68936\n",
      "step: 2000, train loss: 1.38712, val loss: 1.74249\n",
      "step: 2050, train loss: 1.42197, val loss: 1.70412\n",
      "step: 2100, train loss: 1.35344, val loss: 1.68793\n",
      "step: 2150, train loss: 1.37104, val loss: 1.68409\n",
      "step: 2200, train loss: 1.38087, val loss: 1.69169\n",
      "step: 2250, train loss: 1.36814, val loss: 1.6875\n",
      "step: 2300, train loss: 1.33954, val loss: 1.69814\n",
      "step: 2350, train loss: 1.35277, val loss: 1.68585\n",
      "step: 2400, train loss: 1.37948, val loss: 1.71947\n",
      "step: 2450, train loss: 1.36424, val loss: 1.6919\n"
     ]
    }
   ],
   "source": [
    "train_flow = TrainFlowDecoupled(steps=2500, input_size=4, output_size=8)\n",
    "model_flow = train_flow.train(X, Y, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t, _ = params.get_params(n_items, seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagating predictions to Newsvendor Problem\n",
    "M = 16\n",
    "\n",
    "Y_pred_ANN = train_ANN.model(X_val).unsqueeze(0)\n",
    "Y_pred_ANN = inverse_transform(Y_pred_ANN)\n",
    "\n",
    "train_BNN.model.update_n_samples(n_samples=M)\n",
    "Y_pred_BNN = train_BNN.model.forward_dist(X_val)\n",
    "Y_pred_BNN = inverse_transform(Y_pred_BNN)\n",
    "#M = Y_pred_BNN.shape[0]\n",
    "\n",
    "N = X_val.shape[0]\n",
    "Y_pred_flow = torch.zeros((M, N, n_items))\n",
    "for i in range(0, N):\n",
    "    Y_pred_flow[:,i,:] = model_flow.condition(X_val[i]).sample(torch.Size([M,])).squeeze()\n",
    "Y_pred_flow = inverse_transform(Y_pred_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8488, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3078, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3018, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "print(mse_loss(Y_pred_ANN.mean(axis=0), Y_val_original))\n",
    "print(mse_loss(Y_pred_BNN.mean(axis=0), Y_val_original))\n",
    "print(mse_loss(Y_pred_flow.mean(axis=0), Y_val_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t['B'] = torch.tensor([20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t['cs'] = 5*params_t['cs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t, 1)\n",
    "newsvendor_solve_kkt_M = SolveNewsvendorWithKKT(params_t, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/qpth/qp.py:83: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2910.)\n",
      "  e, _ = torch.eig(Q[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(219.7427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(431.2005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(663.9111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(907.9290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(1187.4045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(1443.8464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(1676.6126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(1896.4263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(2134.6857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(2359.6012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(2640.7107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(2843.3051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(3059.9432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(3324.4056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(3535.1858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(3771.9072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(4011.5202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(4247.0194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(4498.7382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(4749.2227, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(5002.9237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(5279.1789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(5547.4822, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(5879.4281, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(6212.0616, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(6523.4786, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(6849.3037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(7154.5701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(7470.4476, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(7837.6122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(8102.9919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(8413.3892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(8705.1496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(9023.7644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(9322.6970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(9574.5087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(9834.3470, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(10118.3152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(10382.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(10683.3718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(11003.9517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(11314.7809, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(11599.8149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(11865.0949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(12144.9150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(12435.4467, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(12698.5421, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(12997.5621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(13329.1297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(13573.5867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(13837.6482, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(14095.6501, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(14393.9858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(14647.5202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(14993.7242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(15363.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(15625.6208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "57 tensor(15895.6207, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "58 tensor(16151.4875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "59 tensor(16407.2681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "60 tensor(16762.1772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "61 tensor(17082.4094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "62 tensor(17366.2216, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "63 tensor(17666.9916, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "64 tensor(17962.3654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "65 tensor(18297.6671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "66 tensor(18563.2099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "67 tensor(18865.1808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "68 tensor(19128.6352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "69 tensor(19449.6103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "70 tensor(19737.8862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "71 tensor(20020.0236, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "72 tensor(20301.6682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "73 tensor(20575.7338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "74 tensor(20901.4311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "75 tensor(21191.8532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "76 tensor(21478.0183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "77 tensor(21803.0419, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "78 tensor(22059.9563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "79 tensor(22395.5183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "80 tensor(22666.4576, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "81 tensor(22968.2577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "82 tensor(23235.5481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "83 tensor(23564.3586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "84 tensor(23815.6590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "85 tensor(24120.4910, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "86 tensor(24368.4424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "87 tensor(24676.7996, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "88 tensor(24920.5961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "89 tensor(25193.7501, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "90 tensor(25438.3903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "91 tensor(25696.7975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "92 tensor(25967.6026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "93 tensor(26181.9164, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_ANN.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_ANN.shape[1]\n",
    "    f_total += cost_fn(Y_pred_ANN[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(205.9630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(424.5264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(685.7454, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(883.9132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(1136.4299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(1384.5668, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(1635.8922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(1844.2177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(2082.1145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(2298.9089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(2548.6087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(2745.0889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(2936.1108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(3176.2841, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(3363.1397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(3568.9273, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(3754.0287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(3960.2661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(4186.7685, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(4439.2695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(4676.0405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(4939.3705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(5187.0377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(5456.8483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(5746.1025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(6045.5089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(6317.8749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(6594.7019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(6879.2711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(7142.8754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(7359.0821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(7617.2300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(7875.1084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(8094.0900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(8361.3364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(8631.4235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(8882.4782, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(9126.7136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(9379.3030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(9648.9189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(9921.4370, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(10203.8056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(10485.0224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(10697.1011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(10923.5038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(11196.6687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(11437.2706, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(11720.0275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(11978.9105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(12183.2840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(12388.5153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(12636.2854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(12918.8321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(13129.2789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(13400.4624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(13678.8631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(13921.2351, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'double'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;241m==\u001b[39m n_batches\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m     i_up \u001b[38;5;241m=\u001b[39m n_batches\u001b[38;5;241m*\u001b[39mY_pred_BNN\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m f_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcost_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_pred_BNN\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi_low\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val_original\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_low\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mn_batches\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(b, f_total)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mcost_fn\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_fn\u001b[39m(y_pred, y):\n\u001b[0;32m---> 30\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_f_per_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     f_total \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_total\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mcalc_f_per_day\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_f_per_day\u001b[39m(y_pred, y):\n\u001b[0;32m---> 25\u001b[0m     f_per_item \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_f_por_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(f_per_item, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mcalc_f_por_item\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_f_por_item\u001b[39m(y_pred, y):\n\u001b[1;32m     19\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m reshape_outcomes(y_pred)\n\u001b[0;32m---> 20\u001b[0m     z_star \u001b[38;5;241m=\u001b[39m  \u001b[43margmin_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     f_per_item \u001b[38;5;241m=\u001b[39m cost_per_item(z_star, y)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_per_item\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36margmin_solver\u001b[0;34m(y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmin_solver\u001b[39m(y_pred):\n\u001b[0;32m----> 2\u001b[0m     z_star \u001b[38;5;241m=\u001b[39m \u001b[43mnewsvendor_solve_kkt_M\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z_star\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mSolveNewsvendorWithKKT.forward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m bound \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack((uncert_bound, determ_bound))     \n\u001b[1;32m    112\u001b[0m e \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor()\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[0;32m--> 114\u001b[0m argmin \u001b[38;5;241m=\u001b[39m \u001b[43mQPFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mineqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m argmin[:,:n_items]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'double'"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_BNN.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_BNN.shape[1]\n",
    "    f_total += cost_fn(Y_pred_BNN[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(190.3964, dtype=torch.float64)\n",
      "1 tensor(369.9579, dtype=torch.float64)\n",
      "2 tensor(590.4687, dtype=torch.float64)\n",
      "3 tensor(782.5824, dtype=torch.float64)\n",
      "4 tensor(975.4891, dtype=torch.float64)\n",
      "5 tensor(1171.7013, dtype=torch.float64)\n",
      "6 tensor(1360.3300, dtype=torch.float64)\n",
      "7 tensor(1517.9134, dtype=torch.float64)\n",
      "8 tensor(1717.5365, dtype=torch.float64)\n",
      "9 tensor(1877.5952, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_flow.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_flow.shape[1]\n",
    "    f_total += cost_fn(Y_pred_flow[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13554.7681, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_val_original.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_val_original.shape[1]\n",
    "    f_total += cost_fn(Y_val_original[i_low:i_up,:].unsqueeze(0), Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0070,  5.6180,  0.4540,  ...,  5.6970,  2.0970,  4.0900],\n",
       "        [ 8.1370,  4.5640,  6.4230,  ..., 10.3960, 19.9330, 21.6580],\n",
       "        [ 0.3950,  7.5780,  4.4040,  ..., 11.2520, 15.8290, 11.5620],\n",
       "        ...,\n",
       "        [ 5.4410,  3.4550,  1.9840,  ...,  2.0790,  9.3480, 15.2870],\n",
       "        [ 5.2250,  1.4480,  3.2950,  ...,  4.7980,  9.5330, 11.0260],\n",
       "        [ 3.1210,  2.5350,  2.0630,  ...,  3.9480, 14.1670, 15.3820]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_BNN[:,:10,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': tensor([3., 6., 5., 7., 7., 4., 6., 5.]),\n",
       " 'qs': tensor([23., 20., 18., 21., 17., 23., 15., 24.]),\n",
       " 'qw': tensor([16., 14., 17., 13., 14., 17., 10., 16.]),\n",
       " 'c': tensor([27., 29., 31., 34., 26., 26., 27., 30.]),\n",
       " 'cs': tensor([265., 227., 223., 221., 229., 212., 198., 260.]),\n",
       " 'cw': tensor([ 87., 119.,  83.,  82.,  88.,  89.,  92., 102.]),\n",
       " 'pr': tensor([247., 283., 315., 326., 291., 312., 275., 275.]),\n",
       " 'B': tensor([7271.6777]),\n",
       " 'si': tensor([262., 220., 334., 298., 256., 345., 305., 224.]),\n",
       " 'S': tensor([9181.8213])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0070,  5.6180,  0.4540,  1.4640,  6.9450,  5.6970,  2.0970,  4.0900],\n",
       "        [ 8.1370,  4.5640,  6.4230, 15.1250, 12.7400, 10.3960, 19.9330, 21.6580],\n",
       "        [ 0.3950,  7.5780,  4.4040, 10.9700,  7.7020, 11.2520, 15.8290, 11.5620]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_original[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8452.7811, 20000.0000, 20000.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "(params_t['pr']*argmin_solver(reshape_outcomes(Y_val_original[:3,:].unsqueeze(0)))).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0070e+00, 5.2556e+00, 2.9634e-01, 6.2984e-01, 5.9350e+00, 4.6487e+00,\n",
       "         1.6025e+00, 4.0900e+00],\n",
       "        [3.7207e+00, 6.8705e-16, 6.1038e-17, 2.5419e+00, 1.3610e+00, 3.8036e-01,\n",
       "         4.9692e+00, 1.3247e+01],\n",
       "        [1.5357e-13, 2.2781e+00, 1.3106e-16, 2.9616e+00, 1.4749e+00, 4.6191e+00,\n",
       "         6.0149e+00, 7.7712e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "argmin_solver(reshape_outcomes(Y_val_original[:3,:].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3046,  5.8885,  2.6716,  2.5851,  9.3277,  6.4692,  3.1727,  4.8759],\n",
       "        [ 7.8597,  3.6131,  7.0088, 15.8783, 11.6528,  8.6631, 20.9295, 22.7702],\n",
       "        [ 2.2485,  4.9931,  7.3643, 12.8174,  6.8757, 10.4046, 17.9280, 13.7108]],\n",
       "       dtype=torch.float64, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_BNN[:,:3,:].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0621e+00,  4.5241e+00,  8.4962e-01,  4.6565e-01,  6.5761e+00,\n",
       "          4.3091e+00,  1.2404e+00,  4.4232e+00],\n",
       "        [ 3.3379e+00,  1.6379e-17, -4.7237e-15,  2.9384e+00,  4.1553e-01,\n",
       "         -3.1687e-15,  5.4916e+00,  1.4030e+01],\n",
       "        [ 1.0761e+00,  3.4230e-16,  1.0293e-15,  3.9541e+00,  4.8021e-01,\n",
       "          3.5071e+00,  7.0721e+00,  9.2294e+00]], dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "argmin_solver(reshape_outcomes(Y_pred_BNN[:,:3,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
