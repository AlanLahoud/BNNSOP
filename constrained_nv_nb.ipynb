{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import data_generator\n",
    "import params_newsvendor as params\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from model import VariationalLayer, VariationalNet, StandardNet, VariationalNet2\n",
    "\n",
    "from train import TrainDecoupled\n",
    "\n",
    "from train_normflow import TrainFlowDecoupled\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 9\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters (change if necessary)\n",
    "N = 8000 # Total data size\n",
    "N_train = 5000 # Training data size\n",
    "N_SAMPLES = 16 # Sampling size while training\n",
    "BATCH_SIZE_LOADER = 32 # Standard batch size\n",
    "EPOCHS = 80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "N_valid = N - N_train\n",
    "X, Y_original = data_generator.data_4to8(N_train, noise_level=nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_multi.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output normalization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Y_original)\n",
    "tmean = torch.tensor(scaler.mean_)\n",
    "tstd = torch.tensor(scaler.scale_)\n",
    "joblib.dump(scaler, 'scaler_multi.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(yy):\n",
    "    return yy*tstd + tmean\n",
    "\n",
    "Y = scaler.transform(Y_original).copy()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_generator.ArtificialDataset(X, Y)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "X_val, Y_val_original = data_generator.data_4to8(N_valid, noise_level=nl)\n",
    "Y_val = scaler.transform(Y_val_original).copy()\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_original = torch.tensor(Y_val_original, dtype=torch.float32)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "data_valid = data_generator.ArtificialDataset(X_val, Y_val)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    data_valid, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t, n_samples):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "            \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items  \n",
    "        self.n_samples = n_samples\n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident_samples = torch.eye(n_items*n_samples)\n",
    "        ident3 = torch.eye(n_items + 2*n_items*n_samples)\n",
    "        zeros_matrix = torch.zeros((n_items*n_samples, n_items*n_samples))\n",
    "        zeros_array = torch.zeros(n_items*n_samples)\n",
    "        ones_array = torch.ones(n_items*n_samples)\n",
    "             \n",
    "        self.Q = torch.diag(\n",
    "            torch.hstack(\n",
    "                (\n",
    "                    params_t['q'], \n",
    "                    (1/n_samples)*params_t['qs'].repeat_interleave(n_samples), \n",
    "                    (1/n_samples)*params_t['qw'].repeat_interleave(n_samples)\n",
    "                )\n",
    "            )).to(dev)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.hstack(\n",
    "                                (\n",
    "                                    params_t['c'], \n",
    "                                    (1/n_samples)*params_t['cs'].repeat_interleave(n_samples), \n",
    "                                    (1/n_samples)*params_t['cw'].repeat_interleave(n_samples)\n",
    "                                )).to(dev)\n",
    "             \n",
    "            \n",
    "        shortage_ineq = torch.hstack(\n",
    "            (\n",
    "                -ident.repeat_interleave(n_samples, 0), \n",
    "                -ident_samples, \n",
    "                zeros_matrix\n",
    "            )\n",
    "        )  \n",
    "        \n",
    "        \n",
    "        excess_ineq = torch.hstack(\n",
    "            (\n",
    "                ident.repeat_interleave(n_samples, 0), \n",
    "                zeros_matrix, \n",
    "                -ident_samples\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        price_ineq = torch.hstack(\n",
    "            (\n",
    "                params_t['pr'], \n",
    "                zeros_array, \n",
    "                zeros_array\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        \n",
    "        self.ineqs = torch.vstack(\n",
    "            (\n",
    "                shortage_ineq, \n",
    "                excess_ineq, \n",
    "                price_ineq, \n",
    "                positive_ineq\n",
    "            )\n",
    "        ).to(dev)\n",
    " \n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        \n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items*n_samples), \n",
    "                                          torch.zeros(n_items*n_samples))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, n_samples_items = y.size()\n",
    "                \n",
    "        assert self.n_samples*self.n_items == n_samples_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(batch_size, Q.size(0), Q.size(1))\n",
    "        \n",
    "        lin = self.lin\n",
    "        lin = lin.expand(batch_size, lin.size(0))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(batch_size, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            batch_size, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_item = lambda Z, Y : params_t['q'].to(dev)*Z.to(dev)**2 \\\n",
    "                            + params_t['qs'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)))**2 \\\n",
    "                            + params_t['qw'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev)))**2 \\\n",
    "                            + params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "\n",
    "def reshape_outcomes(y_pred):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    batch_size = y_pred.shape[1]\n",
    "    n_items = y_pred.shape[2]\n",
    "\n",
    "    y_pred = y_pred.permute((1, 2, 0)).reshape((batch_size, n_samples*n_items))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred = reshape_outcomes(y_pred)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f\n",
    "\n",
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ann = StandardNet(input_size, output_size, 0).to(dev)\n",
    "h_bnn = VariationalNet2(N_SAMPLES, input_size, output_size, 1.0).to(dev)\n",
    "\n",
    "opt_h_ann = torch.optim.Adam(h_ann.parameters(), lr=0.0010)\n",
    "opt_h_bnn = torch.optim.Adam(h_bnn.parameters(), lr=0.0015)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 0.492 valid 0.327\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.49 valid 0.33\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 0.296 valid 0.273\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.3 valid 0.27\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.25 valid 0.257\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.25 valid 0.26\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.23 valid 0.242\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.23 valid 0.24\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.217 valid 0.23\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.22 valid 0.23\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.207 valid 0.221\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.21 valid 0.22\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.199 valid 0.212\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.2 valid 0.21\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.192 valid 0.204\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.19 valid 0.2\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.185 valid 0.198\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.19 valid 0.2\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.178 valid 0.191\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.18 valid 0.19\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.17 valid 0.184\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.17 valid 0.18\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.161 valid 0.176\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.16 valid 0.18\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.152 valid 0.168\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.15 valid 0.17\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.142 valid 0.162\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.14 valid 0.16\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.133 valid 0.152\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.13 valid 0.15\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.125 valid 0.145\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.14\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.118 valid 0.138\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.14\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.112 valid 0.134\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.13\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.107 valid 0.129\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.13\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.103 valid 0.127\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.099 valid 0.124\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.12\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.095 valid 0.122\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.12\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.091 valid 0.118\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.088 valid 0.115\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.085 valid 0.113\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.083 valid 0.11\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.08 valid 0.108\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.078 valid 0.107\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.076 valid 0.105\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.1\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.075 valid 0.103\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.074 valid 0.102\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.072 valid 0.101\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.071 valid 0.101\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.07 valid 0.1\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.069 valid 0.099\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.069 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.067 valid 0.097\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.067 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.066 valid 0.095\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.09\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.065 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.09\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.065 valid 0.095\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.064 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.064 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.063 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.062 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.062 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.062 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.061 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.061 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.06 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.06 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.059 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.059 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.058 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.058 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.057 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 57------------------\n",
      "DATA LOSS \t train 0.057 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 58------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.056 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.056 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.056 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.055 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.055 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.054 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.054 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.054 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.053 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.053 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.052 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.052 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.052 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.051 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.051 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.051 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.05 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.05 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.05 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.05 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.049 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.049 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.049 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.049 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.049 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.049 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.048 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.048 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.048 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.048 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.048 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.048 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.048 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.048 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.048 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.047 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.047 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.047 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.046 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.047 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.046 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.046 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.046 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.046 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.046 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.046 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.045 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.045 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.045 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.045 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.045 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.045 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.045 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.045 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.045 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n",
      "------------------EPOCH 113------------------\n",
      "DATA LOSS \t train 0.044 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.044 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n",
      "------------------EPOCH 115------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.044 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.044 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.044 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.044 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.045 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.044 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.08\n"
     ]
    }
   ],
   "source": [
    "train_ANN = TrainDecoupled(\n",
    "                    bnn = False,\n",
    "                    model=h_ann,\n",
    "                    opt=opt_h_ann,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=0.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader\n",
    "                )\n",
    "\n",
    "train_ANN.train(EPOCHS=120)\n",
    "model_ann = train_ANN.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 1.537 valid 0.911\n",
      "KL LOSS \t train 4.34 valid 4.34\n",
      "ELBO LOSS \t train 5.88 valid 5.25\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 0.796 valid 0.688\n",
      "KL LOSS \t train 4.34 valid 4.33\n",
      "ELBO LOSS \t train 5.13 valid 5.02\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.618 valid 0.573\n",
      "KL LOSS \t train 4.33 valid 4.33\n",
      "ELBO LOSS \t train 4.95 valid 4.91\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.512 valid 0.472\n",
      "KL LOSS \t train 4.34 valid 4.34\n",
      "ELBO LOSS \t train 4.85 valid 4.81\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.414 valid 0.388\n",
      "KL LOSS \t train 4.34 valid 4.35\n",
      "ELBO LOSS \t train 4.76 valid 4.73\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.355 valid 0.345\n",
      "KL LOSS \t train 4.35 valid 4.36\n",
      "ELBO LOSS \t train 4.71 valid 4.7\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.318 valid 0.314\n",
      "KL LOSS \t train 4.36 valid 4.36\n",
      "ELBO LOSS \t train 4.68 valid 4.68\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.293 valid 0.294\n",
      "KL LOSS \t train 4.37 valid 4.37\n",
      "ELBO LOSS \t train 4.66 valid 4.66\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.274 valid 0.278\n",
      "KL LOSS \t train 4.37 valid 4.37\n",
      "ELBO LOSS \t train 4.64 valid 4.65\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.26 valid 0.265\n",
      "KL LOSS \t train 4.38 valid 4.38\n",
      "ELBO LOSS \t train 4.64 valid 4.65\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.248 valid 0.255\n",
      "KL LOSS \t train 4.38 valid 4.38\n",
      "ELBO LOSS \t train 4.63 valid 4.64\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.237 valid 0.245\n",
      "KL LOSS \t train 4.39 valid 4.39\n",
      "ELBO LOSS \t train 4.62 valid 4.63\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.23 valid 0.238\n",
      "KL LOSS \t train 4.39 valid 4.39\n",
      "ELBO LOSS \t train 4.62 valid 4.63\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.222 valid 0.23\n",
      "KL LOSS \t train 4.39 valid 4.4\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.215 valid 0.225\n",
      "KL LOSS \t train 4.4 valid 4.4\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.209 valid 0.218\n",
      "KL LOSS \t train 4.4 valid 4.41\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.203 valid 0.213\n",
      "KL LOSS \t train 4.41 valid 4.41\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.196 valid 0.209\n",
      "KL LOSS \t train 4.41 valid 4.42\n",
      "ELBO LOSS \t train 4.61 valid 4.62\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.191 valid 0.201\n",
      "KL LOSS \t train 4.42 valid 4.42\n",
      "ELBO LOSS \t train 4.61 valid 4.62\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.184 valid 0.197\n",
      "KL LOSS \t train 4.42 valid 4.43\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.179 valid 0.19\n",
      "KL LOSS \t train 4.43 valid 4.43\n",
      "ELBO LOSS \t train 4.61 valid 4.62\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.172 valid 0.183\n",
      "KL LOSS \t train 4.43 valid 4.44\n",
      "ELBO LOSS \t train 4.61 valid 4.62\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.164 valid 0.175\n",
      "KL LOSS \t train 4.44 valid 4.44\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.158 valid 0.169\n",
      "KL LOSS \t train 4.45 valid 4.45\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.151 valid 0.165\n",
      "KL LOSS \t train 4.45 valid 4.46\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.147 valid 0.159\n",
      "KL LOSS \t train 4.46 valid 4.46\n",
      "ELBO LOSS \t train 4.61 valid 4.62\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.141 valid 0.158\n",
      "KL LOSS \t train 4.47 valid 4.47\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.137 valid 0.153\n",
      "KL LOSS \t train 4.47 valid 4.48\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.134 valid 0.149\n",
      "KL LOSS \t train 4.48 valid 4.48\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.13 valid 0.146\n",
      "KL LOSS \t train 4.48 valid 4.49\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.127 valid 0.144\n",
      "KL LOSS \t train 4.49 valid 4.49\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.124 valid 0.142\n",
      "KL LOSS \t train 4.49 valid 4.5\n",
      "ELBO LOSS \t train 4.61 valid 4.64\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.121 valid 0.138\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.118 valid 0.134\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.115 valid 0.133\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.62 valid 4.63\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.113 valid 0.131\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.11 valid 0.129\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.107 valid 0.126\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.62 valid 4.63\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.105 valid 0.125\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.104 valid 0.123\n",
      "KL LOSS \t train 4.52 valid 4.51\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.102 valid 0.122\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.1 valid 0.12\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.098 valid 0.118\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.096 valid 0.117\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.094 valid 0.115\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.62 valid 4.64\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.093 valid 0.114\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.091 valid 0.113\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.61 valid 4.64\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.09 valid 0.111\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.088 valid 0.11\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.087 valid 0.109\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.085 valid 0.109\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.61 valid 4.63\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.084 valid 0.107\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.085 valid 0.108\n",
      "KL LOSS \t train 4.52 valid 4.51\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.083 valid 0.105\n",
      "KL LOSS \t train 4.52 valid 4.52\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.081 valid 0.104\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.6 valid 4.62\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.08 valid 0.104\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.59 valid 4.62\n",
      "------------------EPOCH 57------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.079 valid 0.102\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.59 valid 4.62\n",
      "------------------EPOCH 58------------------\n",
      "DATA LOSS \t train 0.078 valid 0.102\n",
      "KL LOSS \t train 4.51 valid 4.51\n",
      "ELBO LOSS \t train 4.59 valid 4.61\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.077 valid 0.101\n",
      "KL LOSS \t train 4.51 valid 4.5\n",
      "ELBO LOSS \t train 4.58 valid 4.61\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.077 valid 0.101\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.58 valid 4.61\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.077 valid 0.101\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.58 valid 4.6\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.075 valid 0.099\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.57 valid 4.6\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.077 valid 0.103\n",
      "KL LOSS \t train 4.5 valid 4.5\n",
      "ELBO LOSS \t train 4.57 valid 4.6\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.075 valid 0.099\n",
      "KL LOSS \t train 4.49 valid 4.49\n",
      "ELBO LOSS \t train 4.57 valid 4.59\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.074 valid 0.099\n",
      "KL LOSS \t train 4.49 valid 4.49\n",
      "ELBO LOSS \t train 4.57 valid 4.59\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.074 valid 0.098\n",
      "KL LOSS \t train 4.49 valid 4.49\n",
      "ELBO LOSS \t train 4.56 valid 4.59\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.072 valid 0.097\n",
      "KL LOSS \t train 4.48 valid 4.48\n",
      "ELBO LOSS \t train 4.56 valid 4.58\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.073 valid 0.097\n",
      "KL LOSS \t train 4.48 valid 4.48\n",
      "ELBO LOSS \t train 4.55 valid 4.58\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.072 valid 0.097\n",
      "KL LOSS \t train 4.48 valid 4.48\n",
      "ELBO LOSS \t train 4.55 valid 4.58\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.071 valid 0.096\n",
      "KL LOSS \t train 4.48 valid 4.47\n",
      "ELBO LOSS \t train 4.55 valid 4.57\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.071 valid 0.096\n",
      "KL LOSS \t train 4.48 valid 4.48\n",
      "ELBO LOSS \t train 4.55 valid 4.57\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.07 valid 0.095\n",
      "KL LOSS \t train 4.47 valid 4.47\n",
      "ELBO LOSS \t train 4.54 valid 4.57\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.07 valid 0.095\n",
      "KL LOSS \t train 4.47 valid 4.47\n",
      "ELBO LOSS \t train 4.54 valid 4.57\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.07 valid 0.096\n",
      "KL LOSS \t train 4.47 valid 4.47\n",
      "ELBO LOSS \t train 4.54 valid 4.57\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.07 valid 0.098\n",
      "KL LOSS \t train 4.47 valid 4.46\n",
      "ELBO LOSS \t train 4.54 valid 4.56\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.069 valid 0.095\n",
      "KL LOSS \t train 4.47 valid 4.47\n",
      "ELBO LOSS \t train 4.54 valid 4.56\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.07 valid 0.097\n",
      "KL LOSS \t train 4.46 valid 4.46\n",
      "ELBO LOSS \t train 4.53 valid 4.56\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.069 valid 0.094\n",
      "KL LOSS \t train 4.46 valid 4.46\n",
      "ELBO LOSS \t train 4.53 valid 4.55\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.067 valid 0.094\n",
      "KL LOSS \t train 4.46 valid 4.46\n",
      "ELBO LOSS \t train 4.53 valid 4.56\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.068 valid 0.094\n",
      "KL LOSS \t train 4.46 valid 4.46\n",
      "ELBO LOSS \t train 4.53 valid 4.56\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.067 valid 0.093\n",
      "KL LOSS \t train 4.46 valid 4.45\n",
      "ELBO LOSS \t train 4.53 valid 4.55\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.069 valid 0.108\n",
      "KL LOSS \t train 4.46 valid 4.45\n",
      "ELBO LOSS \t train 4.52 valid 4.56\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.071 valid 0.092\n",
      "KL LOSS \t train 4.45 valid 4.45\n",
      "ELBO LOSS \t train 4.52 valid 4.54\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.066 valid 0.091\n",
      "KL LOSS \t train 4.45 valid 4.45\n",
      "ELBO LOSS \t train 4.52 valid 4.54\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.065 valid 0.106\n",
      "KL LOSS \t train 4.45 valid 4.45\n",
      "ELBO LOSS \t train 4.52 valid 4.56\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.065 valid 0.09\n",
      "KL LOSS \t train 4.45 valid 4.45\n",
      "ELBO LOSS \t train 4.51 valid 4.54\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.066 valid 0.091\n",
      "KL LOSS \t train 4.45 valid 4.45\n",
      "ELBO LOSS \t train 4.51 valid 4.54\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.068 valid 0.095\n",
      "KL LOSS \t train 4.45 valid 4.44\n",
      "ELBO LOSS \t train 4.51 valid 4.54\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.069 valid 0.09\n",
      "KL LOSS \t train 4.44 valid 4.45\n",
      "ELBO LOSS \t train 4.51 valid 4.54\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.064 valid 0.091\n",
      "KL LOSS \t train 4.44 valid 4.44\n",
      "ELBO LOSS \t train 4.51 valid 4.53\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.065 valid 0.09\n",
      "KL LOSS \t train 4.44 valid 4.44\n",
      "ELBO LOSS \t train 4.5 valid 4.53\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.065 valid 0.093\n",
      "KL LOSS \t train 4.44 valid 4.44\n",
      "ELBO LOSS \t train 4.5 valid 4.53\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.064 valid 0.09\n",
      "KL LOSS \t train 4.44 valid 4.44\n",
      "ELBO LOSS \t train 4.5 valid 4.53\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.064 valid 0.088\n",
      "KL LOSS \t train 4.43 valid 4.44\n",
      "ELBO LOSS \t train 4.5 valid 4.52\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.064 valid 0.089\n",
      "KL LOSS \t train 4.43 valid 4.43\n",
      "ELBO LOSS \t train 4.5 valid 4.52\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.063 valid 0.088\n",
      "KL LOSS \t train 4.43 valid 4.43\n",
      "ELBO LOSS \t train 4.49 valid 4.52\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.065 valid 0.089\n",
      "KL LOSS \t train 4.43 valid 4.43\n",
      "ELBO LOSS \t train 4.5 valid 4.52\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.063 valid 0.088\n",
      "KL LOSS \t train 4.43 valid 4.43\n",
      "ELBO LOSS \t train 4.49 valid 4.51\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.07 valid 0.091\n",
      "KL LOSS \t train 4.43 valid 4.43\n",
      "ELBO LOSS \t train 4.5 valid 4.52\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.065 valid 0.089\n",
      "KL LOSS \t train 4.43 valid 4.42\n",
      "ELBO LOSS \t train 4.49 valid 4.51\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.063 valid 0.088\n",
      "KL LOSS \t train 4.42 valid 4.43\n",
      "ELBO LOSS \t train 4.49 valid 4.51\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.062 valid 0.088\n",
      "KL LOSS \t train 4.42 valid 4.42\n",
      "ELBO LOSS \t train 4.48 valid 4.51\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.062 valid 0.096\n",
      "KL LOSS \t train 4.42 valid 4.42\n",
      "ELBO LOSS \t train 4.48 valid 4.52\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.063 valid 0.088\n",
      "KL LOSS \t train 4.42 valid 4.41\n",
      "ELBO LOSS \t train 4.48 valid 4.5\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.061 valid 0.088\n",
      "KL LOSS \t train 4.42 valid 4.42\n",
      "ELBO LOSS \t train 4.48 valid 4.5\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.066 valid 0.09\n",
      "KL LOSS \t train 4.41 valid 4.41\n",
      "ELBO LOSS \t train 4.48 valid 4.5\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.071 valid 0.087\n",
      "KL LOSS \t train 4.41 valid 4.41\n",
      "ELBO LOSS \t train 4.48 valid 4.5\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.061 valid 0.087\n",
      "KL LOSS \t train 4.41 valid 4.41\n",
      "ELBO LOSS \t train 4.47 valid 4.5\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.063 valid 0.088\n",
      "KL LOSS \t train 4.41 valid 4.4\n",
      "ELBO LOSS \t train 4.47 valid 4.49\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.062 valid 0.088\n",
      "KL LOSS \t train 4.41 valid 4.4\n",
      "ELBO LOSS \t train 4.47 valid 4.49\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.061 valid 0.086\n",
      "KL LOSS \t train 4.4 valid 4.4\n",
      "ELBO LOSS \t train 4.46 valid 4.48\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.06 valid 0.086\n",
      "KL LOSS \t train 4.4 valid 4.4\n",
      "ELBO LOSS \t train 4.46 valid 4.49\n",
      "------------------EPOCH 113------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.066 valid 0.108\n",
      "KL LOSS \t train 4.4 valid 4.4\n",
      "ELBO LOSS \t train 4.46 valid 4.5\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.067 valid 0.094\n",
      "KL LOSS \t train 4.4 valid 4.4\n",
      "ELBO LOSS \t train 4.46 valid 4.49\n",
      "------------------EPOCH 115------------------\n",
      "DATA LOSS \t train 0.075 valid 0.092\n",
      "KL LOSS \t train 4.39 valid 4.39\n",
      "ELBO LOSS \t train 4.47 valid 4.48\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.066 valid 0.087\n",
      "KL LOSS \t train 4.39 valid 4.39\n",
      "ELBO LOSS \t train 4.46 valid 4.47\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.077 valid 0.106\n",
      "KL LOSS \t train 4.39 valid 4.38\n",
      "ELBO LOSS \t train 4.46 valid 4.49\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.072 valid 0.088\n",
      "KL LOSS \t train 4.38 valid 4.38\n",
      "ELBO LOSS \t train 4.46 valid 4.47\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.065 valid 0.093\n",
      "KL LOSS \t train 4.38 valid 4.38\n",
      "ELBO LOSS \t train 4.45 valid 4.47\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.064 valid 0.086\n",
      "KL LOSS \t train 4.38 valid 4.38\n",
      "ELBO LOSS \t train 4.44 valid 4.46\n",
      "------------------EPOCH 121------------------\n",
      "DATA LOSS \t train 0.063 valid 0.087\n",
      "KL LOSS \t train 4.38 valid 4.37\n",
      "ELBO LOSS \t train 4.44 valid 4.46\n",
      "------------------EPOCH 122------------------\n",
      "DATA LOSS \t train 0.062 valid 0.085\n",
      "KL LOSS \t train 4.37 valid 4.37\n",
      "ELBO LOSS \t train 4.43 valid 4.46\n",
      "------------------EPOCH 123------------------\n",
      "DATA LOSS \t train 0.06 valid 0.086\n",
      "KL LOSS \t train 4.37 valid 4.37\n",
      "ELBO LOSS \t train 4.43 valid 4.46\n",
      "------------------EPOCH 124------------------\n",
      "DATA LOSS \t train 0.06 valid 0.084\n",
      "KL LOSS \t train 4.37 valid 4.37\n",
      "ELBO LOSS \t train 4.43 valid 4.45\n",
      "------------------EPOCH 125------------------\n",
      "DATA LOSS \t train 0.059 valid 0.084\n",
      "KL LOSS \t train 4.37 valid 4.36\n",
      "ELBO LOSS \t train 4.43 valid 4.45\n",
      "------------------EPOCH 126------------------\n",
      "DATA LOSS \t train 0.061 valid 0.084\n",
      "KL LOSS \t train 4.36 valid 4.36\n",
      "ELBO LOSS \t train 4.42 valid 4.44\n",
      "------------------EPOCH 127------------------\n",
      "DATA LOSS \t train 0.059 valid 0.085\n",
      "KL LOSS \t train 4.36 valid 4.36\n",
      "ELBO LOSS \t train 4.42 valid 4.45\n",
      "------------------EPOCH 128------------------\n",
      "DATA LOSS \t train 0.058 valid 0.084\n",
      "KL LOSS \t train 4.36 valid 4.36\n",
      "ELBO LOSS \t train 4.42 valid 4.44\n",
      "------------------EPOCH 129------------------\n",
      "DATA LOSS \t train 0.061 valid 0.11\n",
      "KL LOSS \t train 4.36 valid 4.35\n",
      "ELBO LOSS \t train 4.42 valid 4.46\n",
      "------------------EPOCH 130------------------\n",
      "DATA LOSS \t train 0.067 valid 0.084\n",
      "KL LOSS \t train 4.35 valid 4.35\n",
      "ELBO LOSS \t train 4.42 valid 4.43\n",
      "------------------EPOCH 131------------------\n",
      "DATA LOSS \t train 0.059 valid 0.085\n",
      "KL LOSS \t train 4.35 valid 4.35\n",
      "ELBO LOSS \t train 4.41 valid 4.43\n",
      "------------------EPOCH 132------------------\n",
      "DATA LOSS \t train 0.06 valid 0.084\n",
      "KL LOSS \t train 4.35 valid 4.35\n",
      "ELBO LOSS \t train 4.41 valid 4.43\n",
      "------------------EPOCH 133------------------\n",
      "DATA LOSS \t train 0.058 valid 0.084\n",
      "KL LOSS \t train 4.34 valid 4.34\n",
      "ELBO LOSS \t train 4.4 valid 4.43\n",
      "------------------EPOCH 134------------------\n",
      "DATA LOSS \t train 0.058 valid 0.085\n",
      "KL LOSS \t train 4.34 valid 4.34\n",
      "ELBO LOSS \t train 4.4 valid 4.43\n",
      "------------------EPOCH 135------------------\n",
      "DATA LOSS \t train 0.075 valid 0.089\n",
      "KL LOSS \t train 4.34 valid 4.34\n",
      "ELBO LOSS \t train 4.41 valid 4.43\n",
      "------------------EPOCH 136------------------\n",
      "DATA LOSS \t train 0.064 valid 0.087\n",
      "KL LOSS \t train 4.34 valid 4.34\n",
      "ELBO LOSS \t train 4.4 valid 4.42\n",
      "------------------EPOCH 137------------------\n",
      "DATA LOSS \t train 0.061 valid 0.087\n",
      "KL LOSS \t train 4.33 valid 4.33\n",
      "ELBO LOSS \t train 4.39 valid 4.42\n",
      "------------------EPOCH 138------------------\n",
      "DATA LOSS \t train 0.076 valid 0.098\n",
      "KL LOSS \t train 4.33 valid 4.33\n",
      "ELBO LOSS \t train 4.41 valid 4.43\n",
      "------------------EPOCH 139------------------\n",
      "DATA LOSS \t train 0.079 valid 0.104\n",
      "KL LOSS \t train 4.33 valid 4.32\n",
      "ELBO LOSS \t train 4.41 valid 4.43\n",
      "------------------EPOCH 140------------------\n",
      "DATA LOSS \t train 0.09 valid 0.092\n",
      "KL LOSS \t train 4.32 valid 4.32\n",
      "ELBO LOSS \t train 4.41 valid 4.42\n",
      "------------------EPOCH 141------------------\n",
      "DATA LOSS \t train 0.068 valid 0.087\n",
      "KL LOSS \t train 4.32 valid 4.32\n",
      "ELBO LOSS \t train 4.39 valid 4.41\n",
      "------------------EPOCH 142------------------\n",
      "DATA LOSS \t train 0.063 valid 0.085\n",
      "KL LOSS \t train 4.32 valid 4.32\n",
      "ELBO LOSS \t train 4.38 valid 4.4\n",
      "------------------EPOCH 143------------------\n",
      "DATA LOSS \t train 0.064 valid 0.092\n",
      "KL LOSS \t train 4.31 valid 4.31\n",
      "ELBO LOSS \t train 4.38 valid 4.4\n",
      "------------------EPOCH 144------------------\n",
      "DATA LOSS \t train 0.066 valid 0.084\n",
      "KL LOSS \t train 4.31 valid 4.31\n",
      "ELBO LOSS \t train 4.38 valid 4.39\n",
      "------------------EPOCH 145------------------\n",
      "DATA LOSS \t train 0.06 valid 0.082\n",
      "KL LOSS \t train 4.31 valid 4.31\n",
      "ELBO LOSS \t train 4.37 valid 4.39\n",
      "------------------EPOCH 146------------------\n",
      "DATA LOSS \t train 0.063 valid 0.088\n",
      "KL LOSS \t train 4.31 valid 4.3\n",
      "ELBO LOSS \t train 4.37 valid 4.39\n",
      "------------------EPOCH 147------------------\n",
      "DATA LOSS \t train 0.06 valid 0.083\n",
      "KL LOSS \t train 4.3 valid 4.3\n",
      "ELBO LOSS \t train 4.36 valid 4.39\n",
      "------------------EPOCH 148------------------\n",
      "DATA LOSS \t train 0.058 valid 0.081\n",
      "KL LOSS \t train 4.3 valid 4.3\n",
      "ELBO LOSS \t train 4.36 valid 4.38\n",
      "------------------EPOCH 149------------------\n",
      "DATA LOSS \t train 0.058 valid 0.081\n",
      "KL LOSS \t train 4.3 valid 4.3\n",
      "ELBO LOSS \t train 4.35 valid 4.38\n",
      "------------------EPOCH 150------------------\n",
      "DATA LOSS \t train 0.058 valid 0.088\n",
      "KL LOSS \t train 4.29 valid 4.29\n",
      "ELBO LOSS \t train 4.35 valid 4.38\n"
     ]
    }
   ],
   "source": [
    "train_BNN = TrainDecoupled(\n",
    "                    bnn = True,\n",
    "                    model=h_bnn,\n",
    "                    opt=opt_h_bnn,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=1.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader\n",
    "                )\n",
    "\n",
    "train_BNN.train(EPOCHS=150)\n",
    "model_bnn = train_BNN.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 5.12326, val loss: 5.14346\n",
      "step: 50, train loss: 2.13778, val loss: 2.15931\n",
      "step: 100, train loss: 1.8985, val loss: 1.9508\n",
      "step: 150, train loss: 1.8401, val loss: 1.88361\n",
      "step: 200, train loss: 1.73643, val loss: 1.82557\n",
      "step: 250, train loss: 1.73839, val loss: 1.77618\n",
      "step: 300, train loss: 1.63881, val loss: 1.70825\n",
      "step: 350, train loss: 1.64095, val loss: 1.73542\n",
      "step: 400, train loss: 1.58708, val loss: 1.63954\n",
      "step: 450, train loss: 1.52352, val loss: 1.5941\n",
      "step: 500, train loss: 1.5568, val loss: 1.61974\n",
      "step: 550, train loss: 1.46915, val loss: 1.59045\n",
      "step: 600, train loss: 1.52485, val loss: 1.60449\n",
      "step: 650, train loss: 1.45155, val loss: 1.53815\n",
      "step: 700, train loss: 1.45144, val loss: 1.53169\n",
      "step: 750, train loss: 1.44863, val loss: 1.53663\n",
      "step: 800, train loss: 1.48449, val loss: 1.53341\n",
      "step: 850, train loss: 1.40554, val loss: 1.51814\n",
      "step: 900, train loss: 1.41138, val loss: 1.49453\n",
      "step: 950, train loss: 1.46639, val loss: 1.53898\n",
      "step: 1000, train loss: 1.40238, val loss: 1.50473\n",
      "step: 1050, train loss: 1.4113, val loss: 1.52136\n",
      "step: 1100, train loss: 1.37956, val loss: 1.49721\n",
      "step: 1150, train loss: 1.35544, val loss: 1.47944\n",
      "step: 1200, train loss: 1.37116, val loss: 1.49997\n",
      "step: 1250, train loss: 1.38574, val loss: 1.49163\n",
      "step: 1300, train loss: 1.40251, val loss: 1.48304\n",
      "step: 1350, train loss: 1.37262, val loss: 1.48896\n",
      "step: 1400, train loss: 1.57955, val loss: 1.56055\n",
      "step: 1450, train loss: 1.36853, val loss: 1.4875\n",
      "step: 1500, train loss: 1.34935, val loss: 1.50711\n",
      "step: 1550, train loss: 1.37531, val loss: 1.50882\n",
      "step: 1600, train loss: 1.34223, val loss: 1.46548\n",
      "step: 1650, train loss: 1.37689, val loss: 1.47563\n",
      "step: 1700, train loss: 1.39565, val loss: 1.51046\n",
      "step: 1750, train loss: 1.32635, val loss: 1.46805\n",
      "step: 1800, train loss: 1.34071, val loss: 1.46523\n",
      "step: 1850, train loss: 1.3501, val loss: 1.4596\n",
      "step: 1900, train loss: 1.3679, val loss: 1.48618\n",
      "step: 1950, train loss: 1.36939, val loss: 1.47313\n",
      "step: 2000, train loss: 1.31742, val loss: 1.4654\n",
      "step: 2050, train loss: 1.36629, val loss: 1.47043\n",
      "step: 2100, train loss: 1.65595, val loss: 1.7283\n",
      "step: 2150, train loss: 1.37156, val loss: 1.47866\n",
      "step: 2200, train loss: 1.37, val loss: 1.47268\n",
      "step: 2250, train loss: 1.40012, val loss: 1.49425\n",
      "step: 2300, train loss: 1.34036, val loss: 1.45794\n",
      "step: 2350, train loss: 1.34414, val loss: 1.51184\n",
      "step: 2400, train loss: 1.35558, val loss: 1.50815\n",
      "step: 2450, train loss: 1.34311, val loss: 1.45693\n"
     ]
    }
   ],
   "source": [
    "train_flow = TrainFlowDecoupled(steps=4000, input_size=4, output_size=8)\n",
    "model_flow = train_flow.train(X, Y, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t, _ = params.get_params(n_items, seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagating predictions to Newsvendor Problem\n",
    "M = 4\n",
    "\n",
    "Y_pred_ANN = train_ANN.model(X_val).unsqueeze(0)\n",
    "Y_pred_ANN = inverse_transform(Y_pred_ANN)\n",
    "\n",
    "train_BNN.model.update_n_samples(n_samples=M)\n",
    "Y_pred_BNN = train_BNN.model.forward_dist(X_val)\n",
    "Y_pred_BNN = inverse_transform(Y_pred_BNN)\n",
    "M = Y_pred_BNN.shape[0]\n",
    "\n",
    "N = X_val.shape[0]\n",
    "Y_pred_flow = torch.zeros((M, N, n_items))\n",
    "for i in range(0, N):\n",
    "    Y_pred_flow[:,i,:] = model_flow.condition(X_val[i]).sample(torch.Size([M,])).squeeze()\n",
    "Y_pred_flow = inverse_transform(Y_pred_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7952, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8478, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2457, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "print(mse_loss(Y_pred_ANN.mean(axis=0), Y_val_original))\n",
    "print(mse_loss(Y_pred_BNN.mean(axis=0), Y_val_original))\n",
    "print(mse_loss(Y_pred_flow.mean(axis=0), Y_val_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Construct the solver\u001b[39;00m\n\u001b[1;32m      2\u001b[0m newsvendor_solve_kkt \u001b[38;5;241m=\u001b[39m SolveNewsvendorWithKKT(params_t, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m newsvendor_solve_kkt_M \u001b[38;5;241m=\u001b[39m SolveNewsvendorWithKKT(params_t, \u001b[43mM\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t, 1)\n",
    "newsvendor_solve_kkt_M = SolveNewsvendorWithKKT(params_t, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(368.8417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(719.3489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(1074.0162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(1392.0964, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/qpth/qp.py:83: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2910.)\n",
      "  e, _ = torch.eig(Q[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(1755.9449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(2138.9331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(2484.1407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(2769.4024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(3173.2590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(3488.9858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(3853.0679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(4192.5381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(4540.0599, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(4868.4100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(5141.7237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(5485.1656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(5810.2758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(6150.4915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(6511.3681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(6844.5463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(7155.6367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(7496.1636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(7848.9941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(8301.1432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(8677.8268, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(9102.6880, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(9458.9964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(9762.4236, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(10165.6124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(10502.1217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(10758.8138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(11086.2107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(11400.8162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(11733.2217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(12200.6192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(12553.8290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(12923.0788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(13375.2255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(13740.7400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(14088.6540, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(14522.1468, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(14886.8301, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(15325.5781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(15635.8436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(15982.0757, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(16365.7764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(16717.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(17127.7184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(17483.4113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(17819.3106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(18128.4720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(18513.7401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(18999.5074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(19431.7927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(19822.3172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(20192.9609, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(20549.0362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "57 tensor(20927.5093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "58 tensor(21318.1143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "59 tensor(21642.2155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "60 tensor(22052.0337, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "61 tensor(22418.0368, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "62 tensor(22757.1312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "63 tensor(23089.5582, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "64 tensor(23428.8228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "65 tensor(23888.3852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "66 tensor(24229.6422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "67 tensor(24547.4187, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "68 tensor(24861.6219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "69 tensor(25213.1283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "70 tensor(25688.2425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "71 tensor(26041.9252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "72 tensor(26433.8862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "73 tensor(26793.2038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "74 tensor(27167.0672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "75 tensor(27488.2499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "76 tensor(27870.7548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "77 tensor(28241.7924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "78 tensor(28590.4345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "79 tensor(28968.2603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "80 tensor(29343.3432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "81 tensor(29689.6894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "82 tensor(30021.5349, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "83 tensor(30445.8675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "84 tensor(30853.8514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "85 tensor(31204.3042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "86 tensor(31542.7695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "87 tensor(31874.0794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "88 tensor(32248.6612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "89 tensor(32690.6166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "90 tensor(33148.0448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "91 tensor(33585.2219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "92 tensor(33997.2621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "93 tensor(34328.8288, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_ANN.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_ANN.shape[1]\n",
    "    f_total += cost_fn(Y_pred_ANN[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(360.6398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(702.9200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(1052.3830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(1365.4478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(1724.8241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(2107.8547, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(2445.2796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(2721.7141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(3121.2553, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(3431.9031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(3787.1780, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(4118.1884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(4460.6765, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(4784.5170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(5053.0189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(5393.8698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(5710.3712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(6043.7433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(6397.4359, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(6722.3894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(7025.9789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(7368.4964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(7716.0658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(8165.7481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(8536.4601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(8957.2628, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(9307.1356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(9611.6740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(10012.3833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(10336.2136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(10593.6275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(10918.0850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(11229.9182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(11551.3640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(12005.8534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(12358.7055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(12725.5216, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(13170.8532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(13534.6768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(13878.4845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(14307.5710, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(14669.8605, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(15101.9992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(15410.1588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(15746.8774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(16127.8418, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(16477.6110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(16883.5921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(17233.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(17563.3629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(17863.7479, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(18246.0199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(18722.3485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(19147.2254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(19528.7611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(19889.6781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(20240.8010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "57 tensor(20612.9463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "58 tensor(21006.2180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "59 tensor(21321.3112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "60 tensor(21731.7950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "61 tensor(22096.3080, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "62 tensor(22430.5556, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "63 tensor(22755.4379, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "64 tensor(23088.4902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "65 tensor(23542.7948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "66 tensor(23879.8239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "67 tensor(24200.2687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "68 tensor(24509.1198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "69 tensor(24856.4404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "70 tensor(25318.1966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "71 tensor(25667.1344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "72 tensor(26051.7283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "73 tensor(26408.9875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "74 tensor(26776.3528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "75 tensor(27091.6559, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "76 tensor(27471.7308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "77 tensor(27842.0891, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "78 tensor(28184.3354, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "79 tensor(28561.4521, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "80 tensor(28933.2107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "81 tensor(29280.2753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "82 tensor(29606.2938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "83 tensor(30022.8697, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "84 tensor(30428.2600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "85 tensor(30772.9326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "86 tensor(31103.8365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "87 tensor(31433.1079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "88 tensor(31805.2581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "89 tensor(32238.6353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "90 tensor(32688.6611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "91 tensor(33122.9541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "92 tensor(33527.0290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "93 tensor(33854.6270, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_BNN.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_BNN.shape[1]\n",
    "    f_total += cost_fn(Y_pred_BNN[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(354.4790, dtype=torch.float64)\n",
      "1 tensor(689.6819, dtype=torch.float64)\n",
      "2 tensor(1031.8333, dtype=torch.float64)\n",
      "3 tensor(1340.4632, dtype=torch.float64)\n",
      "4 tensor(1690.9033, dtype=torch.float64)\n",
      "5 tensor(2065.6534, dtype=torch.float64)\n",
      "6 tensor(2398.2139, dtype=torch.float64)\n",
      "7 tensor(2671.9200, dtype=torch.float64)\n",
      "8 tensor(3067.8034, dtype=torch.float64)\n",
      "9 tensor(3370.2477, dtype=torch.float64)\n",
      "10 tensor(3720.3259, dtype=torch.float64)\n",
      "11 tensor(4047.8493, dtype=torch.float64)\n",
      "12 tensor(4380.8409, dtype=torch.float64)\n",
      "13 tensor(4700.4676, dtype=torch.float64)\n",
      "14 tensor(4963.4861, dtype=torch.float64)\n",
      "15 tensor(5294.0947, dtype=torch.float64)\n",
      "16 tensor(5606.7736, dtype=torch.float64)\n",
      "17 tensor(5938.9002, dtype=torch.float64)\n",
      "18 tensor(6289.8334, dtype=torch.float64)\n",
      "19 tensor(6611.9189, dtype=torch.float64)\n",
      "20 tensor(6908.5777, dtype=torch.float64)\n",
      "21 tensor(7239.0349, dtype=torch.float64)\n",
      "22 tensor(7579.4811, dtype=torch.float64)\n",
      "23 tensor(8024.3282, dtype=torch.float64)\n",
      "24 tensor(8387.7170, dtype=torch.float64)\n",
      "25 tensor(8803.9367, dtype=torch.float64)\n",
      "26 tensor(9147.1379, dtype=torch.float64)\n",
      "27 tensor(9445.4492, dtype=torch.float64)\n",
      "28 tensor(9845.8501, dtype=torch.float64)\n",
      "29 tensor(10165.4235, dtype=torch.float64)\n",
      "30 tensor(10415.8659, dtype=torch.float64)\n",
      "31 tensor(10734.1548, dtype=torch.float64)\n",
      "32 tensor(11039.9724, dtype=torch.float64)\n",
      "33 tensor(11352.8595, dtype=torch.float64)\n",
      "34 tensor(11803.9360, dtype=torch.float64)\n",
      "35 tensor(12147.4941, dtype=torch.float64)\n",
      "36 tensor(12509.3078, dtype=torch.float64)\n",
      "37 tensor(12947.2681, dtype=torch.float64)\n",
      "38 tensor(13312.3928, dtype=torch.float64)\n",
      "39 tensor(13645.2607, dtype=torch.float64)\n",
      "40 tensor(14065.4386, dtype=torch.float64)\n",
      "41 tensor(14422.7689, dtype=torch.float64)\n",
      "42 tensor(14851.7073, dtype=torch.float64)\n",
      "43 tensor(15156.6521, dtype=torch.float64)\n",
      "44 tensor(15493.9420, dtype=torch.float64)\n",
      "45 tensor(15870.7810, dtype=torch.float64)\n",
      "46 tensor(16213.9542, dtype=torch.float64)\n",
      "47 tensor(16613.7576, dtype=torch.float64)\n",
      "48 tensor(16962.9997, dtype=torch.float64)\n",
      "49 tensor(17293.2634, dtype=torch.float64)\n",
      "50 tensor(17594.0039, dtype=torch.float64)\n",
      "51 tensor(17972.0759, dtype=torch.float64)\n",
      "52 tensor(18446.5306, dtype=torch.float64)\n",
      "53 tensor(18864.3550, dtype=torch.float64)\n",
      "54 tensor(19236.5221, dtype=torch.float64)\n",
      "55 tensor(19597.8164, dtype=torch.float64)\n",
      "56 tensor(19948.1708, dtype=torch.float64)\n",
      "57 tensor(20316.1634, dtype=torch.float64)\n",
      "58 tensor(20702.5246, dtype=torch.float64)\n",
      "59 tensor(21018.2031, dtype=torch.float64)\n",
      "60 tensor(21423.3371, dtype=torch.float64)\n",
      "61 tensor(21781.8232, dtype=torch.float64)\n",
      "62 tensor(22108.1125, dtype=torch.float64)\n",
      "63 tensor(22429.0484, dtype=torch.float64)\n",
      "64 tensor(22761.6933, dtype=torch.float64)\n",
      "65 tensor(23213.2650, dtype=torch.float64)\n",
      "66 tensor(23544.4018, dtype=torch.float64)\n",
      "67 tensor(23857.0502, dtype=torch.float64)\n",
      "68 tensor(24158.8682, dtype=torch.float64)\n",
      "69 tensor(24501.3261, dtype=torch.float64)\n",
      "70 tensor(24957.4558, dtype=torch.float64)\n",
      "71 tensor(25302.3283, dtype=torch.float64)\n",
      "72 tensor(25682.5004, dtype=torch.float64)\n",
      "73 tensor(26035.0231, dtype=torch.float64)\n",
      "74 tensor(26396.9249, dtype=torch.float64)\n",
      "75 tensor(26710.4272, dtype=torch.float64)\n",
      "76 tensor(27086.5615, dtype=torch.float64)\n",
      "77 tensor(27449.6856, dtype=torch.float64)\n",
      "78 tensor(27786.7654, dtype=torch.float64)\n",
      "79 tensor(28161.8398, dtype=torch.float64)\n",
      "80 tensor(28530.1144, dtype=torch.float64)\n",
      "81 tensor(28868.1310, dtype=torch.float64)\n",
      "82 tensor(29190.4646, dtype=torch.float64)\n",
      "83 tensor(29608.7877, dtype=torch.float64)\n",
      "84 tensor(30006.3084, dtype=torch.float64)\n",
      "85 tensor(30352.7216, dtype=torch.float64)\n",
      "86 tensor(30680.2266, dtype=torch.float64)\n",
      "87 tensor(31005.3489, dtype=torch.float64)\n",
      "88 tensor(31370.9743, dtype=torch.float64)\n",
      "89 tensor(31801.6546, dtype=torch.float64)\n",
      "90 tensor(32244.7254, dtype=torch.float64)\n",
      "91 tensor(32669.0571, dtype=torch.float64)\n",
      "92 tensor(33067.2415, dtype=torch.float64)\n",
      "93 tensor(33394.9109, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_flow.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_flow.shape[1]\n",
    "    f_total += cost_fn(Y_pred_flow[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/qpth/qp.py:83: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2910.)\n",
      "  e, _ = torch.eig(Q[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(289.7633, dtype=torch.float64)\n",
      "tensor(629.3350, dtype=torch.float64)\n",
      "tensor(937.4621, dtype=torch.float64)\n",
      "tensor(1242.4037, dtype=torch.float64)\n",
      "tensor(1539.6558, dtype=torch.float64)\n",
      "tensor(1824.8601, dtype=torch.float64)\n",
      "tensor(2110.9020, dtype=torch.float64)\n",
      "tensor(2328.5754, dtype=torch.float64)\n",
      "tensor(2645.5299, dtype=torch.float64)\n",
      "tensor(2951.5173, dtype=torch.float64)\n",
      "tensor(3240.8417, dtype=torch.float64)\n",
      "tensor(3481.8489, dtype=torch.float64)\n",
      "tensor(3837.9704, dtype=torch.float64)\n",
      "tensor(4042.0496, dtype=torch.float64)\n",
      "tensor(4283.3652, dtype=torch.float64)\n",
      "tensor(4554.5071, dtype=torch.float64)\n",
      "tensor(4897.7962, dtype=torch.float64)\n",
      "tensor(5231.1427, dtype=torch.float64)\n",
      "tensor(5496.6919, dtype=torch.float64)\n",
      "tensor(5855.5249, dtype=torch.float64)\n",
      "tensor(6165.3436, dtype=torch.float64)\n",
      "tensor(6462.6735, dtype=torch.float64)\n",
      "tensor(6764.1950, dtype=torch.float64)\n",
      "tensor(7143.3074, dtype=torch.float64)\n",
      "tensor(7493.8135, dtype=torch.float64)\n",
      "tensor(7807.1558, dtype=torch.float64)\n",
      "tensor(8143.6404, dtype=torch.float64)\n",
      "tensor(8495.0124, dtype=torch.float64)\n",
      "tensor(8805.3183, dtype=torch.float64)\n",
      "tensor(9063.8966, dtype=torch.float64)\n",
      "tensor(9429.3031, dtype=torch.float64)\n",
      "tensor(9767.6170, dtype=torch.float64)\n",
      "tensor(10094.7474, dtype=torch.float64)\n",
      "tensor(10416.9667, dtype=torch.float64)\n",
      "tensor(10765.1281, dtype=torch.float64)\n",
      "tensor(11104.8958, dtype=torch.float64)\n",
      "tensor(11433.2364, dtype=torch.float64)\n",
      "tensor(11783.3861, dtype=torch.float64)\n",
      "tensor(12053.9718, dtype=torch.float64)\n",
      "tensor(12405.7969, dtype=torch.float64)\n",
      "tensor(12676.4555, dtype=torch.float64)\n",
      "tensor(13000.0094, dtype=torch.float64)\n",
      "tensor(13285.0028, dtype=torch.float64)\n",
      "tensor(13639.3391, dtype=torch.float64)\n",
      "tensor(13975.5626, dtype=torch.float64)\n",
      "tensor(14419.8913, dtype=torch.float64)\n",
      "tensor(14720.1669, dtype=torch.float64)\n",
      "tensor(15099.1215, dtype=torch.float64)\n",
      "tensor(15403.5221, dtype=torch.float64)\n",
      "tensor(15715.1860, dtype=torch.float64)\n",
      "tensor(16044.1630, dtype=torch.float64)\n",
      "tensor(16355.3034, dtype=torch.float64)\n",
      "tensor(16701.2447, dtype=torch.float64)\n",
      "tensor(16991.6398, dtype=torch.float64)\n",
      "tensor(17247.2554, dtype=torch.float64)\n",
      "tensor(17643.7893, dtype=torch.float64)\n",
      "tensor(18020.5000, dtype=torch.float64)\n",
      "tensor(18408.3018, dtype=torch.float64)\n",
      "tensor(18859.7313, dtype=torch.float64)\n",
      "tensor(19147.8619, dtype=torch.float64)\n",
      "tensor(19419.1213, dtype=torch.float64)\n",
      "tensor(19789.1114, dtype=torch.float64)\n",
      "tensor(20130.4868, dtype=torch.float64)\n",
      "tensor(20433.6916, dtype=torch.float64)\n",
      "tensor(20753.2725, dtype=torch.float64)\n",
      "tensor(21073.6826, dtype=torch.float64)\n",
      "tensor(21411.1993, dtype=torch.float64)\n",
      "tensor(21685.7404, dtype=torch.float64)\n",
      "tensor(22031.9336, dtype=torch.float64)\n",
      "tensor(22318.7514, dtype=torch.float64)\n",
      "tensor(22598.9028, dtype=torch.float64)\n",
      "tensor(22900.0219, dtype=torch.float64)\n",
      "tensor(23214.5850, dtype=torch.float64)\n",
      "tensor(23541.9274, dtype=torch.float64)\n",
      "tensor(23951.6245, dtype=torch.float64)\n",
      "tensor(24289.0150, dtype=torch.float64)\n",
      "tensor(24639.4075, dtype=torch.float64)\n",
      "tensor(24959.0565, dtype=torch.float64)\n",
      "tensor(25329.3023, dtype=torch.float64)\n",
      "tensor(25649.1952, dtype=torch.float64)\n",
      "tensor(25942.7555, dtype=torch.float64)\n",
      "tensor(26326.1144, dtype=torch.float64)\n",
      "tensor(26658.5696, dtype=torch.float64)\n",
      "tensor(27083.2927, dtype=torch.float64)\n",
      "tensor(27367.0108, dtype=torch.float64)\n",
      "tensor(27670.6891, dtype=torch.float64)\n",
      "tensor(28038.0493, dtype=torch.float64)\n",
      "tensor(28381.1410, dtype=torch.float64)\n",
      "tensor(28782.0969, dtype=torch.float64)\n",
      "tensor(29033.5961, dtype=torch.float64)\n",
      "tensor(29403.4319, dtype=torch.float64)\n",
      "tensor(29743.0817, dtype=torch.float64)\n",
      "tensor(30075.6667, dtype=torch.float64)\n",
      "tensor(30483.5357, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_val_original.shape[0]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_val_original.shape[0]\n",
    "    f_total += cost_fn(Y_val_original[i_low:i_up,:].unsqueeze(0), Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1911.3810, 3548.2932, 3548.2932, 3548.2932, 2453.8240, 3548.2932,\n",
       "        3548.2932, 3548.2932, 3548.2932, 3548.2932, 3548.2932, 3548.2932,\n",
       "        3548.2932, 3548.2932, 3548.2932, 3548.2932, 3548.2932, 3548.2932,\n",
       "        3548.2932, 3548.2932, 3548.2932, 3548.2932, 3548.2932, 3548.2932,\n",
       "        3077.1531, 1905.3210, 3548.2932, 3548.2932, 3548.2932, 3548.2932,\n",
       "        3548.2932, 3548.2932, 2699.9140, 3548.2932, 1427.2790, 3548.2932,\n",
       "        3548.2932, 3548.2932, 3455.7500, 3548.2932, 3548.2932, 3548.2932,\n",
       "        3548.2932, 3548.2932, 3548.2932, 3548.2932, 3347.9950, 3135.0530,\n",
       "        3548.2932, 3548.2932], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(argmin_solver(reshape_outcomes(Y_val_original[0:50,:].unsqueeze(0)))*params_t['pr']).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.83 1.64 2.43 28104 27870 27787 13598\n",
    "1.87 1.89 2.46 42607 42070 41574 28124\n",
    "1.80 1.85 2.25 34329 33855 33395 15849\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
