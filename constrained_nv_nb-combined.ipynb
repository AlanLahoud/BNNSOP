{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import data_generator\n",
    "import params_newsvendor as params\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from model import VariationalNet, StandardNet, VariationalNet2\n",
    "\n",
    "from train import TrainDecoupled, TrainCombined\n",
    "\n",
    "from train_normflow import TrainFlowDecoupled, TrainFlowCombined\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = mp.cpu_count()\n",
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')\n",
    "    cpu_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 0\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters (change if necessary)\n",
    "N = 8000 # Total data size\n",
    "N_train = 5000 # Training data size\n",
    "N_SAMPLES = 16 # Sampling size while training (IT HAS TO BE MULTIPLE OF 4)\n",
    "BATCH_SIZE_LOADER = 64 # Standard batch size\n",
    "EPOCHS = 150 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "N_valid = N - N_train\n",
    "X, Y_original = data_generator.data_4to8(N_train, noise_level=nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_multi.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output normalization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Y_original)\n",
    "tmean = torch.tensor(scaler.mean_)\n",
    "tstd = torch.tensor(scaler.scale_)\n",
    "joblib.dump(scaler, 'scaler_multi.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(yy, dev):\n",
    "    return yy*tstd.to(dev) + tmean.to(dev)\n",
    "\n",
    "Y = scaler.transform(Y_original).copy()\n",
    "X = torch.tensor(X, dtype=torch.float32)#.to(dev)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)#.to(dev)\n",
    "Y_original = torch.tensor(Y_original, dtype=torch.float32)#.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_original = data_generator.ArtificialDataset(X, Y_original)\n",
    "training_loader_original = torch.utils.data.DataLoader(\n",
    "    data_train_original, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=cpu_count)\n",
    "   \n",
    "    \n",
    "data_train = data_generator.ArtificialDataset(X, Y)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=cpu_count)\n",
    "    \n",
    "    \n",
    "X_val, Y_val_original = data_generator.data_4to8(N_valid, noise_level=nl)\n",
    "Y_val = scaler.transform(Y_val_original).copy()\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)#.to(dev)\n",
    "Y_val_original = torch.tensor(Y_val_original, dtype=torch.float32)#.to(dev)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)#.to(dev)\n",
    "\n",
    "\n",
    "data_valid_original = data_generator.ArtificialDataset(X_val, Y_val_original)\n",
    "validation_loader_original = torch.utils.data.DataLoader(\n",
    "    data_valid_original, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=cpu_count)\n",
    "\n",
    "    \n",
    "data_valid = data_generator.ArtificialDataset(X_val, Y_val)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    data_valid, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=cpu_count)\n",
    "    \n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t, n_samples):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "        \n",
    "        self.params_t = params_t\n",
    "        \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items  \n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items).to(dev)\n",
    "        ident_samples = torch.eye(n_items*n_samples).to(dev)\n",
    "        ident3 = torch.eye(n_items + 2*n_items*n_samples).to(dev)\n",
    "        zeros_matrix = torch.zeros((n_items*n_samples, n_items*n_samples)).to(dev)\n",
    "        zeros_array = torch.zeros(n_items*n_samples).to(dev)\n",
    "        ones_array = torch.ones(n_items*n_samples).to(dev)\n",
    "             \n",
    "        self.Q = torch.diag(\n",
    "            torch.hstack(\n",
    "                (\n",
    "                    params_t['q'], \n",
    "                    (1/n_samples)*params_t['qs'].repeat_interleave(n_samples), \n",
    "                    (1/n_samples)*params_t['qw'].repeat_interleave(n_samples)\n",
    "                )\n",
    "            )).to(dev)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.hstack(\n",
    "                                (\n",
    "                                    params_t['c'], \n",
    "                                    (1/n_samples)*params_t['cs'].repeat_interleave(n_samples), \n",
    "                                    (1/n_samples)*params_t['cw'].repeat_interleave(n_samples)\n",
    "                                )).to(dev)\n",
    "             \n",
    "            \n",
    "        shortage_ineq = torch.hstack(\n",
    "            (\n",
    "                -ident.repeat_interleave(n_samples, 0), \n",
    "                -ident_samples, \n",
    "                zeros_matrix\n",
    "            )\n",
    "        )  \n",
    "        \n",
    "        \n",
    "        excess_ineq = torch.hstack(\n",
    "            (\n",
    "                ident.repeat_interleave(n_samples, 0), \n",
    "                zeros_matrix, \n",
    "                -ident_samples\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        price_ineq = torch.hstack(\n",
    "            (\n",
    "                params_t['pr'], \n",
    "                zeros_array, \n",
    "                zeros_array\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        \n",
    "        self.ineqs = torch.vstack(\n",
    "            (\n",
    "                shortage_ineq, \n",
    "                excess_ineq, \n",
    "                price_ineq, \n",
    "                positive_ineq\n",
    "            )\n",
    "        ).to(dev)\n",
    " \n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        \n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items*n_samples), \n",
    "                                          torch.zeros(n_items*n_samples))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, n_samples_items = y.size()\n",
    "                \n",
    "        assert self.n_samples*self.n_items == n_samples_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(batch_size, Q.size(0), Q.size(1))\n",
    "        \n",
    "        lin = self.lin\n",
    "        lin = lin.expand(batch_size, lin.size(0))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(batch_size, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            batch_size, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]\n",
    "    \n",
    "    def cost_per_item(self, Z, Y):\n",
    "        return params_t['q']*Z.to(dev)**2 \\\n",
    "            + self.params_t['qs']*(torch.max(torch.zeros((self.n_items)).to(dev),Y.to(dev)-Z.to(dev)))**2 \\\n",
    "            + self.params_t['qw']*(torch.max(torch.zeros((self.n_items)).to(dev),Z.to(dev)-Y.to(dev)))**2 \\\n",
    "            + self.params_t['c']*Z.to(dev) \\\n",
    "            + self.params_t['cs']*torch.max(torch.zeros((self.n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "            + self.params_t['cw']*torch.max(torch.zeros((self.n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "    \n",
    "    def reshape_outcomes(self, y_pred):\n",
    "                \n",
    "        if len(y_pred.shape) == 2:\n",
    "            y_pred = y_pred.unsqueeze(0)\n",
    "\n",
    "        n_samples = y_pred.shape[0]\n",
    "        batch_size = y_pred.shape[1]\n",
    "        #n_items = y_pred.shape[2]\n",
    "        y_pred = y_pred.permute((1, 2, 0)).reshape((batch_size, n_samples*self.n_items))\n",
    "        return y_pred\n",
    "    \n",
    "    def calc_f_por_item(self, y_pred, y):\n",
    "        #pdb.set_trace()\n",
    "        y_pred = self.reshape_outcomes(y_pred)\n",
    "        z_star =  self.forward(y_pred)\n",
    "        f_per_item = self.cost_per_item(z_star, y)\n",
    "        return f_per_item\n",
    "\n",
    "    def calc_f_per_day(self, y_pred, y):\n",
    "        f_per_item = self.calc_f_por_item(y_pred, y)\n",
    "        f = torch.sum(f_per_item, 1)\n",
    "        return f\n",
    "\n",
    "    def end_loss(self, y_pred, y):\n",
    "        f = self.calc_f_per_day(y_pred, y)\n",
    "        f_total = torch.mean(f)\n",
    "        return f_total\n",
    "    \n",
    "    def end_loss_dist(self, y_pred, y):\n",
    "        f = self.calc_f_per_day(y_pred, y)\n",
    "        f_total = torch.mean(f)\n",
    "        return f_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost_per_item = lambda Z, Y : params_t['q'].to(dev)*Z.to(dev)**2 \\\n",
    "                            + params_t['qs'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)))**2 \\\n",
    "                            + params_t['qw'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev)))**2 \\\n",
    "                            + params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "\n",
    "def reshape_outcomes(y_pred):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    batch_size = y_pred.shape[1]\n",
    "    n_items = y_pred.shape[2]\n",
    "\n",
    "    y_pred = y_pred.permute((1, 2, 0)).reshape((batch_size, n_samples*n_items))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred = reshape_outcomes(y_pred)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f\n",
    "\n",
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ann_dec = StandardNet(input_size, output_size, 0).to(dev)\n",
    "h_bnn_dec = VariationalNet2(N_SAMPLES//4, input_size, output_size, 1.0, dev).to(dev)\n",
    "\n",
    "h_ann_com = StandardNet(input_size, output_size, 0).to(dev)\n",
    "h_bnn_com = VariationalNet2(N_SAMPLES//4, input_size, output_size, 1.0, dev).to(dev)\n",
    "\n",
    "opt_h_ann_dec = torch.optim.Adam(h_ann_dec.parameters(), lr=0.001)\n",
    "opt_h_bnn_dec = torch.optim.Adam(h_bnn_dec.parameters(), lr=0.001)\n",
    "\n",
    "opt_h_ann_com = torch.optim.Adam(h_ann_com.parameters(), lr=0.002)\n",
    "opt_h_bnn_com = torch.optim.Adam(h_bnn_com.parameters(), lr=0.001)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = output_size\n",
    "params_t, _ = params.get_params(n_items, seed_number, dev)\n",
    "\n",
    "# Construct the solver\n",
    "cn_constrained = SolveNewsvendorWithKKT(params_t, 1)\n",
    "cn_constrained_dist = SolveNewsvendorWithKKT(params_t, N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 0.554 valid 0.413\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.55 valid 0.41\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 0.334 valid 0.318\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.33 valid 0.32\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.273 valid 0.275\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.27 valid 0.28\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.238 valid 0.247\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.24 valid 0.25\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.219 valid 0.232\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.22 valid 0.23\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.209 valid 0.222\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.21 valid 0.22\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.2 valid 0.214\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.2 valid 0.21\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.193 valid 0.208\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.19 valid 0.21\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.186 valid 0.203\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.19 valid 0.2\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.18 valid 0.197\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.18 valid 0.2\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.174 valid 0.192\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.17 valid 0.19\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.168 valid 0.187\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.17 valid 0.19\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.161 valid 0.181\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.16 valid 0.18\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.154 valid 0.174\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.15 valid 0.17\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.146 valid 0.166\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.15 valid 0.17\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.138 valid 0.158\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.14 valid 0.16\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.13 valid 0.151\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.13 valid 0.15\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.124 valid 0.145\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.15\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.118 valid 0.14\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.14\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.113 valid 0.137\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.14\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.109 valid 0.133\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.13\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.106 valid 0.13\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.13\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.103 valid 0.128\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.1 valid 0.125\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.098 valid 0.122\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.12\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.095 valid 0.121\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.093 valid 0.118\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.091 valid 0.116\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.088 valid 0.114\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.11\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.086 valid 0.112\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.11\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.084 valid 0.11\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.082 valid 0.109\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.081 valid 0.107\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.079 valid 0.106\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.077 valid 0.105\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.076 valid 0.105\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.1\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.075 valid 0.104\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.074 valid 0.103\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.073 valid 0.103\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.072 valid 0.103\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.071 valid 0.102\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.071 valid 0.102\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.07 valid 0.101\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.069 valid 0.1\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.069 valid 0.1\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.068 valid 0.1\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.068 valid 0.099\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.067 valid 0.097\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.066 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.065 valid 0.095\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.065 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.064 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.063 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.063 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.062 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.062 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 57------------------\n",
      "DATA LOSS \t train 0.062 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 58------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.061 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.061 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.06 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.06 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.06 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.059 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.059 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.059 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.058 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.058 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.058 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.058 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.057 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.057 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.057 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.057 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.057 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.056 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.056 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.056 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.056 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.055 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.055 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.055 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.055 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.054 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.054 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.054 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.054 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.053 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.054 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.054 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.055 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.055 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.08\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.056 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.056 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.055 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.053 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.052 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.052 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.051 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.051 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.051 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.05 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.05 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.05 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.05 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.049 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.049 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.049 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.049 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.049 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.048 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.048 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.048 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 113------------------\n",
      "DATA LOSS \t train 0.048 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.048 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 115------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.048 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.048 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.048 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.047 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.047 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.047 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 121------------------\n",
      "DATA LOSS \t train 0.047 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 122------------------\n",
      "DATA LOSS \t train 0.047 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 123------------------\n",
      "DATA LOSS \t train 0.047 valid 0.082\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 124------------------\n",
      "DATA LOSS \t train 0.047 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 125------------------\n",
      "DATA LOSS \t train 0.046 valid 0.082\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 126------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 127------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 128------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 129------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 130------------------\n",
      "DATA LOSS \t train 0.046 valid 0.082\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 131------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 132------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 133------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 134------------------\n",
      "DATA LOSS \t train 0.046 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 135------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 136------------------\n",
      "DATA LOSS \t train 0.046 valid 0.083\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 137------------------\n",
      "DATA LOSS \t train 0.046 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 138------------------\n",
      "DATA LOSS \t train 0.046 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 139------------------\n",
      "DATA LOSS \t train 0.046 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 140------------------\n",
      "DATA LOSS \t train 0.045 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 141------------------\n",
      "DATA LOSS \t train 0.046 valid 0.084\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 142------------------\n",
      "DATA LOSS \t train 0.046 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 143------------------\n",
      "DATA LOSS \t train 0.046 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.08\n",
      "------------------EPOCH 144------------------\n",
      "DATA LOSS \t train 0.046 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 145------------------\n",
      "DATA LOSS \t train 0.046 valid 0.085\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 146------------------\n",
      "DATA LOSS \t train 0.046 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 147------------------\n",
      "DATA LOSS \t train 0.046 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 148------------------\n",
      "DATA LOSS \t train 0.046 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 149------------------\n",
      "DATA LOSS \t train 0.046 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 150------------------\n",
      "DATA LOSS \t train 0.047 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n"
     ]
    }
   ],
   "source": [
    "train_ANN_dec = TrainDecoupled(\n",
    "                    bnn = False,\n",
    "                    model=h_ann_dec,\n",
    "                    opt=opt_h_ann_dec,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=0.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader,\n",
    "                    dev = dev\n",
    "                )\n",
    "\n",
    "train_ANN_dec.train(EPOCHS=EPOCHS)\n",
    "model_ann_dec = train_ANN_dec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/qpth/qp.py:83: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2897.)\n",
      "  e, _ = torch.eig(Q[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END LOSS \t train 38668.544 valid 33850.663\n",
      "------------------EPOCH 2------------------\n",
      "END LOSS \t train 33656.265 valid 32833.036\n",
      "------------------EPOCH 3------------------\n",
      "END LOSS \t train 32835.284 valid 32373.647\n",
      "------------------EPOCH 4------------------\n",
      "END LOSS \t train 32337.158 valid 32051.418\n",
      "------------------EPOCH 5------------------\n",
      "END LOSS \t train 32091.965 valid 31824.321\n",
      "------------------EPOCH 6------------------\n",
      "END LOSS \t train 31915.838 valid 31896.913\n",
      "------------------EPOCH 7------------------\n",
      "END LOSS \t train 31836.862 valid 31660.149\n",
      "------------------EPOCH 8------------------\n",
      "END LOSS \t train 31689.918 valid 31503.081\n",
      "------------------EPOCH 9------------------\n",
      "END LOSS \t train 31523.076 valid 31563.318\n",
      "------------------EPOCH 10------------------\n",
      "END LOSS \t train 31414.446 valid 31632.473\n",
      "------------------EPOCH 11------------------\n",
      "END LOSS \t train 31287.764 valid 30933.833\n",
      "------------------EPOCH 12------------------\n",
      "END LOSS \t train 31094.031 valid 31517.155\n",
      "------------------EPOCH 13------------------\n",
      "END LOSS \t train 31109.518 valid 30679.244\n",
      "------------------EPOCH 14------------------\n",
      "END LOSS \t train 30854.584 valid 30918.722\n",
      "------------------EPOCH 15------------------\n",
      "END LOSS \t train 30807.088 valid 30686.813\n",
      "------------------EPOCH 16------------------\n",
      "END LOSS \t train 30770.987 valid 30568.896\n",
      "------------------EPOCH 17------------------\n",
      "END LOSS \t train 30680.206 valid 30634.172\n",
      "------------------EPOCH 18------------------\n",
      "END LOSS \t train 30646.279 valid 30583.93\n",
      "------------------EPOCH 19------------------\n",
      "END LOSS \t train 30591.249 valid 30556.565\n",
      "------------------EPOCH 20------------------\n",
      "END LOSS \t train 30519.212 valid 30678.129\n",
      "------------------EPOCH 21------------------\n",
      "END LOSS \t train 30539.346 valid 30509.087\n",
      "------------------EPOCH 22------------------\n",
      "END LOSS \t train 30464.187 valid 30301.265\n",
      "------------------EPOCH 23------------------\n",
      "END LOSS \t train 30402.486 valid 30296.526\n",
      "------------------EPOCH 24------------------\n",
      "END LOSS \t train 30367.556 valid 30686.105\n",
      "------------------EPOCH 25------------------\n",
      "END LOSS \t train 30463.924 valid 30379.414\n",
      "------------------EPOCH 26------------------\n",
      "END LOSS \t train 30383.857 valid 30494.884\n",
      "------------------EPOCH 27------------------\n",
      "END LOSS \t train 30381.128 valid 30383.317\n",
      "------------------EPOCH 28------------------\n",
      "END LOSS \t train 30360.148 valid 30367.06\n",
      "------------------EPOCH 29------------------\n",
      "END LOSS \t train 30338.809 valid 30399.407\n",
      "------------------EPOCH 30------------------\n",
      "END LOSS \t train 30284.571 valid 30285.819\n",
      "------------------EPOCH 31------------------\n",
      "END LOSS \t train 30295.297 valid 30534.268\n",
      "------------------EPOCH 32------------------\n",
      "END LOSS \t train 30339.778 valid 30230.113\n",
      "------------------EPOCH 33------------------\n",
      "END LOSS \t train 30267.23 valid 30225.067\n",
      "------------------EPOCH 34------------------\n",
      "END LOSS \t train 30229.346 valid 30290.939\n",
      "------------------EPOCH 35------------------\n",
      "END LOSS \t train 30208.375 valid 30282.422\n",
      "------------------EPOCH 36------------------\n",
      "END LOSS \t train 30256.862 valid 30211.354\n",
      "------------------EPOCH 37------------------\n",
      "END LOSS \t train 30226.926 valid 30575.92\n",
      "------------------EPOCH 38------------------\n",
      "END LOSS \t train 30234.522 valid 30340.527\n",
      "------------------EPOCH 39------------------\n",
      "END LOSS \t train 30211.794 valid 30134.205\n",
      "------------------EPOCH 40------------------\n",
      "END LOSS \t train 30170.695 valid 30403.893\n",
      "------------------EPOCH 41------------------\n",
      "END LOSS \t train 30216.629 valid 30192.415\n",
      "------------------EPOCH 42------------------\n",
      "END LOSS \t train 30150.425 valid 30488.574\n",
      "------------------EPOCH 43------------------\n",
      "END LOSS \t train 30231.253 valid 30092.139\n",
      "------------------EPOCH 44------------------\n",
      "END LOSS \t train 30170.818 valid 30607.487\n",
      "------------------EPOCH 45------------------\n",
      "END LOSS \t train 30294.983 valid 30186.167\n",
      "------------------EPOCH 46------------------\n",
      "END LOSS \t train 30183.582 valid 30314.191\n",
      "------------------EPOCH 47------------------\n",
      "END LOSS \t train 30189.104 valid 30183.0\n",
      "------------------EPOCH 48------------------\n",
      "END LOSS \t train 30186.611 valid 30406.562\n",
      "------------------EPOCH 49------------------\n",
      "END LOSS \t train 30323.901 valid 30164.889\n",
      "------------------EPOCH 50------------------\n",
      "END LOSS \t train 30192.94 valid 30079.254\n"
     ]
    }
   ],
   "source": [
    "train_ANN_com = TrainCombined(\n",
    "                    bnn = False,\n",
    "                    model=h_ann_com,\n",
    "                    opt=opt_h_ann_com,\n",
    "                    training_loader=training_loader_original,\n",
    "                    validation_loader=validation_loader_original,\n",
    "                    OP = cn_constrained,\n",
    "                    dev = dev\n",
    "                )\n",
    "\n",
    "train_ANN_com.train(EPOCHS=50)\n",
    "model_ann_com = train_ANN_com.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 2.488 valid 1.066\n",
      "KL LOSS \t train 5.62 valid 5.62\n",
      "ELBO LOSS \t train 8.11 valid 6.69\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 1.015 valid 0.95\n",
      "KL LOSS \t train 5.61 valid 5.6\n",
      "ELBO LOSS \t train 6.63 valid 6.55\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.921 valid 0.867\n",
      "KL LOSS \t train 5.61 valid 5.61\n",
      "ELBO LOSS \t train 6.53 valid 6.47\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.83 valid 0.784\n",
      "KL LOSS \t train 5.6 valid 5.6\n",
      "ELBO LOSS \t train 6.43 valid 6.38\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.733 valid 0.693\n",
      "KL LOSS \t train 5.59 valid 5.59\n",
      "ELBO LOSS \t train 6.33 valid 6.28\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.633 valid 0.61\n",
      "KL LOSS \t train 5.59 valid 5.59\n",
      "ELBO LOSS \t train 6.22 valid 6.2\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.555 valid 0.551\n",
      "KL LOSS \t train 5.58 valid 5.57\n",
      "ELBO LOSS \t train 6.14 valid 6.12\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.501 valid 0.504\n",
      "KL LOSS \t train 5.58 valid 5.57\n",
      "ELBO LOSS \t train 6.08 valid 6.07\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.456 valid 0.467\n",
      "KL LOSS \t train 5.57 valid 5.57\n",
      "ELBO LOSS \t train 6.03 valid 6.04\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.419 valid 0.435\n",
      "KL LOSS \t train 5.57 valid 5.56\n",
      "ELBO LOSS \t train 5.99 valid 6.0\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.391 valid 0.408\n",
      "KL LOSS \t train 5.56 valid 5.56\n",
      "ELBO LOSS \t train 5.95 valid 5.97\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.366 valid 0.389\n",
      "KL LOSS \t train 5.56 valid 5.56\n",
      "ELBO LOSS \t train 5.92 valid 5.95\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.343 valid 0.364\n",
      "KL LOSS \t train 5.55 valid 5.55\n",
      "ELBO LOSS \t train 5.9 valid 5.92\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.323 valid 0.342\n",
      "KL LOSS \t train 5.55 valid 5.54\n",
      "ELBO LOSS \t train 5.87 valid 5.89\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.305 valid 0.323\n",
      "KL LOSS \t train 5.54 valid 5.54\n",
      "ELBO LOSS \t train 5.85 valid 5.86\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.289 valid 0.313\n",
      "KL LOSS \t train 5.54 valid 5.53\n",
      "ELBO LOSS \t train 5.83 valid 5.85\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.277 valid 0.299\n",
      "KL LOSS \t train 5.53 valid 5.53\n",
      "ELBO LOSS \t train 5.81 valid 5.83\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.266 valid 0.292\n",
      "KL LOSS \t train 5.53 valid 5.53\n",
      "ELBO LOSS \t train 5.8 valid 5.82\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.256 valid 0.28\n",
      "KL LOSS \t train 5.52 valid 5.53\n",
      "ELBO LOSS \t train 5.78 valid 5.81\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.249 valid 0.28\n",
      "KL LOSS \t train 5.52 valid 5.52\n",
      "ELBO LOSS \t train 5.77 valid 5.8\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.244 valid 0.272\n",
      "KL LOSS \t train 5.52 valid 5.52\n",
      "ELBO LOSS \t train 5.76 valid 5.79\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.238 valid 0.262\n",
      "KL LOSS \t train 5.51 valid 5.51\n",
      "ELBO LOSS \t train 5.75 valid 5.77\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.233 valid 0.261\n",
      "KL LOSS \t train 5.5 valid 5.5\n",
      "ELBO LOSS \t train 5.74 valid 5.76\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.229 valid 0.26\n",
      "KL LOSS \t train 5.5 valid 5.5\n",
      "ELBO LOSS \t train 5.73 valid 5.76\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.225 valid 0.253\n",
      "KL LOSS \t train 5.5 valid 5.49\n",
      "ELBO LOSS \t train 5.72 valid 5.75\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.22 valid 0.253\n",
      "KL LOSS \t train 5.49 valid 5.48\n",
      "ELBO LOSS \t train 5.71 valid 5.74\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.217 valid 0.25\n",
      "KL LOSS \t train 5.48 valid 5.48\n",
      "ELBO LOSS \t train 5.7 valid 5.73\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.214 valid 0.245\n",
      "KL LOSS \t train 5.48 valid 5.47\n",
      "ELBO LOSS \t train 5.69 valid 5.72\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.212 valid 0.244\n",
      "KL LOSS \t train 5.47 valid 5.47\n",
      "ELBO LOSS \t train 5.68 valid 5.71\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.209 valid 0.237\n",
      "KL LOSS \t train 5.47 valid 5.46\n",
      "ELBO LOSS \t train 5.67 valid 5.7\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.205 valid 0.235\n",
      "KL LOSS \t train 5.46 valid 5.46\n",
      "ELBO LOSS \t train 5.66 valid 5.69\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.202 valid 0.231\n",
      "KL LOSS \t train 5.45 valid 5.45\n",
      "ELBO LOSS \t train 5.66 valid 5.68\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.2 valid 0.227\n",
      "KL LOSS \t train 5.45 valid 5.44\n",
      "ELBO LOSS \t train 5.65 valid 5.67\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.197 valid 0.229\n",
      "KL LOSS \t train 5.44 valid 5.43\n",
      "ELBO LOSS \t train 5.64 valid 5.66\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.193 valid 0.225\n",
      "KL LOSS \t train 5.44 valid 5.43\n",
      "ELBO LOSS \t train 5.63 valid 5.66\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.191 valid 0.219\n",
      "KL LOSS \t train 5.43 valid 5.43\n",
      "ELBO LOSS \t train 5.62 valid 5.65\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.187 valid 0.214\n",
      "KL LOSS \t train 5.42 valid 5.42\n",
      "ELBO LOSS \t train 5.61 valid 5.64\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.185 valid 0.211\n",
      "KL LOSS \t train 5.42 valid 5.41\n",
      "ELBO LOSS \t train 5.6 valid 5.62\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.181 valid 0.207\n",
      "KL LOSS \t train 5.41 valid 5.41\n",
      "ELBO LOSS \t train 5.59 valid 5.62\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.178 valid 0.206\n",
      "KL LOSS \t train 5.41 valid 5.4\n",
      "ELBO LOSS \t train 5.59 valid 5.61\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.173 valid 0.199\n",
      "KL LOSS \t train 5.4 valid 5.4\n",
      "ELBO LOSS \t train 5.58 valid 5.6\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.169 valid 0.198\n",
      "KL LOSS \t train 5.4 valid 5.4\n",
      "ELBO LOSS \t train 5.57 valid 5.59\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.168 valid 0.194\n",
      "KL LOSS \t train 5.39 valid 5.39\n",
      "ELBO LOSS \t train 5.56 valid 5.58\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.165 valid 0.188\n",
      "KL LOSS \t train 5.39 valid 5.38\n",
      "ELBO LOSS \t train 5.55 valid 5.57\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.16 valid 0.186\n",
      "KL LOSS \t train 5.38 valid 5.38\n",
      "ELBO LOSS \t train 5.54 valid 5.57\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.157 valid 0.182\n",
      "KL LOSS \t train 5.38 valid 5.37\n",
      "ELBO LOSS \t train 5.53 valid 5.55\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.154 valid 0.18\n",
      "KL LOSS \t train 5.37 valid 5.36\n",
      "ELBO LOSS \t train 5.52 valid 5.54\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.153 valid 0.18\n",
      "KL LOSS \t train 5.36 valid 5.36\n",
      "ELBO LOSS \t train 5.52 valid 5.54\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.15 valid 0.175\n",
      "KL LOSS \t train 5.36 valid 5.35\n",
      "ELBO LOSS \t train 5.51 valid 5.53\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.149 valid 0.171\n",
      "KL LOSS \t train 5.36 valid 5.36\n",
      "ELBO LOSS \t train 5.5 valid 5.53\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.146 valid 0.173\n",
      "KL LOSS \t train 5.35 valid 5.34\n",
      "ELBO LOSS \t train 5.5 valid 5.51\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.144 valid 0.169\n",
      "KL LOSS \t train 5.34 valid 5.34\n",
      "ELBO LOSS \t train 5.49 valid 5.51\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.144 valid 0.17\n",
      "KL LOSS \t train 5.34 valid 5.34\n",
      "ELBO LOSS \t train 5.48 valid 5.51\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.14 valid 0.168\n",
      "KL LOSS \t train 5.33 valid 5.33\n",
      "ELBO LOSS \t train 5.47 valid 5.5\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.14 valid 0.164\n",
      "KL LOSS \t train 5.33 valid 5.33\n",
      "ELBO LOSS \t train 5.47 valid 5.49\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.137 valid 0.163\n",
      "KL LOSS \t train 5.32 valid 5.31\n",
      "ELBO LOSS \t train 5.46 valid 5.48\n",
      "------------------EPOCH 57------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.136 valid 0.162\n",
      "KL LOSS \t train 5.32 valid 5.31\n",
      "ELBO LOSS \t train 5.45 valid 5.48\n",
      "------------------EPOCH 58------------------\n",
      "DATA LOSS \t train 0.136 valid 0.159\n",
      "KL LOSS \t train 5.31 valid 5.31\n",
      "ELBO LOSS \t train 5.45 valid 5.47\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.131 valid 0.157\n",
      "KL LOSS \t train 5.31 valid 5.31\n",
      "ELBO LOSS \t train 5.44 valid 5.46\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.132 valid 0.155\n",
      "KL LOSS \t train 5.3 valid 5.3\n",
      "ELBO LOSS \t train 5.43 valid 5.46\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.131 valid 0.154\n",
      "KL LOSS \t train 5.29 valid 5.29\n",
      "ELBO LOSS \t train 5.42 valid 5.45\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.129 valid 0.153\n",
      "KL LOSS \t train 5.29 valid 5.29\n",
      "ELBO LOSS \t train 5.42 valid 5.44\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.129 valid 0.154\n",
      "KL LOSS \t train 5.28 valid 5.27\n",
      "ELBO LOSS \t train 5.41 valid 5.43\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.128 valid 0.151\n",
      "KL LOSS \t train 5.28 valid 5.28\n",
      "ELBO LOSS \t train 5.4 valid 5.43\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.127 valid 0.149\n",
      "KL LOSS \t train 5.27 valid 5.26\n",
      "ELBO LOSS \t train 5.4 valid 5.41\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.126 valid 0.151\n",
      "KL LOSS \t train 5.27 valid 5.26\n",
      "ELBO LOSS \t train 5.39 valid 5.41\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.124 valid 0.151\n",
      "KL LOSS \t train 5.26 valid 5.26\n",
      "ELBO LOSS \t train 5.38 valid 5.41\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.125 valid 0.149\n",
      "KL LOSS \t train 5.25 valid 5.25\n",
      "ELBO LOSS \t train 5.38 valid 5.4\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.123 valid 0.146\n",
      "KL LOSS \t train 5.25 valid 5.25\n",
      "ELBO LOSS \t train 5.37 valid 5.4\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.122 valid 0.146\n",
      "KL LOSS \t train 5.24 valid 5.24\n",
      "ELBO LOSS \t train 5.36 valid 5.39\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.123 valid 0.145\n",
      "KL LOSS \t train 5.24 valid 5.24\n",
      "ELBO LOSS \t train 5.36 valid 5.38\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.121 valid 0.148\n",
      "KL LOSS \t train 5.23 valid 5.22\n",
      "ELBO LOSS \t train 5.35 valid 5.37\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.121 valid 0.145\n",
      "KL LOSS \t train 5.23 valid 5.23\n",
      "ELBO LOSS \t train 5.35 valid 5.37\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.12 valid 0.142\n",
      "KL LOSS \t train 5.22 valid 5.22\n",
      "ELBO LOSS \t train 5.34 valid 5.37\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.119 valid 0.144\n",
      "KL LOSS \t train 5.22 valid 5.22\n",
      "ELBO LOSS \t train 5.34 valid 5.36\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.118 valid 0.139\n",
      "KL LOSS \t train 5.21 valid 5.21\n",
      "ELBO LOSS \t train 5.33 valid 5.35\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.116 valid 0.141\n",
      "KL LOSS \t train 5.2 valid 5.2\n",
      "ELBO LOSS \t train 5.32 valid 5.34\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.116 valid 0.139\n",
      "KL LOSS \t train 5.2 valid 5.19\n",
      "ELBO LOSS \t train 5.32 valid 5.33\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.115 valid 0.141\n",
      "KL LOSS \t train 5.19 valid 5.19\n",
      "ELBO LOSS \t train 5.31 valid 5.33\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.115 valid 0.139\n",
      "KL LOSS \t train 5.19 valid 5.18\n",
      "ELBO LOSS \t train 5.3 valid 5.32\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.113 valid 0.141\n",
      "KL LOSS \t train 5.18 valid 5.19\n",
      "ELBO LOSS \t train 5.3 valid 5.33\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.113 valid 0.137\n",
      "KL LOSS \t train 5.18 valid 5.17\n",
      "ELBO LOSS \t train 5.29 valid 5.31\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.113 valid 0.138\n",
      "KL LOSS \t train 5.17 valid 5.17\n",
      "ELBO LOSS \t train 5.28 valid 5.31\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.112 valid 0.137\n",
      "KL LOSS \t train 5.17 valid 5.16\n",
      "ELBO LOSS \t train 5.28 valid 5.3\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.112 valid 0.136\n",
      "KL LOSS \t train 5.16 valid 5.16\n",
      "ELBO LOSS \t train 5.27 valid 5.29\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.111 valid 0.136\n",
      "KL LOSS \t train 5.15 valid 5.15\n",
      "ELBO LOSS \t train 5.26 valid 5.29\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.111 valid 0.135\n",
      "KL LOSS \t train 5.15 valid 5.15\n",
      "ELBO LOSS \t train 5.26 valid 5.29\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.109 valid 0.131\n",
      "KL LOSS \t train 5.14 valid 5.14\n",
      "ELBO LOSS \t train 5.25 valid 5.27\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.109 valid 0.133\n",
      "KL LOSS \t train 5.14 valid 5.14\n",
      "ELBO LOSS \t train 5.25 valid 5.27\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.11 valid 0.133\n",
      "KL LOSS \t train 5.13 valid 5.13\n",
      "ELBO LOSS \t train 5.24 valid 5.26\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.109 valid 0.134\n",
      "KL LOSS \t train 5.13 valid 5.12\n",
      "ELBO LOSS \t train 5.24 valid 5.26\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.108 valid 0.132\n",
      "KL LOSS \t train 5.12 valid 5.11\n",
      "ELBO LOSS \t train 5.23 valid 5.25\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.108 valid 0.133\n",
      "KL LOSS \t train 5.12 valid 5.11\n",
      "ELBO LOSS \t train 5.22 valid 5.24\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.107 valid 0.131\n",
      "KL LOSS \t train 5.11 valid 5.1\n",
      "ELBO LOSS \t train 5.22 valid 5.24\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.105 valid 0.129\n",
      "KL LOSS \t train 5.1 valid 5.1\n",
      "ELBO LOSS \t train 5.21 valid 5.23\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.106 valid 0.129\n",
      "KL LOSS \t train 5.1 valid 5.1\n",
      "ELBO LOSS \t train 5.2 valid 5.23\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.105 valid 0.129\n",
      "KL LOSS \t train 5.09 valid 5.09\n",
      "ELBO LOSS \t train 5.2 valid 5.22\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.106 valid 0.129\n",
      "KL LOSS \t train 5.09 valid 5.08\n",
      "ELBO LOSS \t train 5.19 valid 5.21\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.105 valid 0.129\n",
      "KL LOSS \t train 5.08 valid 5.07\n",
      "ELBO LOSS \t train 5.19 valid 5.2\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.104 valid 0.128\n",
      "KL LOSS \t train 5.08 valid 5.08\n",
      "ELBO LOSS \t train 5.18 valid 5.21\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.104 valid 0.126\n",
      "KL LOSS \t train 5.07 valid 5.07\n",
      "ELBO LOSS \t train 5.18 valid 5.19\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.104 valid 0.128\n",
      "KL LOSS \t train 5.07 valid 5.07\n",
      "ELBO LOSS \t train 5.17 valid 5.19\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.103 valid 0.126\n",
      "KL LOSS \t train 5.06 valid 5.06\n",
      "ELBO LOSS \t train 5.16 valid 5.19\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.103 valid 0.125\n",
      "KL LOSS \t train 5.06 valid 5.05\n",
      "ELBO LOSS \t train 5.16 valid 5.18\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.102 valid 0.127\n",
      "KL LOSS \t train 5.05 valid 5.04\n",
      "ELBO LOSS \t train 5.15 valid 5.17\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.102 valid 0.124\n",
      "KL LOSS \t train 5.05 valid 5.04\n",
      "ELBO LOSS \t train 5.15 valid 5.17\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.101 valid 0.124\n",
      "KL LOSS \t train 5.04 valid 5.03\n",
      "ELBO LOSS \t train 5.14 valid 5.16\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.101 valid 0.124\n",
      "KL LOSS \t train 5.04 valid 5.02\n",
      "ELBO LOSS \t train 5.14 valid 5.15\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.101 valid 0.124\n",
      "KL LOSS \t train 5.03 valid 5.03\n",
      "ELBO LOSS \t train 5.13 valid 5.16\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.1 valid 0.123\n",
      "KL LOSS \t train 5.03 valid 5.02\n",
      "ELBO LOSS \t train 5.13 valid 5.14\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.099 valid 0.124\n",
      "KL LOSS \t train 5.02 valid 5.03\n",
      "ELBO LOSS \t train 5.12 valid 5.15\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.099 valid 0.123\n",
      "KL LOSS \t train 5.01 valid 5.0\n",
      "ELBO LOSS \t train 5.11 valid 5.13\n",
      "------------------EPOCH 113------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.099 valid 0.123\n",
      "KL LOSS \t train 5.01 valid 5.01\n",
      "ELBO LOSS \t train 5.11 valid 5.13\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.101 valid 0.122\n",
      "KL LOSS \t train 5.01 valid 5.0\n",
      "ELBO LOSS \t train 5.11 valid 5.12\n",
      "------------------EPOCH 115------------------\n",
      "DATA LOSS \t train 0.099 valid 0.123\n",
      "KL LOSS \t train 5.0 valid 5.0\n",
      "ELBO LOSS \t train 5.1 valid 5.12\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.097 valid 0.122\n",
      "KL LOSS \t train 4.99 valid 4.99\n",
      "ELBO LOSS \t train 5.09 valid 5.11\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.098 valid 0.12\n",
      "KL LOSS \t train 4.99 valid 4.99\n",
      "ELBO LOSS \t train 5.09 valid 5.11\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.096 valid 0.119\n",
      "KL LOSS \t train 4.98 valid 4.98\n",
      "ELBO LOSS \t train 5.08 valid 5.1\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.096 valid 0.121\n",
      "KL LOSS \t train 4.98 valid 4.97\n",
      "ELBO LOSS \t train 5.08 valid 5.1\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.097 valid 0.119\n",
      "KL LOSS \t train 4.97 valid 4.97\n",
      "ELBO LOSS \t train 5.07 valid 5.09\n",
      "------------------EPOCH 121------------------\n",
      "DATA LOSS \t train 0.095 valid 0.122\n",
      "KL LOSS \t train 4.97 valid 4.96\n",
      "ELBO LOSS \t train 5.07 valid 5.08\n",
      "------------------EPOCH 122------------------\n",
      "DATA LOSS \t train 0.095 valid 0.12\n",
      "KL LOSS \t train 4.97 valid 4.97\n",
      "ELBO LOSS \t train 5.06 valid 5.09\n",
      "------------------EPOCH 123------------------\n",
      "DATA LOSS \t train 0.094 valid 0.119\n",
      "KL LOSS \t train 4.96 valid 4.95\n",
      "ELBO LOSS \t train 5.05 valid 5.07\n",
      "------------------EPOCH 124------------------\n",
      "DATA LOSS \t train 0.095 valid 0.119\n",
      "KL LOSS \t train 4.96 valid 4.95\n",
      "ELBO LOSS \t train 5.05 valid 5.07\n",
      "------------------EPOCH 125------------------\n",
      "DATA LOSS \t train 0.095 valid 0.118\n",
      "KL LOSS \t train 4.95 valid 4.95\n",
      "ELBO LOSS \t train 5.05 valid 5.07\n",
      "------------------EPOCH 126------------------\n",
      "DATA LOSS \t train 0.094 valid 0.118\n",
      "KL LOSS \t train 4.95 valid 4.94\n",
      "ELBO LOSS \t train 5.04 valid 5.06\n",
      "------------------EPOCH 127------------------\n",
      "DATA LOSS \t train 0.094 valid 0.116\n",
      "KL LOSS \t train 4.94 valid 4.94\n",
      "ELBO LOSS \t train 5.04 valid 5.05\n",
      "------------------EPOCH 128------------------\n",
      "DATA LOSS \t train 0.093 valid 0.117\n",
      "KL LOSS \t train 4.94 valid 4.94\n",
      "ELBO LOSS \t train 5.03 valid 5.05\n",
      "------------------EPOCH 129------------------\n",
      "DATA LOSS \t train 0.093 valid 0.117\n",
      "KL LOSS \t train 4.93 valid 4.93\n",
      "ELBO LOSS \t train 5.02 valid 5.05\n",
      "------------------EPOCH 130------------------\n",
      "DATA LOSS \t train 0.094 valid 0.118\n",
      "KL LOSS \t train 4.93 valid 4.92\n",
      "ELBO LOSS \t train 5.02 valid 5.04\n",
      "------------------EPOCH 131------------------\n",
      "DATA LOSS \t train 0.092 valid 0.116\n",
      "KL LOSS \t train 4.93 valid 4.93\n",
      "ELBO LOSS \t train 5.02 valid 5.04\n",
      "------------------EPOCH 132------------------\n",
      "DATA LOSS \t train 0.092 valid 0.116\n",
      "KL LOSS \t train 4.92 valid 4.93\n",
      "ELBO LOSS \t train 5.01 valid 5.04\n",
      "------------------EPOCH 133------------------\n",
      "DATA LOSS \t train 0.092 valid 0.115\n",
      "KL LOSS \t train 4.92 valid 4.92\n",
      "ELBO LOSS \t train 5.01 valid 5.04\n",
      "------------------EPOCH 134------------------\n",
      "DATA LOSS \t train 0.092 valid 0.114\n",
      "KL LOSS \t train 4.91 valid 4.91\n",
      "ELBO LOSS \t train 5.0 valid 5.02\n",
      "------------------EPOCH 135------------------\n",
      "DATA LOSS \t train 0.091 valid 0.116\n",
      "KL LOSS \t train 4.91 valid 4.91\n",
      "ELBO LOSS \t train 5.0 valid 5.03\n",
      "------------------EPOCH 136------------------\n",
      "DATA LOSS \t train 0.092 valid 0.113\n",
      "KL LOSS \t train 4.9 valid 4.91\n",
      "ELBO LOSS \t train 5.0 valid 5.02\n",
      "------------------EPOCH 137------------------\n",
      "DATA LOSS \t train 0.09 valid 0.113\n",
      "KL LOSS \t train 4.9 valid 4.9\n",
      "ELBO LOSS \t train 4.99 valid 5.01\n",
      "------------------EPOCH 138------------------\n",
      "DATA LOSS \t train 0.091 valid 0.113\n",
      "KL LOSS \t train 4.9 valid 4.9\n",
      "ELBO LOSS \t train 4.99 valid 5.01\n",
      "------------------EPOCH 139------------------\n",
      "DATA LOSS \t train 0.09 valid 0.112\n",
      "KL LOSS \t train 4.89 valid 4.89\n",
      "ELBO LOSS \t train 4.98 valid 5.0\n",
      "------------------EPOCH 140------------------\n",
      "DATA LOSS \t train 0.089 valid 0.114\n",
      "KL LOSS \t train 4.89 valid 4.89\n",
      "ELBO LOSS \t train 4.98 valid 5.01\n",
      "------------------EPOCH 141------------------\n",
      "DATA LOSS \t train 0.091 valid 0.112\n",
      "KL LOSS \t train 4.88 valid 4.89\n",
      "ELBO LOSS \t train 4.97 valid 5.0\n",
      "------------------EPOCH 142------------------\n",
      "DATA LOSS \t train 0.089 valid 0.113\n",
      "KL LOSS \t train 4.88 valid 4.88\n",
      "ELBO LOSS \t train 4.97 valid 5.0\n",
      "------------------EPOCH 143------------------\n",
      "DATA LOSS \t train 0.09 valid 0.112\n",
      "KL LOSS \t train 4.88 valid 4.88\n",
      "ELBO LOSS \t train 4.97 valid 4.99\n",
      "------------------EPOCH 144------------------\n",
      "DATA LOSS \t train 0.089 valid 0.111\n",
      "KL LOSS \t train 4.87 valid 4.87\n",
      "ELBO LOSS \t train 4.96 valid 4.98\n",
      "------------------EPOCH 145------------------\n",
      "DATA LOSS \t train 0.088 valid 0.11\n",
      "KL LOSS \t train 4.87 valid 4.88\n",
      "ELBO LOSS \t train 4.96 valid 4.99\n",
      "------------------EPOCH 146------------------\n",
      "DATA LOSS \t train 0.088 valid 0.112\n",
      "KL LOSS \t train 4.87 valid 4.87\n",
      "ELBO LOSS \t train 4.95 valid 4.98\n",
      "------------------EPOCH 147------------------\n",
      "DATA LOSS \t train 0.088 valid 0.111\n",
      "KL LOSS \t train 4.86 valid 4.86\n",
      "ELBO LOSS \t train 4.95 valid 4.97\n",
      "------------------EPOCH 148------------------\n",
      "DATA LOSS \t train 0.088 valid 0.11\n",
      "KL LOSS \t train 4.86 valid 4.86\n",
      "ELBO LOSS \t train 4.95 valid 4.97\n",
      "------------------EPOCH 149------------------\n",
      "DATA LOSS \t train 0.088 valid 0.111\n",
      "KL LOSS \t train 4.86 valid 4.85\n",
      "ELBO LOSS \t train 4.94 valid 4.96\n",
      "------------------EPOCH 150------------------\n",
      "DATA LOSS \t train 0.088 valid 0.111\n",
      "KL LOSS \t train 4.85 valid 4.85\n",
      "ELBO LOSS \t train 4.94 valid 4.96\n"
     ]
    }
   ],
   "source": [
    "train_BNN_dec = TrainDecoupled(\n",
    "                    bnn = True,\n",
    "                    model=h_bnn_dec,\n",
    "                    opt=opt_h_bnn_dec,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=1.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader,\n",
    "                    dev = dev\n",
    "                )\n",
    "\n",
    "train_BNN_dec.train(EPOCHS=EPOCHS)\n",
    "model_bnn_dec = train_BNN_dec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "END LOSS \t train 343109.231 valid 273548.51\n",
      "------------------EPOCH 2------------------\n",
      "END LOSS \t train 264546.503 valid 260245.577\n",
      "------------------EPOCH 3------------------\n",
      "END LOSS \t train 254964.327 valid 253438.243\n",
      "------------------EPOCH 4------------------\n",
      "END LOSS \t train 249598.655 valid 250568.998\n",
      "------------------EPOCH 5------------------\n",
      "END LOSS \t train 246518.785 valid 247893.442\n",
      "------------------EPOCH 6------------------\n",
      "END LOSS \t train 244491.072 valid 246701.143\n",
      "------------------EPOCH 7------------------\n",
      "END LOSS \t train 242664.275 valid 244365.795\n",
      "------------------EPOCH 8------------------\n",
      "END LOSS \t train 241515.054 valid 242944.842\n",
      "------------------EPOCH 9------------------\n",
      "END LOSS \t train 240373.171 valid 242390.365\n",
      "------------------EPOCH 10------------------\n",
      "END LOSS \t train 239699.826 valid 242245.968\n",
      "------------------EPOCH 11------------------\n",
      "END LOSS \t train 239112.596 valid 241497.627\n",
      "------------------EPOCH 12------------------\n",
      "END LOSS \t train 238264.802 valid 240496.165\n",
      "------------------EPOCH 13------------------\n",
      "END LOSS \t train 237619.023 valid 240258.103\n",
      "------------------EPOCH 14------------------\n",
      "END LOSS \t train 237231.798 valid 239939.885\n",
      "------------------EPOCH 15------------------\n",
      "END LOSS \t train 236881.233 valid 239311.193\n",
      "------------------EPOCH 16------------------\n",
      "END LOSS \t train 236009.269 valid 238935.093\n",
      "------------------EPOCH 17------------------\n",
      "END LOSS \t train 235900.601 valid 238781.092\n",
      "------------------EPOCH 18------------------\n",
      "END LOSS \t train 235364.264 valid 237893.993\n",
      "------------------EPOCH 19------------------\n",
      "END LOSS \t train 234831.574 valid 237890.893\n",
      "------------------EPOCH 20------------------\n",
      "END LOSS \t train 234699.109 valid 237564.706\n"
     ]
    }
   ],
   "source": [
    "train_BNN_com = TrainCombined(\n",
    "                    bnn = True,\n",
    "                    model=h_bnn_com,\n",
    "                    opt=opt_h_bnn_com,\n",
    "                    training_loader=training_loader_original,\n",
    "                    validation_loader=validation_loader_original,\n",
    "                    OP = cn_constrained_dist,\n",
    "                    dev=dev\n",
    "                )\n",
    "\n",
    "train_BNN_com.train(EPOCHS=20)\n",
    "model_bnn_com = train_BNN_com.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 5.35376, val loss: 5.26884\n",
      "step: 50, train loss: 4.01875, val loss: 4.00326\n",
      "step: 100, train loss: 3.25858, val loss: 3.30947\n",
      "step: 150, train loss: 2.81578, val loss: 2.89016\n",
      "step: 200, train loss: 2.526, val loss: 2.61903\n",
      "step: 250, train loss: 2.31568, val loss: 2.43951\n",
      "step: 300, train loss: 2.18478, val loss: 2.29381\n",
      "step: 350, train loss: 2.03493, val loss: 2.17975\n",
      "step: 400, train loss: 1.9522, val loss: 2.09662\n",
      "step: 450, train loss: 1.89116, val loss: 2.02781\n",
      "step: 500, train loss: 1.82802, val loss: 1.9799\n",
      "step: 550, train loss: 1.77131, val loss: 1.92838\n",
      "step: 600, train loss: 1.71991, val loss: 1.88957\n",
      "step: 650, train loss: 1.68522, val loss: 1.85624\n",
      "step: 700, train loss: 1.67114, val loss: 1.82398\n",
      "step: 750, train loss: 1.6248, val loss: 1.79691\n",
      "step: 800, train loss: 1.60518, val loss: 1.77973\n",
      "step: 850, train loss: 1.55722, val loss: 1.73906\n",
      "step: 900, train loss: 1.53228, val loss: 1.72885\n",
      "step: 950, train loss: 1.55889, val loss: 1.73162\n",
      "step: 1000, train loss: 1.49724, val loss: 1.69836\n",
      "step: 1050, train loss: 1.4712, val loss: 1.6849\n",
      "step: 1100, train loss: 1.47201, val loss: 1.67361\n",
      "step: 1150, train loss: 1.45492, val loss: 1.6625\n",
      "step: 1200, train loss: 1.45731, val loss: 1.65982\n",
      "step: 1250, train loss: 1.43152, val loss: 1.65032\n",
      "step: 1300, train loss: 1.41565, val loss: 1.6527\n",
      "step: 1350, train loss: 1.40974, val loss: 1.64598\n",
      "step: 1400, train loss: 1.41893, val loss: 1.64582\n",
      "step: 1450, train loss: 1.3988, val loss: 1.64673\n",
      "step: 1500, train loss: 1.40619, val loss: 1.63361\n",
      "step: 1550, train loss: 1.39516, val loss: 1.63567\n",
      "step: 1600, train loss: 1.37723, val loss: 1.62701\n",
      "step: 1650, train loss: 1.38169, val loss: 1.64389\n",
      "step: 1700, train loss: 1.37975, val loss: 1.62553\n",
      "step: 1750, train loss: 1.37709, val loss: 1.6296\n",
      "step: 1800, train loss: 1.37466, val loss: 1.62166\n",
      "step: 1850, train loss: 1.34364, val loss: 1.63229\n",
      "step: 1900, train loss: 1.3703, val loss: 1.64049\n",
      "step: 1950, train loss: 1.34962, val loss: 1.63974\n",
      "step: 2000, train loss: 1.33952, val loss: 1.62103\n",
      "step: 2050, train loss: 1.34749, val loss: 1.61924\n",
      "step: 2100, train loss: 1.35222, val loss: 1.62183\n",
      "step: 2150, train loss: 1.33809, val loss: 1.62264\n",
      "step: 2200, train loss: 1.31893, val loss: 1.62059\n",
      "step: 2250, train loss: 1.32688, val loss: 1.61782\n",
      "step: 2300, train loss: 1.31213, val loss: 1.62079\n",
      "step: 2350, train loss: 1.32133, val loss: 1.61506\n",
      "step: 2400, train loss: 1.32357, val loss: 1.61913\n",
      "step: 2450, train loss: 1.32732, val loss: 1.61608\n",
      "step: 2500, train loss: 1.30318, val loss: 1.62059\n",
      "step: 2550, train loss: 1.30727, val loss: 1.61889\n",
      "step: 2600, train loss: 1.29782, val loss: 1.62135\n",
      "step: 2650, train loss: 1.30558, val loss: 1.61249\n",
      "step: 2700, train loss: 1.30232, val loss: 1.62578\n",
      "step: 2750, train loss: 1.29607, val loss: 1.60966\n",
      "step: 2800, train loss: 1.3075, val loss: 1.61564\n",
      "step: 2850, train loss: 1.2923, val loss: 1.61493\n",
      "step: 2900, train loss: 1.30284, val loss: 1.62733\n",
      "step: 2950, train loss: 1.30659, val loss: 1.62391\n",
      "step: 3000, train loss: 1.28253, val loss: 1.61618\n",
      "step: 3050, train loss: 1.2872, val loss: 1.61643\n",
      "step: 3100, train loss: 1.28418, val loss: 1.62087\n",
      "step: 3150, train loss: 1.30096, val loss: 1.61526\n",
      "step: 3200, train loss: 1.28043, val loss: 1.61657\n",
      "step: 3250, train loss: 1.28503, val loss: 1.62046\n",
      "step: 3300, train loss: 1.27314, val loss: 1.62109\n",
      "step: 3350, train loss: 1.30701, val loss: 1.61923\n",
      "step: 3400, train loss: 1.3089, val loss: 1.62481\n",
      "step: 3450, train loss: 1.27182, val loss: 1.61292\n",
      "step: 3500, train loss: 1.26824, val loss: 1.61539\n",
      "step: 3550, train loss: 1.32984, val loss: 1.62354\n",
      "step: 3600, train loss: 1.26517, val loss: 1.62337\n",
      "step: 3650, train loss: 1.28672, val loss: 1.61965\n",
      "step: 3700, train loss: 1.27229, val loss: 1.62488\n",
      "step: 3750, train loss: 1.25845, val loss: 1.61023\n",
      "step: 3800, train loss: 1.26295, val loss: 1.61626\n",
      "step: 3850, train loss: 1.38032, val loss: 1.64732\n",
      "step: 3900, train loss: 1.2861, val loss: 1.62151\n",
      "step: 3950, train loss: 1.27455, val loss: 1.62606\n",
      "step: 4000, train loss: 1.29782, val loss: 1.614\n",
      "step: 4050, train loss: 1.29925, val loss: 1.62589\n",
      "step: 4100, train loss: 1.26571, val loss: 1.61648\n",
      "step: 4150, train loss: 1.40642, val loss: 1.68497\n",
      "step: 4200, train loss: 1.2623, val loss: 1.61829\n",
      "step: 4250, train loss: 1.26803, val loss: 1.61318\n",
      "step: 4300, train loss: 1.26934, val loss: 1.61221\n",
      "step: 4350, train loss: 1.27225, val loss: 1.60883\n",
      "step: 4400, train loss: 1.24726, val loss: 1.61867\n",
      "step: 4450, train loss: 1.25537, val loss: 1.62646\n",
      "step: 4500, train loss: 1.28696, val loss: 1.61982\n",
      "step: 4550, train loss: 1.25346, val loss: 1.61332\n",
      "step: 4600, train loss: 1.26073, val loss: 1.61057\n",
      "step: 4650, train loss: 1.24258, val loss: 1.61702\n",
      "step: 4700, train loss: 1.24535, val loss: 1.6193\n",
      "step: 4750, train loss: 1.25014, val loss: 1.6193\n",
      "step: 4800, train loss: 1.24209, val loss: 1.61944\n",
      "step: 4850, train loss: 1.23296, val loss: 1.62608\n",
      "step: 4900, train loss: 1.22746, val loss: 1.61001\n",
      "step: 4950, train loss: 1.23544, val loss: 1.62162\n"
     ]
    }
   ],
   "source": [
    "trfl = TrainFlowDecoupled(steps = 5000, input_size=4, output_size=8)\n",
    "pyx = trfl.train(X, Y, X_val, Y_val)\n",
    "model_flow_dec = pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training regression with FLOW\n",
    "trfl = TrainFlowCombined(\n",
    "    steps = 250, \n",
    "    input_size=4,\n",
    "    output_size=8,\n",
    "    lr=8e-3, \n",
    "    OP = cn_constrained_dist,\n",
    "    n_samples=N_SAMPLES)\n",
    "pyx = trfl.train(X, Y, X_val, Y_val)\n",
    "model_flow_com = pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.to(dev)\n",
    "Y_val = Y_val.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagating predictions to Newsvendor Problem\n",
    "\n",
    "Y_pred_ANN_dec = model_ann_dec(X_val).unsqueeze(0)\n",
    "Y_pred_ANN_dec = inverse_transform(Y_pred_ANN_dec, dev)\n",
    "\n",
    "Y_pred_ANN_com = model_ann_com(X_val).unsqueeze(0)\n",
    "\n",
    "M = 4\n",
    "model_bnn_dec.update_n_samples(n_samples=M)\n",
    "Y_pred_BNN_dec = train_BNN_dec.model.forward_dist(X_val)\n",
    "Y_pred_BNN_dec = inverse_transform(Y_pred_BNN_dec, dev)\n",
    "\n",
    "model_bnn_com.update_n_samples(n_samples=M)\n",
    "Y_pred_BNN_com = train_BNN_com.model.forward_dist(X_val)\n",
    "M = Y_pred_BNN_com.shape[0]\n",
    "\n",
    "N = X_val.shape[0]\n",
    "Y_pred_flow_dec = torch.zeros((M, N, n_items)).to('cpu')\n",
    "for i in range(0, N):\n",
    "    Y_pred_flow_dec[:,i,:] = model_flow_dec.condition(X_val[i].to('cpu')).sample(torch.Size([M,])).squeeze()\n",
    "Y_pred_flow_dec = inverse_transform(Y_pred_flow_dec, 'cpu')\n",
    "\n",
    "#Y_pred_flow_com = torch.zeros((M, N, n_items))\n",
    "#for i in range(0, N):\n",
    "#    Y_pred_flow_com[:,i,:] = model_com.condition(X_val[i]).sample(torch.Size([M,])).squeeze()\n",
    "#Y_pred_flow_com = inverse_transform(Y_pred_flow_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9122, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(72.0898, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.3652, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(46.8540, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.0815, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "print(mse_loss(Y_pred_ANN_dec.mean(axis=0), Y_val_original.to(dev)))\n",
    "print(mse_loss(Y_pred_ANN_com.mean(axis=0), Y_val_original.to(dev)))\n",
    "\n",
    "print(mse_loss(Y_pred_BNN_dec.mean(axis=0), Y_val_original.to(dev)))\n",
    "print(mse_loss(Y_pred_BNN_com.mean(axis=0), Y_val_original.to(dev)))\n",
    "\n",
    "print(mse_loss(Y_pred_flow_dec.mean(axis=0), Y_val_original))\n",
    "#print(mse_loss(Y_pred_flow_com.mean(axis=0), Y_val_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t, 1)\n",
    "newsvendor_solve_kkt_M = SolveNewsvendorWithKKT(params_t, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4bb94bc31a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'variables' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def optimize_final_block(Y_pred, cost):\n",
    "\n",
    "    n_batches = int(np.ceil(Y_pred.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "    f_total = 0\n",
    "    f_total_best = 0\n",
    "\n",
    "    for b in range(0, n_batches):\n",
    "        i_low = b*BATCH_SIZE_LOADER\n",
    "        i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "        if b == n_batches-1:\n",
    "            i_up = n_batches*Y_pred.shape[1]\n",
    "        f_total += cost(Y_pred[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "        torch.cuda.empty_cache()\n",
    "        del X, Y, Y_original\n",
    "        gc.collect()\n",
    "        print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2249"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del X, Y, Y_original\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(534.0134, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(1164.4112, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(1675.9766, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(2308.2619, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(2874.9020, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(3471.3116, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(4093.9062, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(4675.0817, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(5211.3541, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(5840.4511, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(6371.0783, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(6937.6818, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(7488.6256, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(8086.1235, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(8759.3550, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(9398.1531, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(10022.0049, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "17 tensor(10631.0933, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "18 tensor(11239.0433, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "19 tensor(11835.5789, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "20 tensor(12449.8768, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "21 tensor(12978.6469, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "22 tensor(13589.7002, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "23 tensor(14140.2767, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "24 tensor(14762.9291, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "25 tensor(15472.3024, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "26 tensor(16116.0245, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "27 tensor(16683.8316, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "28 tensor(17213.2311, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "29 tensor(17772.2747, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "30 tensor(18513.7858, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "31 tensor(19227.7051, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "32 tensor(19810.5509, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "33 tensor(20429.6042, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "34 tensor(21053.8315, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "35 tensor(21708.5961, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "36 tensor(22208.1450, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "37 tensor(22756.4284, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "38 tensor(23330.5218, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "39 tensor(23945.4936, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "40 tensor(24482.3127, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "41 tensor(25131.4167, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "42 tensor(25670.9225, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "43 tensor(26143.5851, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "44 tensor(26659.3472, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "45 tensor(27271.4211, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "46 tensor(27932.4445, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimize_final_block(Y_pred_ANN_dec, newsvendor_solve_kkt.end_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(596.4487, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(1267.0186, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(1822.8551, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(2503.9690, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(3127.2937, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(3768.8301, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(4434.9386, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(5089.1112, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(5681.0644, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(6352.6998, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(6928.5031, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(7554.0100, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(8149.7893, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(8789.8028, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(9477.2520, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(10145.7697, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "16 tensor(10803.2805, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "17 tensor(11446.2811, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "18 tensor(12091.3481, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "19 tensor(12734.2155, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "20 tensor(13364.3890, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "21 tensor(13940.4815, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "22 tensor(14580.3434, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "23 tensor(15184.1358, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "24 tensor(15853.1013, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "25 tensor(16600.2069, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "26 tensor(17287.9884, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "27 tensor(17910.7239, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "28 tensor(18490.7346, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "29 tensor(19102.1387, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "30 tensor(19888.3777, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "31 tensor(20632.6235, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "32 tensor(21259.3569, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "33 tensor(21930.9071, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "34 tensor(22581.5155, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "35 tensor(23277.9901, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "36 tensor(23832.7443, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "37 tensor(24440.8070, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "38 tensor(25076.2964, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "39 tensor(25733.3709, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "40 tensor(26326.7931, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "41 tensor(27031.8264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "42 tensor(27616.5640, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "43 tensor(28151.3491, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "44 tensor(28737.3854, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "45 tensor(29388.2623, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "46 tensor(30079.2540, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimize_final_block(Y_pred_ANN_com, newsvendor_solve_kkt.end_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(541.4113, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(1176.5426, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(1697.4888, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(2335.6747, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(2912.8955, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(3523.8161, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(4160.6795, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(4748.2720, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(5300.0763, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(5940.2464, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(6486.6590, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(7066.6236, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(7618.7585, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(8227.4130, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(8907.2100, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(9555.7429, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(10181.6553, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "17 tensor(10796.8737, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "18 tensor(11409.9196, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "19 tensor(12020.9311, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "20 tensor(12637.0704, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "21 tensor(13173.9255, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-6c3a6c1813b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimize_final_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred_BNN_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewsvendor_solve_kkt_M\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_loss_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-9a467894135a>\u001b[0m in \u001b[0;36moptimize_final_block\u001b[0;34m(Y_pred, cost)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mi_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mf_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi_low\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_up\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_low\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_up\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2ca3ce4ac7b4>\u001b[0m in \u001b[0;36mend_loss_dist\u001b[0;34m(self, y_pred, y)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend_loss_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_f_per_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mf_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2ca3ce4ac7b4>\u001b[0m in \u001b[0;36mcalc_f_per_day\u001b[0;34m(self, y_pred, y)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_f_per_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mf_per_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_f_por_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_per_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2ca3ce4ac7b4>\u001b[0m in \u001b[0;36mcalc_f_por_item\u001b[0;34m(self, y_pred, y)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_outcomes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mz_star\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mf_per_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_per_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf_per_item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2ca3ce4ac7b4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0margmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQPFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             (Q.double(), lin.double(), ineqs.double(), \n\u001b[0;32m--> 119\u001b[0;31m              bound.double(), e, e).double()\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margmin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/qpth/qp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, Q_, p_, G_, h_, A_, b_)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 zhats, ctx.nus, ctx.lams, ctx.slacks = pdipm_b.forward(\n\u001b[1;32m     95\u001b[0m                     \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_LU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_LU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     eps, verbose, notImprovedLim, maxIter)\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQPSolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCVXPY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/qpth/solvers/pdipm/batch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(Q, p, G, h, A, b, Q_LU, S_LU, R, eps, verbose, notImprovedLim, maxIter, solver)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mKKTSolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLU_PARTIAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n\u001b[0;32m--> 180\u001b[0;31m                 Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mKKTSolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIR_UNOPT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/qpth/solvers/pdipm/batch.py\u001b[0m in \u001b[0;36msolve_kkt\u001b[0;34m(Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvQ_rx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mS_LU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0mg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mrx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "optimize_final_block(Y_pred_BNN_dec, newsvendor_solve_kkt_M.end_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_BNN_com, newsvendor_solve_kkt_M.end_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_flow_dec, newsvendor_solve_kkt_M.end_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(Y_pred):\n",
    "            z_star = newsvendor_solve_kkt_M.forward(Y_pred)\n",
    "            return z_star\n",
    "\n",
    "optimize_final_block(Y_pred_BNN_dec, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_BNN_com, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_flow_dec, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_flow_com, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_ANN_dec.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_ANN_dec.shape[1]\n",
    "    f_total += cost_fn(Y_pred_ANN_dec[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_BNN.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_BNN.shape[1]\n",
    "    f_total += cost_fn(Y_pred_BNN[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_flow.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_flow.shape[1]\n",
    "    f_total += cost_fn(Y_pred_flow[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_val_original.shape[0]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_val_original.shape[0]\n",
    "    f_total += cost_fn(Y_val_original[i_low:i_up,:].unsqueeze(0), Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(argmin_solver(reshape_outcomes(Y_val_original[0:50,:].unsqueeze(0)))*params_t['pr']).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.83 1.64 2.43 28104 27870 27787 13598\n",
    "1.87 1.89 2.46 42607 42070 41574 28124\n",
    "1.80 1.85 2.25 34329 33855 33395 15849\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
