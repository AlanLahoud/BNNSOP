{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import data_generator\n",
    "import params_newsvendor as params\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from model import VariationalLayer, VariationalNet, StandardNet, VariationalNet2\n",
    "\n",
    "from train import TrainDecoupled, TrainCombined\n",
    "\n",
    "from train_normflow import TrainFlowDecoupled, TrainFlowCombined\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 0\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters (change if necessary)\n",
    "N = 8000 # Total data size\n",
    "N_train = 5000 # Training data size\n",
    "N_SAMPLES = 2 # Sampling size while training\n",
    "BATCH_SIZE_LOADER = 16 # Standard batch size\n",
    "EPOCHS = 150 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "N_valid = N - N_train\n",
    "X, Y_original = data_generator.data_4to8(N_train, noise_level=nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_multi.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output normalization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Y_original)\n",
    "tmean = torch.tensor(scaler.mean_)\n",
    "tstd = torch.tensor(scaler.scale_)\n",
    "joblib.dump(scaler, 'scaler_multi.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(yy):\n",
    "    return yy*tstd + tmean\n",
    "\n",
    "Y = scaler.transform(Y_original).copy()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_original = data_generator.ArtificialDataset(X, Y_original)\n",
    "training_loader_original = torch.utils.data.DataLoader(\n",
    "    data_train_original, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "data_train = data_generator.ArtificialDataset(X, Y)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "X_val, Y_val_original = data_generator.data_4to8(N_valid, noise_level=nl)\n",
    "Y_val = scaler.transform(Y_val_original).copy()\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_original = torch.tensor(Y_val_original, dtype=torch.float32)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "data_valid_original = data_generator.ArtificialDataset(X_val, Y_val_original)\n",
    "validation_loader_original = torch.utils.data.DataLoader(\n",
    "    data_valid_original, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "data_valid = data_generator.ArtificialDataset(X_val, Y_val)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    data_valid, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t, n_samples):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "        \n",
    "        self.params_t = params_t\n",
    "        \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items  \n",
    "        self.n_samples = n_samples\n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident_samples = torch.eye(n_items*n_samples)\n",
    "        ident3 = torch.eye(n_items + 2*n_items*n_samples)\n",
    "        zeros_matrix = torch.zeros((n_items*n_samples, n_items*n_samples))\n",
    "        zeros_array = torch.zeros(n_items*n_samples)\n",
    "        ones_array = torch.ones(n_items*n_samples)\n",
    "             \n",
    "        self.Q = torch.diag(\n",
    "            torch.hstack(\n",
    "                (\n",
    "                    params_t['q'], \n",
    "                    (1/n_samples)*params_t['qs'].repeat_interleave(n_samples), \n",
    "                    (1/n_samples)*params_t['qw'].repeat_interleave(n_samples)\n",
    "                )\n",
    "            )).to(dev)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.hstack(\n",
    "                                (\n",
    "                                    params_t['c'], \n",
    "                                    (1/n_samples)*params_t['cs'].repeat_interleave(n_samples), \n",
    "                                    (1/n_samples)*params_t['cw'].repeat_interleave(n_samples)\n",
    "                                )).to(dev)\n",
    "             \n",
    "            \n",
    "        shortage_ineq = torch.hstack(\n",
    "            (\n",
    "                -ident.repeat_interleave(n_samples, 0), \n",
    "                -ident_samples, \n",
    "                zeros_matrix\n",
    "            )\n",
    "        )  \n",
    "        \n",
    "        \n",
    "        excess_ineq = torch.hstack(\n",
    "            (\n",
    "                ident.repeat_interleave(n_samples, 0), \n",
    "                zeros_matrix, \n",
    "                -ident_samples\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        price_ineq = torch.hstack(\n",
    "            (\n",
    "                params_t['pr'], \n",
    "                zeros_array, \n",
    "                zeros_array\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        \n",
    "        self.ineqs = torch.vstack(\n",
    "            (\n",
    "                shortage_ineq, \n",
    "                excess_ineq, \n",
    "                price_ineq, \n",
    "                positive_ineq\n",
    "            )\n",
    "        ).to(dev)\n",
    " \n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        \n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items*n_samples), \n",
    "                                          torch.zeros(n_items*n_samples))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, n_samples_items = y.size()\n",
    "                \n",
    "        assert self.n_samples*self.n_items == n_samples_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(batch_size, Q.size(0), Q.size(1))\n",
    "        \n",
    "        lin = self.lin\n",
    "        lin = lin.expand(batch_size, lin.size(0))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(batch_size, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            batch_size, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]\n",
    "    \n",
    "    def cost_per_item(self, Z, Y):\n",
    "        return params_t['q'].to(dev)*Z.to(dev)**2 \\\n",
    "            + self.params_t['qs'].to(dev)*(torch.max(torch.zeros((self.n_items)).to(dev),Y.to(dev)-Z.to(dev)))**2 \\\n",
    "            + self.params_t['qw'].to(dev)*(torch.max(torch.zeros((self.n_items)).to(dev),Z.to(dev)-Y.to(dev)))**2 \\\n",
    "            + self.params_t['c'].to(dev)*Z.to(dev) \\\n",
    "            + self.params_t['cs'].to(dev)*torch.max(torch.zeros((self.n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "            + self.params_t['cw'].to(dev)*torch.max(torch.zeros((self.n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "    \n",
    "    def reshape_outcomes(self, y_pred):\n",
    "                \n",
    "        if len(y_pred.shape) == 2:\n",
    "            y_pred = y_pred.unsqueeze(0)\n",
    "\n",
    "        n_samples = y_pred.shape[0]\n",
    "        batch_size = y_pred.shape[1]\n",
    "        #n_items = y_pred.shape[2]\n",
    "        y_pred = y_pred.permute((1, 2, 0)).reshape((batch_size, n_samples*self.n_items))\n",
    "        return y_pred\n",
    "    \n",
    "    def calc_f_por_item(self, y_pred, y):\n",
    "        #pdb.set_trace()\n",
    "        y_pred = self.reshape_outcomes(y_pred)\n",
    "        z_star =  self.forward(y_pred)\n",
    "        f_per_item = self.cost_per_item(z_star, y)\n",
    "        return f_per_item\n",
    "\n",
    "    def calc_f_per_day(self, y_pred, y):\n",
    "        f_per_item = self.calc_f_por_item(y_pred, y)\n",
    "        f = torch.sum(f_per_item, 1)\n",
    "        return f\n",
    "\n",
    "    def end_loss(self, y_pred, y):\n",
    "        f = self.calc_f_per_day(y_pred, y)\n",
    "        f_total = torch.mean(f)\n",
    "        return f_total\n",
    "    \n",
    "    def end_loss_dist(self, y_pred, y):\n",
    "        f = self.calc_f_per_day(y_pred, y)\n",
    "        f_total = torch.mean(f)\n",
    "        return f_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost_per_item = lambda Z, Y : params_t['q'].to(dev)*Z.to(dev)**2 \\\n",
    "                            + params_t['qs'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)))**2 \\\n",
    "                            + params_t['qw'].to(dev)*(torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev)))**2 \\\n",
    "                            + params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "\n",
    "def reshape_outcomes(y_pred):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    batch_size = y_pred.shape[1]\n",
    "    n_items = y_pred.shape[2]\n",
    "\n",
    "    y_pred = y_pred.permute((1, 2, 0)).reshape((batch_size, n_samples*n_items))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred = reshape_outcomes(y_pred)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f\n",
    "\n",
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ann_dec = StandardNet(input_size, output_size, 0).to(dev)\n",
    "h_bnn_dec = VariationalNet2(N_SAMPLES, input_size, output_size, 1.0).to(dev)\n",
    "\n",
    "h_ann_com = StandardNet(input_size, output_size, 0).to(dev)\n",
    "h_bnn_com = VariationalNet2(N_SAMPLES, input_size, output_size, 1.0).to(dev)\n",
    "\n",
    "opt_h_ann_dec = torch.optim.Adam(h_ann_dec.parameters(), lr=0.001)\n",
    "opt_h_bnn_dec = torch.optim.Adam(h_bnn_dec.parameters(), lr=0.001)\n",
    "\n",
    "opt_h_ann_com = torch.optim.Adam(h_ann_com.parameters(), lr=0.005)\n",
    "opt_h_bnn_com = torch.optim.Adam(h_bnn_com.parameters(), lr=0.005)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = output_size\n",
    "params_t, _ = params.get_params(n_items, seed_number)\n",
    "\n",
    "# Construct the solver\n",
    "cn_constrained = SolveNewsvendorWithKKT(params_t, 1)\n",
    "cn_constrained_dist = SolveNewsvendorWithKKT(params_t, N_SAMPLES*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "DATA LOSS \t train 0.449 valid 0.329\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.45 valid 0.33\n",
      "------------------EPOCH 2------------------\n",
      "DATA LOSS \t train 0.27 valid 0.264\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.27 valid 0.26\n",
      "------------------EPOCH 3------------------\n",
      "DATA LOSS \t train 0.228 valid 0.242\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.23 valid 0.24\n",
      "------------------EPOCH 4------------------\n",
      "DATA LOSS \t train 0.209 valid 0.229\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.21 valid 0.23\n",
      "------------------EPOCH 5------------------\n",
      "DATA LOSS \t train 0.194 valid 0.217\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.19 valid 0.22\n",
      "------------------EPOCH 6------------------\n",
      "DATA LOSS \t train 0.18 valid 0.2\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.18 valid 0.2\n",
      "------------------EPOCH 7------------------\n",
      "DATA LOSS \t train 0.162 valid 0.181\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.16 valid 0.18\n",
      "------------------EPOCH 8------------------\n",
      "DATA LOSS \t train 0.145 valid 0.165\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.15 valid 0.17\n",
      "------------------EPOCH 9------------------\n",
      "DATA LOSS \t train 0.133 valid 0.157\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.13 valid 0.16\n",
      "------------------EPOCH 10------------------\n",
      "DATA LOSS \t train 0.125 valid 0.15\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.15\n",
      "------------------EPOCH 11------------------\n",
      "DATA LOSS \t train 0.118 valid 0.146\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.12 valid 0.15\n",
      "------------------EPOCH 12------------------\n",
      "DATA LOSS \t train 0.113 valid 0.142\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.14\n",
      "------------------EPOCH 13------------------\n",
      "DATA LOSS \t train 0.109 valid 0.139\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.14\n",
      "------------------EPOCH 14------------------\n",
      "DATA LOSS \t train 0.105 valid 0.136\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.11 valid 0.14\n",
      "------------------EPOCH 15------------------\n",
      "DATA LOSS \t train 0.101 valid 0.132\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 16------------------\n",
      "DATA LOSS \t train 0.097 valid 0.129\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.1 valid 0.13\n",
      "------------------EPOCH 17------------------\n",
      "DATA LOSS \t train 0.093 valid 0.125\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.13\n",
      "------------------EPOCH 18------------------\n",
      "DATA LOSS \t train 0.089 valid 0.122\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 19------------------\n",
      "DATA LOSS \t train 0.086 valid 0.121\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.09 valid 0.12\n",
      "------------------EPOCH 20------------------\n",
      "DATA LOSS \t train 0.084 valid 0.118\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.12\n",
      "------------------EPOCH 21------------------\n",
      "DATA LOSS \t train 0.081 valid 0.117\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.12\n",
      "------------------EPOCH 22------------------\n",
      "DATA LOSS \t train 0.079 valid 0.114\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 23------------------\n",
      "DATA LOSS \t train 0.077 valid 0.112\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 24------------------\n",
      "DATA LOSS \t train 0.076 valid 0.111\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.08 valid 0.11\n",
      "------------------EPOCH 25------------------\n",
      "DATA LOSS \t train 0.074 valid 0.109\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.11\n",
      "------------------EPOCH 26------------------\n",
      "DATA LOSS \t train 0.072 valid 0.108\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.11\n",
      "------------------EPOCH 27------------------\n",
      "DATA LOSS \t train 0.071 valid 0.105\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 28------------------\n",
      "DATA LOSS \t train 0.07 valid 0.102\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 29------------------\n",
      "DATA LOSS \t train 0.069 valid 0.101\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 30------------------\n",
      "DATA LOSS \t train 0.068 valid 0.1\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 31------------------\n",
      "DATA LOSS \t train 0.067 valid 0.098\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 32------------------\n",
      "DATA LOSS \t train 0.066 valid 0.099\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 33------------------\n",
      "DATA LOSS \t train 0.065 valid 0.097\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.07 valid 0.1\n",
      "------------------EPOCH 34------------------\n",
      "DATA LOSS \t train 0.064 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 35------------------\n",
      "DATA LOSS \t train 0.064 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 36------------------\n",
      "DATA LOSS \t train 0.063 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 37------------------\n",
      "DATA LOSS \t train 0.063 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 38------------------\n",
      "DATA LOSS \t train 0.062 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 39------------------\n",
      "DATA LOSS \t train 0.061 valid 0.096\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.1\n",
      "------------------EPOCH 40------------------\n",
      "DATA LOSS \t train 0.061 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 41------------------\n",
      "DATA LOSS \t train 0.061 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 42------------------\n",
      "DATA LOSS \t train 0.06 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 43------------------\n",
      "DATA LOSS \t train 0.059 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 44------------------\n",
      "DATA LOSS \t train 0.059 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 45------------------\n",
      "DATA LOSS \t train 0.058 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 46------------------\n",
      "DATA LOSS \t train 0.058 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 47------------------\n",
      "DATA LOSS \t train 0.058 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 48------------------\n",
      "DATA LOSS \t train 0.058 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 49------------------\n",
      "DATA LOSS \t train 0.057 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 50------------------\n",
      "DATA LOSS \t train 0.057 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 51------------------\n",
      "DATA LOSS \t train 0.057 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 52------------------\n",
      "DATA LOSS \t train 0.056 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 53------------------\n",
      "DATA LOSS \t train 0.056 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 54------------------\n",
      "DATA LOSS \t train 0.055 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 55------------------\n",
      "DATA LOSS \t train 0.055 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.06 valid 0.09\n",
      "------------------EPOCH 56------------------\n",
      "DATA LOSS \t train 0.055 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 57------------------\n",
      "DATA LOSS \t train 0.055 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 58------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.054 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 59------------------\n",
      "DATA LOSS \t train 0.054 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 60------------------\n",
      "DATA LOSS \t train 0.053 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 61------------------\n",
      "DATA LOSS \t train 0.053 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 62------------------\n",
      "DATA LOSS \t train 0.053 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 63------------------\n",
      "DATA LOSS \t train 0.053 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 64------------------\n",
      "DATA LOSS \t train 0.052 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 65------------------\n",
      "DATA LOSS \t train 0.052 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 66------------------\n",
      "DATA LOSS \t train 0.052 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 67------------------\n",
      "DATA LOSS \t train 0.052 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 68------------------\n",
      "DATA LOSS \t train 0.052 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 69------------------\n",
      "DATA LOSS \t train 0.051 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 70------------------\n",
      "DATA LOSS \t train 0.051 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 71------------------\n",
      "DATA LOSS \t train 0.051 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 72------------------\n",
      "DATA LOSS \t train 0.051 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 73------------------\n",
      "DATA LOSS \t train 0.051 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 74------------------\n",
      "DATA LOSS \t train 0.05 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 75------------------\n",
      "DATA LOSS \t train 0.05 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 76------------------\n",
      "DATA LOSS \t train 0.05 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 77------------------\n",
      "DATA LOSS \t train 0.049 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 78------------------\n",
      "DATA LOSS \t train 0.049 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 79------------------\n",
      "DATA LOSS \t train 0.049 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 80------------------\n",
      "DATA LOSS \t train 0.048 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 81------------------\n",
      "DATA LOSS \t train 0.048 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 82------------------\n",
      "DATA LOSS \t train 0.048 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 83------------------\n",
      "DATA LOSS \t train 0.047 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 84------------------\n",
      "DATA LOSS \t train 0.049 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 85------------------\n",
      "DATA LOSS \t train 0.048 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 86------------------\n",
      "DATA LOSS \t train 0.047 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 87------------------\n",
      "DATA LOSS \t train 0.047 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 88------------------\n",
      "DATA LOSS \t train 0.047 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 89------------------\n",
      "DATA LOSS \t train 0.048 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 90------------------\n",
      "DATA LOSS \t train 0.047 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 91------------------\n",
      "DATA LOSS \t train 0.046 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 92------------------\n",
      "DATA LOSS \t train 0.046 valid 0.086\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 93------------------\n",
      "DATA LOSS \t train 0.046 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 94------------------\n",
      "DATA LOSS \t train 0.046 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 95------------------\n",
      "DATA LOSS \t train 0.046 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 96------------------\n",
      "DATA LOSS \t train 0.046 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 97------------------\n",
      "DATA LOSS \t train 0.045 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 98------------------\n",
      "DATA LOSS \t train 0.045 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 99------------------\n",
      "DATA LOSS \t train 0.045 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 100------------------\n",
      "DATA LOSS \t train 0.046 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 101------------------\n",
      "DATA LOSS \t train 0.045 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.05 valid 0.09\n",
      "------------------EPOCH 102------------------\n",
      "DATA LOSS \t train 0.044 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 103------------------\n",
      "DATA LOSS \t train 0.044 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 104------------------\n",
      "DATA LOSS \t train 0.044 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 105------------------\n",
      "DATA LOSS \t train 0.044 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 106------------------\n",
      "DATA LOSS \t train 0.044 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 107------------------\n",
      "DATA LOSS \t train 0.044 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 108------------------\n",
      "DATA LOSS \t train 0.043 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 109------------------\n",
      "DATA LOSS \t train 0.044 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 110------------------\n",
      "DATA LOSS \t train 0.044 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 111------------------\n",
      "DATA LOSS \t train 0.044 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 112------------------\n",
      "DATA LOSS \t train 0.043 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 113------------------\n",
      "DATA LOSS \t train 0.043 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 114------------------\n",
      "DATA LOSS \t train 0.044 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 115------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOSS \t train 0.044 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 116------------------\n",
      "DATA LOSS \t train 0.043 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 117------------------\n",
      "DATA LOSS \t train 0.042 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 118------------------\n",
      "DATA LOSS \t train 0.042 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 119------------------\n",
      "DATA LOSS \t train 0.042 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 120------------------\n",
      "DATA LOSS \t train 0.042 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 121------------------\n",
      "DATA LOSS \t train 0.041 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 122------------------\n",
      "DATA LOSS \t train 0.042 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 123------------------\n",
      "DATA LOSS \t train 0.042 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 124------------------\n",
      "DATA LOSS \t train 0.043 valid 0.087\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 125------------------\n",
      "DATA LOSS \t train 0.042 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 126------------------\n",
      "DATA LOSS \t train 0.042 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 127------------------\n",
      "DATA LOSS \t train 0.041 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 128------------------\n",
      "DATA LOSS \t train 0.041 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 129------------------\n",
      "DATA LOSS \t train 0.041 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 130------------------\n",
      "DATA LOSS \t train 0.041 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 131------------------\n",
      "DATA LOSS \t train 0.041 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 132------------------\n",
      "DATA LOSS \t train 0.041 valid 0.088\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 133------------------\n",
      "DATA LOSS \t train 0.041 valid 0.093\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 134------------------\n",
      "DATA LOSS \t train 0.041 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 135------------------\n",
      "DATA LOSS \t train 0.041 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 136------------------\n",
      "DATA LOSS \t train 0.04 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 137------------------\n",
      "DATA LOSS \t train 0.04 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 138------------------\n",
      "DATA LOSS \t train 0.041 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 139------------------\n",
      "DATA LOSS \t train 0.041 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 140------------------\n",
      "DATA LOSS \t train 0.041 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 141------------------\n",
      "DATA LOSS \t train 0.04 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 142------------------\n",
      "DATA LOSS \t train 0.04 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 143------------------\n",
      "DATA LOSS \t train 0.04 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 144------------------\n",
      "DATA LOSS \t train 0.04 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 145------------------\n",
      "DATA LOSS \t train 0.039 valid 0.089\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 146------------------\n",
      "DATA LOSS \t train 0.039 valid 0.09\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 147------------------\n",
      "DATA LOSS \t train 0.039 valid 0.091\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 148------------------\n",
      "DATA LOSS \t train 0.039 valid 0.094\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 149------------------\n",
      "DATA LOSS \t train 0.04 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n",
      "------------------EPOCH 150------------------\n",
      "DATA LOSS \t train 0.04 valid 0.092\n",
      "KL LOSS \t train 0.0 valid 0.0\n",
      "ELBO LOSS \t train 0.04 valid 0.09\n"
     ]
    }
   ],
   "source": [
    "train_ANN_dec = TrainDecoupled(\n",
    "                    bnn = False,\n",
    "                    model=h_ann_dec,\n",
    "                    opt=opt_h_ann_dec,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=0.0,\n",
    "                    training_loader=training_loader,\n",
    "                    validation_loader=validation_loader\n",
    "                )\n",
    "\n",
    "train_ANN_dec.train(EPOCHS=EPOCHS)\n",
    "model_ann_dec = train_ANN_dec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "END LOSS \t train 35054.674 valid 33125.471\n",
      "------------------EPOCH 2------------------\n",
      "END LOSS \t train 32804.396 valid 32330.899\n",
      "------------------EPOCH 3------------------\n",
      "END LOSS \t train 32314.45 valid 31022.274\n",
      "------------------EPOCH 4------------------\n",
      "END LOSS \t train 30408.811 valid 30117.512\n",
      "------------------EPOCH 5------------------\n",
      "END LOSS \t train 29644.298 valid 29263.458\n",
      "------------------EPOCH 6------------------\n",
      "END LOSS \t train 29068.86 valid 28970.194\n",
      "------------------EPOCH 7------------------\n",
      "END LOSS \t train 28831.273 valid 28878.379\n",
      "------------------EPOCH 8------------------\n",
      "END LOSS \t train 28700.526 valid 28692.009\n",
      "------------------EPOCH 9------------------\n",
      "END LOSS \t train 28543.471 valid 28502.843\n",
      "------------------EPOCH 10------------------\n",
      "END LOSS \t train 28419.366 valid 28642.384\n",
      "------------------EPOCH 11------------------\n",
      "END LOSS \t train 28308.034 valid 28393.69\n",
      "------------------EPOCH 12------------------\n",
      "END LOSS \t train 28194.994 valid 28162.447\n",
      "------------------EPOCH 13------------------\n",
      "END LOSS \t train 28144.234 valid 28211.268\n",
      "------------------EPOCH 14------------------\n",
      "END LOSS \t train 28070.455 valid 28329.105\n",
      "------------------EPOCH 15------------------\n",
      "END LOSS \t train 28009.463 valid 28285.758\n",
      "------------------EPOCH 16------------------\n",
      "END LOSS \t train 27953.664 valid 28235.147\n",
      "------------------EPOCH 17------------------\n",
      "END LOSS \t train 27923.065 valid 28211.137\n",
      "------------------EPOCH 18------------------\n",
      "END LOSS \t train 27892.852 valid 28266.324\n",
      "------------------EPOCH 19------------------\n",
      "END LOSS \t train 27845.329 valid 28212.069\n",
      "------------------EPOCH 20------------------\n",
      "END LOSS \t train 27801.238 valid 27938.475\n",
      "------------------EPOCH 21------------------\n",
      "END LOSS \t train 27781.133 valid 28056.311\n",
      "------------------EPOCH 22------------------\n",
      "END LOSS \t train 27723.426 valid 28012.228\n",
      "------------------EPOCH 23------------------\n",
      "END LOSS \t train 27759.878 valid 28045.631\n",
      "------------------EPOCH 24------------------\n",
      "END LOSS \t train 27721.123 valid 28096.971\n",
      "------------------EPOCH 25------------------\n",
      "END LOSS \t train 27708.162 valid 28110.858\n",
      "------------------EPOCH 26------------------\n",
      "END LOSS \t train 27745.626 valid 27948.114\n",
      "------------------EPOCH 27------------------\n",
      "END LOSS \t train 27644.255 valid 27896.882\n",
      "------------------EPOCH 28------------------\n",
      "END LOSS \t train 27670.283 valid 27979.447\n",
      "------------------EPOCH 29------------------\n",
      "END LOSS \t train 27634.478 valid 27938.018\n",
      "------------------EPOCH 30------------------\n",
      "END LOSS \t train 27646.91 valid 27805.21\n",
      "------------------EPOCH 31------------------\n",
      "END LOSS \t train 27593.481 valid 27811.521\n",
      "------------------EPOCH 32------------------\n",
      "END LOSS \t train 27573.275 valid 27846.6\n",
      "------------------EPOCH 33------------------\n",
      "END LOSS \t train 27560.561 valid 27888.107\n",
      "------------------EPOCH 34------------------\n",
      "END LOSS \t train 27569.032 valid 27783.891\n",
      "------------------EPOCH 35------------------\n",
      "END LOSS \t train 27537.647 valid 27789.229\n"
     ]
    }
   ],
   "source": [
    "train_ANN_com = TrainCombined(\n",
    "                    bnn = False,\n",
    "                    model=h_ann_com,\n",
    "                    opt=opt_h_ann_com,\n",
    "                    training_loader=training_loader_original,\n",
    "                    validation_loader=validation_loader_original,\n",
    "                    OP = cn_constrained\n",
    "                )\n",
    "\n",
    "train_ANN_com.train(EPOCHS=35)\n",
    "model_ann_com = train_ANN_com.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_BNN_dec \u001b[38;5;241m=\u001b[39m TrainDecoupled(\n\u001b[1;32m      2\u001b[0m                     bnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                     model\u001b[38;5;241m=\u001b[39mh_bnn_dec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                     validation_loader\u001b[38;5;241m=\u001b[39mvalidation_loader_original\n\u001b[1;32m      9\u001b[0m                 )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_BNN_dec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model_bnn_dec \u001b[38;5;241m=\u001b[39m train_BNN_dec\u001b[38;5;241m.\u001b[39mmodel\n",
      "File \u001b[0;32m~/Desktop/Research/Code/pao_uncertainty/train.py:76\u001b[0m, in \u001b[0;36mTrainDecoupled.train\u001b[0;34m(self, EPOCHS)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m------------------EPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     73\u001b[0m     epoch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m avg_loss_data_loss, avg_kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m avg_loss_data_loss \u001b[38;5;241m+\u001b[39m avg_kl_loss\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Research/Code/pao_uncertainty/train.py:55\u001b[0m, in \u001b[0;36mTrainDecoupled.train_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m loss_data_ \u001b[38;5;241m=\u001b[39m loss_data_\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#through output dimension\u001b[39;00m\n\u001b[1;32m     54\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_data_ \u001b[38;5;241m+\u001b[39m kl_loss_\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m data_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_data_\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "train_BNN_dec = TrainDecoupled(\n",
    "                    bnn = True,\n",
    "                    model=h_bnn_dec,\n",
    "                    opt=opt_h_bnn_dec,\n",
    "                    loss_data=mse_loss,\n",
    "                    K=0.0,\n",
    "                    training_loader=training_loader_original,\n",
    "                    validation_loader=validation_loader_original\n",
    "                )\n",
    "\n",
    "train_BNN_dec.train(EPOCHS=EPOCHS)\n",
    "model_bnn_dec = train_BNN_dec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "END LOSS \t train 67603.191 valid 64863.397\n",
      "------------------EPOCH 2------------------\n",
      "END LOSS \t train 63983.753 valid 62837.398\n",
      "------------------EPOCH 3------------------\n",
      "END LOSS \t train 62835.602 valid 61955.275\n",
      "------------------EPOCH 4------------------\n",
      "END LOSS \t train 61977.076 valid 61275.862\n",
      "------------------EPOCH 5------------------\n",
      "END LOSS \t train 61190.33 valid 60731.408\n",
      "------------------EPOCH 6------------------\n",
      "END LOSS \t train 60356.31 valid 60510.741\n",
      "------------------EPOCH 7------------------\n",
      "END LOSS \t train 59374.942 valid 58785.084\n",
      "------------------EPOCH 8------------------\n",
      "END LOSS \t train 58795.948 valid 58552.18\n"
     ]
    }
   ],
   "source": [
    "train_BNN_com = TrainCombined(\n",
    "                    bnn = True,\n",
    "                    model=h_bnn_com,\n",
    "                    opt=opt_h_bnn_com,\n",
    "                    training_loader=training_loader_original,\n",
    "                    validation_loader=validation_loader_original,\n",
    "                    OP = cn_constrained_dist\n",
    "                )\n",
    "\n",
    "train_BNN_com.train(EPOCHS=8)\n",
    "model_bnn_com = train_BNN_com.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.62412, val loss: 4.55984\n",
      "step: 50, train loss: 3.4359, val loss: 3.4497\n",
      "step: 100, train loss: 2.82206, val loss: 2.88951\n",
      "step: 150, train loss: 2.48787, val loss: 2.57233\n",
      "step: 200, train loss: 2.25889, val loss: 2.38194\n",
      "step: 250, train loss: 2.10464, val loss: 2.24441\n",
      "step: 300, train loss: 2.001, val loss: 2.14645\n",
      "step: 350, train loss: 1.91222, val loss: 2.07915\n",
      "step: 400, train loss: 1.85699, val loss: 2.02924\n",
      "step: 450, train loss: 1.81211, val loss: 1.98739\n",
      "step: 500, train loss: 1.77799, val loss: 1.95539\n",
      "step: 550, train loss: 1.73116, val loss: 1.9221\n",
      "step: 600, train loss: 1.69877, val loss: 1.89715\n",
      "step: 650, train loss: 1.67398, val loss: 1.87381\n",
      "step: 700, train loss: 1.65646, val loss: 1.84964\n",
      "step: 750, train loss: 1.63558, val loss: 1.83198\n",
      "step: 800, train loss: 1.60943, val loss: 1.80499\n",
      "step: 850, train loss: 1.5891, val loss: 1.79062\n",
      "step: 900, train loss: 1.55663, val loss: 1.76639\n",
      "step: 950, train loss: 1.53143, val loss: 1.75044\n",
      "step: 1000, train loss: 1.51643, val loss: 1.72052\n",
      "step: 1050, train loss: 1.49909, val loss: 1.70834\n",
      "step: 1100, train loss: 1.47097, val loss: 1.70075\n",
      "step: 1150, train loss: 1.44779, val loss: 1.68173\n",
      "step: 1200, train loss: 1.45392, val loss: 1.67819\n",
      "step: 1250, train loss: 1.41568, val loss: 1.66972\n",
      "step: 1300, train loss: 1.42252, val loss: 1.66425\n",
      "step: 1350, train loss: 1.41103, val loss: 1.66882\n",
      "step: 1400, train loss: 1.38803, val loss: 1.64912\n",
      "step: 1450, train loss: 1.40729, val loss: 1.64646\n",
      "step: 1500, train loss: 1.38641, val loss: 1.69999\n",
      "step: 1550, train loss: 1.38091, val loss: 1.63142\n",
      "step: 1600, train loss: 1.37421, val loss: 1.63976\n",
      "step: 1650, train loss: 1.37426, val loss: 1.63389\n",
      "step: 1700, train loss: 1.35864, val loss: 1.64017\n",
      "step: 1750, train loss: 1.35286, val loss: 1.62979\n",
      "step: 1800, train loss: 1.35333, val loss: 1.6386\n",
      "step: 1850, train loss: 1.34378, val loss: 1.63703\n",
      "step: 1900, train loss: 1.34665, val loss: 1.63664\n",
      "step: 1950, train loss: 1.34937, val loss: 1.64311\n",
      "step: 2000, train loss: 1.34006, val loss: 1.63067\n",
      "step: 2050, train loss: 1.34833, val loss: 1.63024\n",
      "step: 2100, train loss: 1.35295, val loss: 1.63311\n",
      "step: 2150, train loss: 1.3269, val loss: 1.62869\n",
      "step: 2200, train loss: 1.31836, val loss: 1.62071\n",
      "step: 2250, train loss: 1.31772, val loss: 1.63306\n",
      "step: 2300, train loss: 1.33284, val loss: 1.63189\n",
      "step: 2350, train loss: 1.29747, val loss: 1.62999\n",
      "step: 2400, train loss: 1.32634, val loss: 1.62805\n",
      "step: 2450, train loss: 1.31316, val loss: 1.62118\n",
      "step: 2500, train loss: 1.29539, val loss: 1.63095\n",
      "step: 2550, train loss: 1.31388, val loss: 1.64031\n",
      "step: 2600, train loss: 1.2998, val loss: 1.62256\n",
      "step: 2650, train loss: 1.29692, val loss: 1.62228\n",
      "step: 2700, train loss: 1.28775, val loss: 1.62519\n",
      "step: 2750, train loss: 1.31004, val loss: 1.63148\n",
      "step: 2800, train loss: 1.291, val loss: 1.63035\n",
      "step: 2850, train loss: 1.28535, val loss: 1.617\n",
      "step: 2900, train loss: 1.2945, val loss: 1.6261\n",
      "step: 2950, train loss: 1.28341, val loss: 1.64211\n",
      "step: 3000, train loss: 1.29066, val loss: 1.64398\n",
      "step: 3050, train loss: 1.29007, val loss: 1.62505\n",
      "step: 3100, train loss: 1.27794, val loss: 1.6289\n",
      "step: 3150, train loss: 1.26876, val loss: 1.6356\n",
      "step: 3200, train loss: 1.28641, val loss: 1.64555\n",
      "step: 3250, train loss: 1.26037, val loss: 1.63069\n",
      "step: 3300, train loss: 1.26593, val loss: 1.65016\n",
      "step: 3350, train loss: 1.26893, val loss: 1.63172\n",
      "step: 3400, train loss: 1.27011, val loss: 1.63079\n",
      "step: 3450, train loss: 1.26957, val loss: 1.63096\n",
      "step: 3500, train loss: 1.31794, val loss: 1.65455\n",
      "step: 3550, train loss: 1.28052, val loss: 1.63839\n",
      "step: 3600, train loss: 1.28536, val loss: 1.62809\n",
      "step: 3650, train loss: 1.25567, val loss: 1.64498\n",
      "step: 3700, train loss: 1.25689, val loss: 1.64336\n",
      "step: 3750, train loss: 1.24078, val loss: 1.63577\n",
      "step: 3800, train loss: 1.26114, val loss: 1.64447\n",
      "step: 3850, train loss: 1.25804, val loss: 1.63407\n",
      "step: 3900, train loss: 1.27573, val loss: 1.6377\n",
      "step: 3950, train loss: 1.2465, val loss: 1.63545\n",
      "step: 4000, train loss: 1.22901, val loss: 1.63484\n",
      "step: 4050, train loss: 1.27774, val loss: 1.64342\n",
      "step: 4100, train loss: 1.25685, val loss: 1.65943\n",
      "step: 4150, train loss: 1.25607, val loss: 1.6443\n",
      "step: 4200, train loss: 1.2403, val loss: 1.64834\n",
      "step: 4250, train loss: 1.23246, val loss: 1.64964\n",
      "step: 4300, train loss: 1.23334, val loss: 1.65159\n",
      "step: 4350, train loss: 1.24362, val loss: 1.63369\n",
      "step: 4400, train loss: 1.21705, val loss: 1.65603\n",
      "step: 4450, train loss: 1.23659, val loss: 1.64976\n",
      "step: 4500, train loss: 1.23056, val loss: 1.64373\n",
      "step: 4550, train loss: 1.22256, val loss: 1.65772\n",
      "step: 4600, train loss: 1.22918, val loss: 1.64338\n",
      "step: 4650, train loss: 1.23971, val loss: 1.64112\n",
      "step: 4700, train loss: 1.23738, val loss: 1.64511\n",
      "step: 4750, train loss: 1.23479, val loss: 1.65064\n",
      "step: 4800, train loss: 1.25176, val loss: 1.64354\n",
      "step: 4850, train loss: 1.21223, val loss: 1.64649\n",
      "step: 4900, train loss: 1.22744, val loss: 1.6578\n",
      "step: 4950, train loss: 1.21793, val loss: 1.64635\n"
     ]
    }
   ],
   "source": [
    "trfl = TrainFlowDecoupled(steps = 5000, input_size=4, output_size=8)\n",
    "pyx = trfl.train(X, Y, X_val, Y_val)\n",
    "model_flow_dec = pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training regression with FLOW\n",
    "trfl = TrainFlowCombined(\n",
    "    steps = 250, \n",
    "    input_size=4,\n",
    "    output_size=8,\n",
    "    lr=8e-3, \n",
    "    OP = cn_constrained_dist,\n",
    "    n_samples=N_SAMPLES*4)\n",
    "pyx = trfl.train(X, Y, X_val, Y_val)\n",
    "model_flow_com = pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagating predictions to Newsvendor Problem\n",
    "\n",
    "\n",
    "Y_pred_ANN_dec = model_ann_dec(X_val).unsqueeze(0)\n",
    "Y_pred_ANN_dec = inverse_transform(Y_pred_ANN_dec)\n",
    "\n",
    "Y_pred_ANN_com = model_ann_com(X_val).unsqueeze(0)\n",
    "Y_pred_ANN_com = inverse_transform(Y_pred_ANN_com)\n",
    "\n",
    "M = 4\n",
    "model_bnn_dec.update_n_samples(n_samples=M)\n",
    "Y_pred_BNN_dec = train_BNN_dec.model.forward_dist(X_val)\n",
    "Y_pred_BNN_dec = inverse_transform(Y_pred_BNN_dec)\n",
    "\n",
    "model_bnn_com.update_n_samples(n_samples=M)\n",
    "Y_pred_BNN_com = train_BNN_com.model.forward_dist(X_val)\n",
    "Y_pred_BNN_com = inverse_transform(Y_pred_BNN_com)\n",
    "M = Y_pred_BNN_com.shape[0]\n",
    "\n",
    "N = X_val.shape[0]\n",
    "Y_pred_flow_dec = torch.zeros((M, N, n_items))\n",
    "for i in range(0, N):\n",
    "    Y_pred_flow_dec[:,i,:] = model_flow_dec.condition(X_val[i]).sample(torch.Size([M,])).squeeze()\n",
    "Y_pred_flow_dec = inverse_transform(Y_pred_flow_dec)\n",
    "\n",
    "#Y_pred_flow_com = torch.zeros((M, N, n_items))\n",
    "#for i in range(0, N):\n",
    "#    Y_pred_flow_com[:,i,:] = model_com.condition(X_val[i]).sample(torch.Size([M,])).squeeze()\n",
    "#Y_pred_flow_com = inverse_transform(Y_pred_flow_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "print(mse_loss(Y_pred_ANN_dec.mean(axis=0), Y_val_original))\n",
    "print(mse_loss(Y_pred_ANN_com.mean(axis=0), Y_val_original))\n",
    "\n",
    "print(mse_loss(Y_pred_BNN_dec.mean(axis=0), Y_val_original))\n",
    "print(mse_loss(Y_pred_BNN_com.mean(axis=0), Y_val_original))\n",
    "\n",
    "print(mse_loss(Y_pred_flow_dec.mean(axis=0), Y_val_original))\n",
    "#print(mse_loss(Y_pred_flow_com.mean(axis=0), Y_val_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t, 1)\n",
    "newsvendor_solve_kkt_M = SolveNewsvendorWithKKT(params_t, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_final_block(Y_pred, cost):\n",
    "\n",
    "    n_batches = int(np.ceil(Y_pred.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "    f_total = 0\n",
    "    f_total_best = 0\n",
    "\n",
    "    for b in range(0, n_batches):\n",
    "        i_low = b*BATCH_SIZE_LOADER\n",
    "        i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "        if b == n_batches-1:\n",
    "            i_up = n_batches*Y_pred.shape[1]\n",
    "        f_total += cost(Y_pred[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "        print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_ANN_dec, newsvendor_solve_kkt.end_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_ANN_com, newsvendor_solve_kkt.end_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_BNN_dec, newsvendor_solve_kkt_M.end_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_BNN_com, newsvendor_solve_kkt_M.end_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_final_block(Y_pred_flow_dec, newsvendor_solve_kkt_M.end_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(317.2303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(545.6716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(868.6623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(1180.7642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(1423.6939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(1697.4194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(2042.8964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(2337.1373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(2586.9145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(2910.9963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(3173.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(3517.5267, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(3846.5958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(4144.6255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(4474.1223, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(4729.8327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(5004.2762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(5276.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(5658.7426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(5911.4266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(6200.7124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(6440.7008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(6763.4416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(7014.9475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(7280.3299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(7564.3329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(7846.7939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(8157.7855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(8485.9621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(8836.3427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(9122.4748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(9470.3564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(9791.6781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(10091.8014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(10406.8197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(10703.6377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(10964.4003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(11307.7264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(11624.7572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(11910.0864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(12208.2377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(12527.0826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(12770.0873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(13056.8119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(13345.9570, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(13662.0244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(13922.1700, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(14212.4365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(14484.5503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(14841.6840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(15208.1437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(15545.3182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(15848.4633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(16187.5134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(16474.3330, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(16755.0356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(17018.1679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "57 tensor(17277.3563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "58 tensor(17605.1956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "59 tensor(17839.5585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "60 tensor(18218.0627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "61 tensor(18578.5665, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "62 tensor(18892.6838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "63 tensor(19291.2933, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "64 tensor(19578.9636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "65 tensor(19883.4007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "66 tensor(20144.7740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "67 tensor(20501.2625, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "68 tensor(20791.2933, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "69 tensor(21121.6068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "70 tensor(21460.6083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "71 tensor(21775.3258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "72 tensor(22033.5136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "73 tensor(22275.2679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "74 tensor(22580.7085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "75 tensor(22822.4611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "76 tensor(23109.4858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "77 tensor(23397.7577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "78 tensor(23693.6314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "79 tensor(24008.7851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "80 tensor(24288.2175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "81 tensor(24539.7155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "82 tensor(24904.2414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "83 tensor(25194.8306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "84 tensor(25488.2548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "85 tensor(25733.4712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "86 tensor(25971.1930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "87 tensor(26216.0587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "88 tensor(26495.3215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "89 tensor(26729.1920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "90 tensor(27009.5736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "91 tensor(27339.4402, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "92 tensor(27670.0473, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "93 tensor(28000.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(Y_pred):\n",
    "            z_star = newsvendor_solve_kkt_M.forward(Y_pred)\n",
    "            return z_star\n",
    "\n",
    "optimize_final_block(Y_pred_BNN_dec, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(343.3991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(613.6026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(971.4962, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(1319.0793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(1599.1197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(1910.8205, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(2283.6338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(2615.8299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(2905.5934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(3260.9891, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(3561.1766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(3937.5485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(4305.7485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(4645.9651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(5018.1453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(5323.1843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(5632.1742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(5945.6719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(6355.3444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(6653.1895, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(6971.1554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(7267.6211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(7627.5680, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(7927.8961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(8232.1096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(8562.7459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(8888.1610, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(9237.9241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(9596.1201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(9979.4686, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(10308.6368, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(10688.7974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(11043.4994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(11377.7377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(11727.8604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(12056.8771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(12359.7306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(12736.4097, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(13089.9759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(13412.2786, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(13738.4150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(14081.7210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(14371.3799, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(14694.6737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(15020.1563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(15378.7684, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(15687.5591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(16018.0937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(16336.8518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(16727.9428, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(17119.6563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(17494.6654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(17822.7946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(18201.4929, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(18519.1134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(18860.0579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(19163.4762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "57 tensor(19462.0863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "58 tensor(19818.8453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "59 tensor(20098.5773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "60 tensor(20522.6874, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "61 tensor(20925.9439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "62 tensor(21272.6104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "63 tensor(21704.2721, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "64 tensor(22031.4637, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "65 tensor(22371.6335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "66 tensor(22674.2127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "67 tensor(23071.6393, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "68 tensor(23400.3474, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "69 tensor(23758.5791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "70 tensor(24136.4132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "71 tensor(24485.3883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "72 tensor(24780.5286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "73 tensor(25073.2996, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "74 tensor(25421.0720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "75 tensor(25707.4942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "76 tensor(26030.4430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "77 tensor(26370.7580, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "78 tensor(26717.4469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "79 tensor(27071.3905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "80 tensor(27391.2000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "81 tensor(27684.8323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "82 tensor(28084.6849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "83 tensor(28416.4506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "84 tensor(28758.0836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "85 tensor(29051.4457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "86 tensor(29330.6102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "87 tensor(29628.0411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "88 tensor(29946.1833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "89 tensor(30222.5608, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "90 tensor(30547.0077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "91 tensor(30914.4205, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "92 tensor(31284.9162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "93 tensor(31648.3951, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimize_final_block(Y_pred_BNN_com, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(306.6597, dtype=torch.float64)\n",
      "1 tensor(526.6195, dtype=torch.float64)\n",
      "2 tensor(839.7867, dtype=torch.float64)\n",
      "3 tensor(1143.7093, dtype=torch.float64)\n",
      "4 tensor(1373.5122, dtype=torch.float64)\n",
      "5 tensor(1641.9703, dtype=torch.float64)\n",
      "6 tensor(1981.2830, dtype=torch.float64)\n",
      "7 tensor(2268.6638, dtype=torch.float64)\n",
      "8 tensor(2510.6073, dtype=torch.float64)\n",
      "9 tensor(2829.9765, dtype=torch.float64)\n",
      "10 tensor(3081.1508, dtype=torch.float64)\n",
      "11 tensor(3421.6138, dtype=torch.float64)\n",
      "12 tensor(3745.0731, dtype=torch.float64)\n",
      "13 tensor(4040.5331, dtype=torch.float64)\n",
      "14 tensor(4363.6344, dtype=torch.float64)\n",
      "15 tensor(4608.8839, dtype=torch.float64)\n",
      "16 tensor(4874.0525, dtype=torch.float64)\n",
      "17 tensor(5140.8082, dtype=torch.float64)\n",
      "18 tensor(5521.7318, dtype=torch.float64)\n",
      "19 tensor(5767.2329, dtype=torch.float64)\n",
      "20 tensor(6050.3973, dtype=torch.float64)\n",
      "21 tensor(6288.7275, dtype=torch.float64)\n",
      "22 tensor(6606.3052, dtype=torch.float64)\n",
      "23 tensor(6863.0680, dtype=torch.float64)\n",
      "24 tensor(7121.8004, dtype=torch.float64)\n",
      "25 tensor(7400.0876, dtype=torch.float64)\n",
      "26 tensor(7688.7031, dtype=torch.float64)\n",
      "27 tensor(8003.3485, dtype=torch.float64)\n",
      "28 tensor(8326.2770, dtype=torch.float64)\n",
      "29 tensor(8675.2162, dtype=torch.float64)\n",
      "30 tensor(8959.2614, dtype=torch.float64)\n",
      "31 tensor(9307.1605, dtype=torch.float64)\n",
      "32 tensor(9631.0568, dtype=torch.float64)\n",
      "33 tensor(9926.2095, dtype=torch.float64)\n",
      "34 tensor(10242.5358, dtype=torch.float64)\n",
      "35 tensor(10531.3442, dtype=torch.float64)\n",
      "36 tensor(10788.2284, dtype=torch.float64)\n",
      "37 tensor(11129.1373, dtype=torch.float64)\n",
      "38 tensor(11441.9330, dtype=torch.float64)\n",
      "39 tensor(11724.6946, dtype=torch.float64)\n",
      "40 tensor(12013.8126, dtype=torch.float64)\n",
      "41 tensor(12333.7109, dtype=torch.float64)\n",
      "42 tensor(12571.4791, dtype=torch.float64)\n",
      "43 tensor(12857.8945, dtype=torch.float64)\n",
      "44 tensor(13149.4888, dtype=torch.float64)\n",
      "45 tensor(13465.6775, dtype=torch.float64)\n",
      "46 tensor(13726.6022, dtype=torch.float64)\n",
      "47 tensor(14016.0037, dtype=torch.float64)\n",
      "48 tensor(14284.8654, dtype=torch.float64)\n",
      "49 tensor(14636.4565, dtype=torch.float64)\n",
      "50 tensor(14998.1593, dtype=torch.float64)\n",
      "51 tensor(15335.8229, dtype=torch.float64)\n",
      "52 tensor(15631.5052, dtype=torch.float64)\n",
      "53 tensor(15969.0491, dtype=torch.float64)\n",
      "54 tensor(16250.7102, dtype=torch.float64)\n",
      "55 tensor(16528.6646, dtype=torch.float64)\n",
      "56 tensor(16790.6312, dtype=torch.float64)\n",
      "57 tensor(17048.5273, dtype=torch.float64)\n",
      "58 tensor(17362.5213, dtype=torch.float64)\n",
      "59 tensor(17595.2276, dtype=torch.float64)\n",
      "60 tensor(17977.1399, dtype=torch.float64)\n",
      "61 tensor(18333.8870, dtype=torch.float64)\n",
      "62 tensor(18637.5085, dtype=torch.float64)\n",
      "63 tensor(19036.1955, dtype=torch.float64)\n",
      "64 tensor(19316.1017, dtype=torch.float64)\n",
      "65 tensor(19617.6296, dtype=torch.float64)\n",
      "66 tensor(19877.1682, dtype=torch.float64)\n",
      "67 tensor(20226.4637, dtype=torch.float64)\n",
      "68 tensor(20510.5394, dtype=torch.float64)\n",
      "69 tensor(20837.1123, dtype=torch.float64)\n",
      "70 tensor(21174.1131, dtype=torch.float64)\n",
      "71 tensor(21483.8090, dtype=torch.float64)\n",
      "72 tensor(21735.7238, dtype=torch.float64)\n",
      "73 tensor(21974.9889, dtype=torch.float64)\n",
      "74 tensor(22278.5878, dtype=torch.float64)\n",
      "75 tensor(22516.9986, dtype=torch.float64)\n",
      "76 tensor(22800.0711, dtype=torch.float64)\n",
      "77 tensor(23088.2133, dtype=torch.float64)\n",
      "78 tensor(23385.1605, dtype=torch.float64)\n",
      "79 tensor(23692.0560, dtype=torch.float64)\n",
      "80 tensor(23970.6376, dtype=torch.float64)\n",
      "81 tensor(24222.0678, dtype=torch.float64)\n",
      "82 tensor(24582.3942, dtype=torch.float64)\n",
      "83 tensor(24866.5449, dtype=torch.float64)\n",
      "84 tensor(25155.7978, dtype=torch.float64)\n",
      "85 tensor(25396.7315, dtype=torch.float64)\n",
      "86 tensor(25625.6799, dtype=torch.float64)\n",
      "87 tensor(25865.5271, dtype=torch.float64)\n",
      "88 tensor(26141.4948, dtype=torch.float64)\n",
      "89 tensor(26375.1976, dtype=torch.float64)\n",
      "90 tensor(26655.5967, dtype=torch.float64)\n",
      "91 tensor(26982.5962, dtype=torch.float64)\n",
      "92 tensor(27307.2621, dtype=torch.float64)\n",
      "93 tensor(27631.7304, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "optimize_final_block(Y_pred_flow_dec, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(310.6956, dtype=torch.float64)\n",
      "1 tensor(536.3654, dtype=torch.float64)\n",
      "2 tensor(858.5078, dtype=torch.float64)\n",
      "3 tensor(1164.1711, dtype=torch.float64)\n",
      "4 tensor(1398.0356, dtype=torch.float64)\n",
      "5 tensor(1666.5549, dtype=torch.float64)\n",
      "6 tensor(2011.1619, dtype=torch.float64)\n",
      "7 tensor(2303.1385, dtype=torch.float64)\n",
      "8 tensor(2545.9053, dtype=torch.float64)\n",
      "9 tensor(2865.7012, dtype=torch.float64)\n",
      "10 tensor(3115.8789, dtype=torch.float64)\n",
      "11 tensor(3459.9512, dtype=torch.float64)\n",
      "12 tensor(3786.4262, dtype=torch.float64)\n",
      "13 tensor(4080.6408, dtype=torch.float64)\n",
      "14 tensor(4406.1084, dtype=torch.float64)\n",
      "15 tensor(4654.1746, dtype=torch.float64)\n",
      "16 tensor(4919.8712, dtype=torch.float64)\n",
      "17 tensor(5187.0698, dtype=torch.float64)\n",
      "18 tensor(5564.7599, dtype=torch.float64)\n",
      "19 tensor(5817.6144, dtype=torch.float64)\n",
      "20 tensor(6110.3205, dtype=torch.float64)\n",
      "21 tensor(6349.8374, dtype=torch.float64)\n",
      "22 tensor(6668.7443, dtype=torch.float64)\n",
      "23 tensor(6925.2494, dtype=torch.float64)\n",
      "24 tensor(7192.6028, dtype=torch.float64)\n",
      "25 tensor(7477.7094, dtype=torch.float64)\n",
      "26 tensor(7764.0094, dtype=torch.float64)\n",
      "27 tensor(8080.0473, dtype=torch.float64)\n",
      "28 tensor(8407.4018, dtype=torch.float64)\n",
      "29 tensor(8757.0244, dtype=torch.float64)\n",
      "30 tensor(9048.9565, dtype=torch.float64)\n",
      "31 tensor(9395.1000, dtype=torch.float64)\n",
      "32 tensor(9717.7732, dtype=torch.float64)\n",
      "33 tensor(10026.1509, dtype=torch.float64)\n",
      "34 tensor(10343.6148, dtype=torch.float64)\n",
      "35 tensor(10651.6573, dtype=torch.float64)\n",
      "36 tensor(10919.4248, dtype=torch.float64)\n",
      "37 tensor(11265.6257, dtype=torch.float64)\n",
      "38 tensor(11584.8291, dtype=torch.float64)\n",
      "39 tensor(11869.2866, dtype=torch.float64)\n",
      "40 tensor(12173.7935, dtype=torch.float64)\n",
      "41 tensor(12490.8242, dtype=torch.float64)\n",
      "42 tensor(12732.7402, dtype=torch.float64)\n",
      "43 tensor(13024.2946, dtype=torch.float64)\n",
      "44 tensor(13318.5975, dtype=torch.float64)\n",
      "45 tensor(13635.3428, dtype=torch.float64)\n",
      "46 tensor(13899.1653, dtype=torch.float64)\n",
      "47 tensor(14187.9810, dtype=torch.float64)\n",
      "48 tensor(14457.7254, dtype=torch.float64)\n",
      "49 tensor(14814.3937, dtype=torch.float64)\n",
      "50 tensor(15183.2223, dtype=torch.float64)\n",
      "51 tensor(15522.2998, dtype=torch.float64)\n",
      "52 tensor(15829.7390, dtype=torch.float64)\n",
      "53 tensor(16172.9877, dtype=torch.float64)\n",
      "54 tensor(16461.5579, dtype=torch.float64)\n",
      "55 tensor(16753.9476, dtype=torch.float64)\n",
      "56 tensor(17025.9780, dtype=torch.float64)\n",
      "57 tensor(17296.1845, dtype=torch.float64)\n",
      "58 tensor(17625.8565, dtype=torch.float64)\n",
      "59 tensor(17867.4946, dtype=torch.float64)\n",
      "60 tensor(18252.7059, dtype=torch.float64)\n",
      "61 tensor(18620.1062, dtype=torch.float64)\n",
      "62 tensor(18930.2779, dtype=torch.float64)\n",
      "63 tensor(19334.6362, dtype=torch.float64)\n",
      "64 tensor(19620.4509, dtype=torch.float64)\n",
      "65 tensor(19922.2715, dtype=torch.float64)\n",
      "66 tensor(20192.1507, dtype=torch.float64)\n",
      "67 tensor(20549.4563, dtype=torch.float64)\n",
      "68 tensor(20847.1725, dtype=torch.float64)\n",
      "69 tensor(21177.4535, dtype=torch.float64)\n",
      "70 tensor(21521.9442, dtype=torch.float64)\n",
      "71 tensor(21844.0080, dtype=torch.float64)\n",
      "72 tensor(22106.7880, dtype=torch.float64)\n",
      "73 tensor(22352.1611, dtype=torch.float64)\n",
      "74 tensor(22661.3645, dtype=torch.float64)\n",
      "75 tensor(22899.6388, dtype=torch.float64)\n",
      "76 tensor(23189.1511, dtype=torch.float64)\n",
      "77 tensor(23477.9274, dtype=torch.float64)\n",
      "78 tensor(23784.8098, dtype=torch.float64)\n",
      "79 tensor(24093.5681, dtype=torch.float64)\n",
      "80 tensor(24375.9821, dtype=torch.float64)\n",
      "81 tensor(24633.6572, dtype=torch.float64)\n",
      "82 tensor(24997.4362, dtype=torch.float64)\n",
      "83 tensor(25293.0189, dtype=torch.float64)\n",
      "84 tensor(25588.2135, dtype=torch.float64)\n",
      "85 tensor(25835.7917, dtype=torch.float64)\n",
      "86 tensor(26069.1856, dtype=torch.float64)\n",
      "87 tensor(26317.8034, dtype=torch.float64)\n",
      "88 tensor(26605.8987, dtype=torch.float64)\n",
      "89 tensor(26848.7125, dtype=torch.float64)\n",
      "90 tensor(27137.4742, dtype=torch.float64)\n",
      "91 tensor(27467.2046, dtype=torch.float64)\n",
      "92 tensor(27796.0827, dtype=torch.float64)\n",
      "93 tensor(28126.8280, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "optimize_final_block(Y_pred_flow_com, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_pred_ANN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     z_star \u001b[38;5;241m=\u001b[39m newsvendor_solve_kkt\u001b[38;5;241m.\u001b[39mforward(y_pred)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z_star\n\u001b[0;32m----> 5\u001b[0m n_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(\u001b[43mY_pred_ANN\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39mBATCH_SIZE_LOADER))\n\u001b[1;32m      7\u001b[0m f_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m f_total_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_pred_ANN' is not defined"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_ANN_dec.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_ANN_dec.shape[1]\n",
    "    f_total += cost_fn(Y_pred_ANN_dec[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(336.8555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "1 tensor(594.3570, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "2 tensor(939.0217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "3 tensor(1272.9802, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "4 tensor(1544.8554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "5 tensor(1846.9580, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "6 tensor(2217.8957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "7 tensor(2530.7089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "8 tensor(2803.0723, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "9 tensor(3155.3992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "10 tensor(3440.7913, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "11 tensor(3810.9939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "12 tensor(4165.9960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "13 tensor(4488.8725, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "14 tensor(4842.5601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "15 tensor(5129.6196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "16 tensor(5435.7308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "17 tensor(5738.7095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "18 tensor(6142.8401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "19 tensor(6419.3826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "20 tensor(6729.1454, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "21 tensor(7004.5833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "22 tensor(7351.0371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "23 tensor(7634.8152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "24 tensor(7933.4784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "25 tensor(8249.2138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "26 tensor(8563.2387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "27 tensor(8906.5118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "28 tensor(9253.2276, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "29 tensor(9626.8620, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "30 tensor(9950.8154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "31 tensor(10325.7440, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "32 tensor(10671.6298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "33 tensor(10998.3855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "34 tensor(11331.5065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "35 tensor(11658.4866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "36 tensor(11947.4956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "37 tensor(12317.5215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "38 tensor(12665.3926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "39 tensor(12973.3671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "40 tensor(13288.7873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "41 tensor(13622.4729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "42 tensor(13898.7888, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "43 tensor(14206.2379, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "44 tensor(14523.5398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "45 tensor(14866.3453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "46 tensor(15162.8147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "47 tensor(15489.3624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "48 tensor(15799.0184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "49 tensor(16176.5831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "50 tensor(16562.2148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "51 tensor(16930.1173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "52 tensor(17256.5076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "53 tensor(17624.6489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "54 tensor(17936.2204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "55 tensor(18254.0997, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "56 tensor(18544.2541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "57 tensor(18836.5862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "58 tensor(19191.2915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "59 tensor(19466.1452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "60 tensor(19874.3854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "61 tensor(20255.5188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "62 tensor(20592.0825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "63 tensor(21021.2190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "64 tensor(21333.7579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "65 tensor(21668.9122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "66 tensor(21965.6628, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "67 tensor(22346.4036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "68 tensor(22661.8011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "69 tensor(23017.9249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "70 tensor(23382.9509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "71 tensor(23729.8051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "72 tensor(24011.5874, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "73 tensor(24279.9650, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "74 tensor(24613.2679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "75 tensor(24892.7567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "76 tensor(25205.4492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "77 tensor(25528.0505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "78 tensor(25858.3867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "79 tensor(26200.0221, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "80 tensor(26514.3906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "81 tensor(26790.0375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "82 tensor(27178.1189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "83 tensor(27502.6987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "84 tensor(27826.7992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "85 tensor(28110.3447, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "86 tensor(28380.7793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "87 tensor(28655.4487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "88 tensor(28965.0443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "89 tensor(29232.8065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "90 tensor(29536.9023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "91 tensor(29888.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "92 tensor(30243.5556, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "93 tensor(30606.0845, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_BNN.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_BNN.shape[1]\n",
    "    f_total += cost_fn(Y_pred_BNN[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(312.2514, dtype=torch.float64)\n",
      "1 tensor(543.1643, dtype=torch.float64)\n",
      "2 tensor(868.0362, dtype=torch.float64)\n",
      "3 tensor(1178.5969, dtype=torch.float64)\n",
      "4 tensor(1417.3065, dtype=torch.float64)\n",
      "5 tensor(1686.3286, dtype=torch.float64)\n",
      "6 tensor(2031.0811, dtype=torch.float64)\n",
      "7 tensor(2325.3115, dtype=torch.float64)\n",
      "8 tensor(2574.8363, dtype=torch.float64)\n",
      "9 tensor(2899.7800, dtype=torch.float64)\n",
      "10 tensor(3151.4408, dtype=torch.float64)\n",
      "11 tensor(3497.7524, dtype=torch.float64)\n",
      "12 tensor(3825.7398, dtype=torch.float64)\n",
      "13 tensor(4123.1164, dtype=torch.float64)\n",
      "14 tensor(4450.7636, dtype=torch.float64)\n",
      "15 tensor(4703.3666, dtype=torch.float64)\n",
      "16 tensor(4968.6834, dtype=torch.float64)\n",
      "17 tensor(5239.2102, dtype=torch.float64)\n",
      "18 tensor(5619.9611, dtype=torch.float64)\n",
      "19 tensor(5873.2957, dtype=torch.float64)\n",
      "20 tensor(6163.6674, dtype=torch.float64)\n",
      "21 tensor(6407.2110, dtype=torch.float64)\n",
      "22 tensor(6728.7920, dtype=torch.float64)\n",
      "23 tensor(6991.6152, dtype=torch.float64)\n",
      "24 tensor(7262.0976, dtype=torch.float64)\n",
      "25 tensor(7553.3282, dtype=torch.float64)\n",
      "26 tensor(7847.7063, dtype=torch.float64)\n",
      "27 tensor(8172.1495, dtype=torch.float64)\n",
      "28 tensor(8500.1389, dtype=torch.float64)\n",
      "29 tensor(8856.9783, dtype=torch.float64)\n",
      "30 tensor(9154.6663, dtype=torch.float64)\n",
      "31 tensor(9506.5847, dtype=torch.float64)\n",
      "32 tensor(9839.0192, dtype=torch.float64)\n",
      "33 tensor(10154.8747, dtype=torch.float64)\n",
      "34 tensor(10481.3105, dtype=torch.float64)\n",
      "35 tensor(10788.4263, dtype=torch.float64)\n",
      "36 tensor(11062.1850, dtype=torch.float64)\n",
      "37 tensor(11420.8974, dtype=torch.float64)\n",
      "38 tensor(11739.3228, dtype=torch.float64)\n",
      "39 tensor(12029.0972, dtype=torch.float64)\n",
      "40 tensor(12331.9577, dtype=torch.float64)\n",
      "41 tensor(12653.9796, dtype=torch.float64)\n",
      "42 tensor(12903.3277, dtype=torch.float64)\n",
      "43 tensor(13196.4177, dtype=torch.float64)\n",
      "44 tensor(13497.0868, dtype=torch.float64)\n",
      "45 tensor(13818.2847, dtype=torch.float64)\n",
      "46 tensor(14083.3266, dtype=torch.float64)\n",
      "47 tensor(14377.0773, dtype=torch.float64)\n",
      "48 tensor(14651.1158, dtype=torch.float64)\n",
      "49 tensor(15010.7214, dtype=torch.float64)\n",
      "50 tensor(15384.5880, dtype=torch.float64)\n",
      "51 tensor(15728.2681, dtype=torch.float64)\n",
      "52 tensor(16036.2837, dtype=torch.float64)\n",
      "53 tensor(16381.3402, dtype=torch.float64)\n",
      "54 tensor(16672.9076, dtype=torch.float64)\n",
      "55 tensor(16963.0410, dtype=torch.float64)\n",
      "56 tensor(17244.6010, dtype=torch.float64)\n",
      "57 tensor(17517.4626, dtype=torch.float64)\n",
      "58 tensor(17850.4844, dtype=torch.float64)\n",
      "59 tensor(18094.9839, dtype=torch.float64)\n",
      "60 tensor(18490.0436, dtype=torch.float64)\n",
      "61 tensor(18860.1215, dtype=torch.float64)\n",
      "62 tensor(19176.5045, dtype=torch.float64)\n",
      "63 tensor(19593.2849, dtype=torch.float64)\n",
      "64 tensor(19884.0330, dtype=torch.float64)\n",
      "65 tensor(20189.9820, dtype=torch.float64)\n",
      "66 tensor(20462.3456, dtype=torch.float64)\n",
      "67 tensor(20823.9748, dtype=torch.float64)\n",
      "68 tensor(21125.3393, dtype=torch.float64)\n",
      "69 tensor(21461.1647, dtype=torch.float64)\n",
      "70 tensor(21814.1556, dtype=torch.float64)\n",
      "71 tensor(22140.3290, dtype=torch.float64)\n",
      "72 tensor(22403.1669, dtype=torch.float64)\n",
      "73 tensor(22658.2938, dtype=torch.float64)\n",
      "74 tensor(22969.7253, dtype=torch.float64)\n",
      "75 tensor(23215.7127, dtype=torch.float64)\n",
      "76 tensor(23510.7934, dtype=torch.float64)\n",
      "77 tensor(23805.1698, dtype=torch.float64)\n",
      "78 tensor(24118.5181, dtype=torch.float64)\n",
      "79 tensor(24434.0557, dtype=torch.float64)\n",
      "80 tensor(24718.6160, dtype=torch.float64)\n",
      "81 tensor(24981.0210, dtype=torch.float64)\n",
      "82 tensor(25349.4204, dtype=torch.float64)\n",
      "83 tensor(25649.8274, dtype=torch.float64)\n",
      "84 tensor(25949.8088, dtype=torch.float64)\n",
      "85 tensor(26203.5007, dtype=torch.float64)\n",
      "86 tensor(26442.1905, dtype=torch.float64)\n",
      "87 tensor(26695.3945, dtype=torch.float64)\n",
      "88 tensor(26989.3405, dtype=torch.float64)\n",
      "89 tensor(27240.5679, dtype=torch.float64)\n",
      "90 tensor(27529.0395, dtype=torch.float64)\n",
      "91 tensor(27865.6737, dtype=torch.float64)\n",
      "92 tensor(28206.2706, dtype=torch.float64)\n",
      "93 tensor(28538.7856, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt_M.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_pred_flow.shape[1]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_pred_flow.shape[1]\n",
    "    f_total += cost_fn(Y_pred_flow[:,i_low:i_up,:], Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(b, f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(277.8333, dtype=torch.float64)\n",
      "tensor(466.7020, dtype=torch.float64)\n",
      "tensor(756.3428, dtype=torch.float64)\n",
      "tensor(1038.8198, dtype=torch.float64)\n",
      "tensor(1241.8659, dtype=torch.float64)\n",
      "tensor(1478.8375, dtype=torch.float64)\n",
      "tensor(1786.0079, dtype=torch.float64)\n",
      "tensor(2042.8452, dtype=torch.float64)\n",
      "tensor(2252.8012, dtype=torch.float64)\n",
      "tensor(2544.8384, dtype=torch.float64)\n",
      "tensor(2767.0454, dtype=torch.float64)\n",
      "tensor(3077.3832, dtype=torch.float64)\n",
      "tensor(3373.4957, dtype=torch.float64)\n",
      "tensor(3640.8854, dtype=torch.float64)\n",
      "tensor(3937.2814, dtype=torch.float64)\n",
      "tensor(4155.9473, dtype=torch.float64)\n",
      "tensor(4383.6695, dtype=torch.float64)\n",
      "tensor(4621.0869, dtype=torch.float64)\n",
      "tensor(4970.8880, dtype=torch.float64)\n",
      "tensor(5186.6554, dtype=torch.float64)\n",
      "tensor(5439.6769, dtype=torch.float64)\n",
      "tensor(5652.0196, dtype=torch.float64)\n",
      "tensor(5935.8175, dtype=torch.float64)\n",
      "tensor(6155.1779, dtype=torch.float64)\n",
      "tensor(6385.4286, dtype=torch.float64)\n",
      "tensor(6632.4113, dtype=torch.float64)\n",
      "tensor(6885.3581, dtype=torch.float64)\n",
      "tensor(7174.2726, dtype=torch.float64)\n",
      "tensor(7472.0028, dtype=torch.float64)\n",
      "tensor(7794.4824, dtype=torch.float64)\n",
      "tensor(8049.1856, dtype=torch.float64)\n",
      "tensor(8368.1417, dtype=torch.float64)\n",
      "tensor(8661.3855, dtype=torch.float64)\n",
      "tensor(8931.9095, dtype=torch.float64)\n",
      "tensor(9216.2141, dtype=torch.float64)\n",
      "tensor(9480.3732, dtype=torch.float64)\n",
      "tensor(9709.5000, dtype=torch.float64)\n",
      "tensor(10020.7096, dtype=torch.float64)\n",
      "tensor(10301.5886, dtype=torch.float64)\n",
      "tensor(10555.8059, dtype=torch.float64)\n",
      "tensor(10815.4595, dtype=torch.float64)\n",
      "tensor(11097.8981, dtype=torch.float64)\n",
      "tensor(11306.0188, dtype=torch.float64)\n",
      "tensor(11566.7714, dtype=torch.float64)\n",
      "tensor(11829.6015, dtype=torch.float64)\n",
      "tensor(12118.9071, dtype=torch.float64)\n",
      "tensor(12342.4156, dtype=torch.float64)\n",
      "tensor(12595.8724, dtype=torch.float64)\n",
      "tensor(12832.2343, dtype=torch.float64)\n",
      "tensor(13160.6985, dtype=torch.float64)\n",
      "tensor(13495.3000, dtype=torch.float64)\n",
      "tensor(13804.0394, dtype=torch.float64)\n",
      "tensor(14075.0764, dtype=torch.float64)\n",
      "tensor(14385.1547, dtype=torch.float64)\n",
      "tensor(14634.4969, dtype=torch.float64)\n",
      "tensor(14882.1878, dtype=torch.float64)\n",
      "tensor(15115.7999, dtype=torch.float64)\n",
      "tensor(15346.8898, dtype=torch.float64)\n",
      "tensor(15630.4102, dtype=torch.float64)\n",
      "tensor(15834.1375, dtype=torch.float64)\n",
      "tensor(16185.3529, dtype=torch.float64)\n",
      "tensor(16518.2424, dtype=torch.float64)\n",
      "tensor(16800.3364, dtype=torch.float64)\n",
      "tensor(17169.0509, dtype=torch.float64)\n",
      "tensor(17425.5241, dtype=torch.float64)\n",
      "tensor(17694.2902, dtype=torch.float64)\n",
      "tensor(17927.5019, dtype=torch.float64)\n",
      "tensor(18249.7518, dtype=torch.float64)\n",
      "tensor(18504.3545, dtype=torch.float64)\n",
      "tensor(18805.9207, dtype=torch.float64)\n",
      "tensor(19117.0750, dtype=torch.float64)\n",
      "tensor(19399.0479, dtype=torch.float64)\n",
      "tensor(19619.3532, dtype=torch.float64)\n",
      "tensor(19828.9731, dtype=torch.float64)\n",
      "tensor(20103.4728, dtype=torch.float64)\n",
      "tensor(20312.1055, dtype=torch.float64)\n",
      "tensor(20567.6985, dtype=torch.float64)\n",
      "tensor(20827.2814, dtype=torch.float64)\n",
      "tensor(21094.9068, dtype=torch.float64)\n",
      "tensor(21379.5182, dtype=torch.float64)\n",
      "tensor(21627.5219, dtype=torch.float64)\n",
      "tensor(21845.1690, dtype=torch.float64)\n",
      "tensor(22177.2573, dtype=torch.float64)\n",
      "tensor(22431.1728, dtype=torch.float64)\n",
      "tensor(22694.2598, dtype=torch.float64)\n",
      "tensor(22905.1768, dtype=torch.float64)\n",
      "tensor(23108.4414, dtype=torch.float64)\n",
      "tensor(23316.9349, dtype=torch.float64)\n",
      "tensor(23570.3472, dtype=torch.float64)\n",
      "tensor(23777.1723, dtype=torch.float64)\n",
      "tensor(24026.7814, dtype=torch.float64)\n",
      "tensor(24327.1554, dtype=torch.float64)\n",
      "tensor(24626.9370, dtype=torch.float64)\n",
      "tensor(24921.1248, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star\n",
    "\n",
    "n_batches = int(np.ceil(Y_val_original.shape[0]/BATCH_SIZE_LOADER))\n",
    "\n",
    "f_total = 0\n",
    "f_total_best = 0\n",
    "\n",
    "for b in range(0, n_batches):\n",
    "    i_low = b*BATCH_SIZE_LOADER\n",
    "    i_up = (b+1)*BATCH_SIZE_LOADER\n",
    "    if b == n_batches-1:\n",
    "        i_up = n_batches*Y_val_original.shape[0]\n",
    "    f_total += cost_fn(Y_val_original[i_low:i_up,:].unsqueeze(0), Y_val_original[i_low:i_up,:])/n_batches\n",
    "    print(f_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3777.6631, 3777.6631, 3777.6631, 3777.6631, 3067.9280, 3777.6631,\n",
       "        3777.6631, 3777.6631, 3777.6631, 3777.6631, 3777.6631, 3777.6631,\n",
       "        3777.6631, 3777.6631, 3496.4890, 3777.6631, 3777.6631, 3777.6631,\n",
       "        3306.2300, 3777.6631, 3777.6631, 3659.2740, 3777.6631, 3777.6631,\n",
       "        3777.6631, 3777.6631, 3710.0201, 3777.6631, 3777.6631, 3223.4030,\n",
       "        3777.6631, 2657.2169, 3777.6631, 3338.9631, 3717.9910, 3777.6631,\n",
       "        3777.6631, 3777.6631, 3777.6631, 3777.6631, 3777.6631, 3777.6631,\n",
       "        3777.6631, 3584.3250, 3777.6631, 2956.0880, 3777.6631, 3777.6631,\n",
       "        3777.6631, 3777.6631], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(argmin_solver(reshape_outcomes(Y_val_original[0:50,:].unsqueeze(0)))*params_t['pr']).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.83 1.64 2.43 28104 27870 27787 13598\n",
    "1.87 1.89 2.46 42607 42070 41574 28124\n",
    "1.80 1.85 2.25 34329 33855 33395 15849\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
