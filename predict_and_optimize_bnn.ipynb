{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import feat_eng as fe\n",
    "import data_selector_items as dsi\n",
    "import params_newsvendor as prm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mip import Model, xsum, minimize, INTEGER, CONTINUOUS, CutType, OptimizationStatus\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 0\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of data files\n",
    "path_data = './data/'\n",
    "\n",
    "# Read historical data\n",
    "sales = pd.read_csv(path_data + 'sales_train_evaluation.csv')\n",
    "\n",
    "# Spliting the data in days\n",
    "start_tr_day = 500\n",
    "start_val_day = 1742\n",
    "start_test_day = 1842\n",
    "end_day = 1941\n",
    "\n",
    "# N of items to use (Lower will be much faster)\n",
    "# In the paper, this is d_z\n",
    "n_items = 4\n",
    "\n",
    "# All useful items\n",
    "sku_ids = dsi.select_items(sales, start_tr_day)\n",
    "\n",
    "# Sample only n_items to use\n",
    "random.seed(seed_number)\n",
    "sku_ids = random.sample(sku_ids, n_items)\n",
    "sku_ids = list(set(sku_ids))\n",
    "n_items = len(sku_ids)\n",
    "\n",
    "\n",
    "# Build training and test from historical data\n",
    "data_train, data_val, data_test, feat, n_items = fe.build_data(\n",
    "    path_data, sales, sku_ids, \n",
    "    start_tr_day, start_val_day, start_test_day, end_day)\n",
    "\n",
    "data_train.fillna(0, inplace=True)\n",
    "data_val.fillna(0, inplace=True)\n",
    "data_test.fillna(0, inplace=True)\n",
    "\n",
    "dx = len(feat)\n",
    "\n",
    "# Number of batch_size samples in the SGDs\n",
    "batch_size = 32 # Number of days for combined approaches\n",
    "\n",
    "# Here we change a bit the test data in order to increase the integrality gap \n",
    "# of the optimization problem (didactic purpose). Otherwise the Continuous and \n",
    "# Discrete version would have approximately the same results.\n",
    "data_train['qty'] = data_train['qty']*np.random.normal(1, 0.07)\n",
    "data_val['qty'] = data_val['qty']*np.random.normal(1, 0.07)\n",
    "data_test['qty'] = data_test['qty']*np.random.normal(1, 0.07)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_train[feat])\n",
    "\n",
    "data_train.loc[:, feat] = scaler.transform(data_train[feat])\n",
    "data_val.loc[:, feat] = scaler.transform(data_val[feat])\n",
    "data_test.loc[:, feat] = scaler.transform(data_test[feat])\n",
    "        \n",
    "X_train = torch.tensor(np.array(data_train[feat]).astype('float32'), requires_grad= True, device=dev)\n",
    "y_train = torch.tensor(np.array(data_train['qty']).astype('float32'), requires_grad= True, device=dev)\n",
    "\n",
    "X_val = torch.tensor(np.array(data_val[feat]).astype('float32'), requires_grad= True, device=dev)\n",
    "y_val = torch.tensor(np.array(data_val['qty']).astype('float32'), requires_grad= True, device=dev)\n",
    "\n",
    "X_test = torch.tensor(np.array(data_test[feat]).astype('float32'), requires_grad= True, device=dev)\n",
    "y_test = torch.tensor(np.array(data_test['qty']).astype('float32'), requires_grad= True, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data):\n",
    "    batches_idx = []\n",
    "    n_batches = int(np.floor(data['d'].nunique() / batch_size))\n",
    "    for i in range(0, n_batches):\n",
    "        days = data['d'].unique()\n",
    "        idx = data[data['d'].isin(\n",
    "            np.random.choice(days, batch_size, replace=False))].index.tolist()\n",
    "        if len(idx) == n_items*batch_size:\n",
    "            batches_idx.append(idx)\n",
    "    return batches_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = len(generate_batches(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t, params_np = prm.get_params(n_items, is_discrete=False, \n",
    "                                     q_factor = 0.01, # Quadratic penalty factor\n",
    "                                     seed_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of deterministic parameters: tensor([147., 109.])\n"
     ]
    }
   ],
   "source": [
    "print('Example of deterministic parameters:', params_t['cs'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_item = lambda Z, Y : params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "\n",
    "def reshape_outcomes(y_pred, y):\n",
    "    y_pred = torch.reshape(y_pred, (y_pred.shape[0]//n_items, n_items))\n",
    "    y = torch.reshape(y, (y.shape[0]//n_items, n_items))\n",
    "    return y_pred, y\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred, y = reshape_outcomes(y_pred, y)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f\n",
    "\n",
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total\n",
    "\n",
    "# Analytical solution to find the argmin\n",
    "# This function allows autograd (backpropagation)\n",
    "def argmin_solver(y_pred):\n",
    "    z_star = y_pred\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(X_train, y_train, loss_function, optimizer, model):\n",
    "    \n",
    "    batches_idx = generate_batches(data_train)\n",
    "    \n",
    "    for b in batches_idx:\n",
    "        x_tr = X_train[b]\n",
    "        y_tr = y_train[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #preds = model(x_tr)\n",
    "\n",
    "        train_loss = loss_function(x_tr, y_tr.reshape(-1))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(X, y, val_function, data):\n",
    "    \n",
    "    batches_idx = generate_batches(data)\n",
    "    f_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in batches_idx:\n",
    "            x_ = X[b]\n",
    "            y_ = y[b]\n",
    "    \n",
    "            f_ = val_function(x_, y_.reshape(-1))  \n",
    "            f_sum = f_sum + f_\n",
    "\n",
    "    return f_sum/len(batches_idx)\n",
    "\n",
    "\n",
    "def validate_one_epoch_final_cost(X, y, model, data):\n",
    "    \n",
    "    batches_idx = generate_batches(data)\n",
    "    f_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in batches_idx:\n",
    "            x_ = X[b]\n",
    "            y_ = y[b]\n",
    "    \n",
    "            f_ = cost_fn(model(x_).reshape(-1), y_.reshape(-1))  \n",
    "            f_sum = f_sum + f_\n",
    "\n",
    "    return f_sum/len(batches_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0,1)\n",
    "    \n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    \n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.rho.size()).to(dev)\n",
    "        return self.mu + self.sigma * epsilon\n",
    "    \n",
    "    def log_prob(self, input):\n",
    "        return (-math.log(math.sqrt(2 * math.pi))\n",
    "                - torch.log(self.sigma)\n",
    "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()\n",
    "\n",
    "class ScaleMixtureGaussian(object):\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
    "    \n",
    "    def log_prob(self, input):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
    "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
    "        return (torch.log(self.pi * prob1 + (1-self.pi) * prob2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = 0.5\n",
    "SIGMA_1 = torch.FloatTensor([math.exp(-0)], device=dev)\n",
    "SIGMA_2 = torch.FloatTensor([math.exp(-6)], device=dev)\n",
    "n_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t, n_samples):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "            \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items  \n",
    "        self.n_samples = n_samples\n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident_samples = torch.eye(n_items*n_samples)\n",
    "        ident3 = torch.eye(n_items + 2*n_items*n_samples)\n",
    "        zeros_matrix = torch.zeros((n_items*n_samples, n_items*n_samples))\n",
    "        zeros_array = torch.zeros(n_items*n_samples)\n",
    "        ones_array = torch.ones(n_items*n_samples)\n",
    "             \n",
    "        self.Q = torch.diag(\n",
    "            torch.hstack(\n",
    "                (\n",
    "                    params_t['q'], \n",
    "                    (1/n_samples)*params_t['qs'].repeat(n_samples), \n",
    "                    (1/n_samples)*params_t['qw'].repeat(n_samples)\n",
    "                )\n",
    "            )).to(dev)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.hstack(\n",
    "                                (\n",
    "                                    params_t['c'], \n",
    "                                    (1/n_samples)*params_t['cs'].repeat(n_samples), \n",
    "                                    (1/n_samples)*params_t['cw'].repeat(n_samples)\n",
    "                                )).to(dev)\n",
    "             \n",
    "            \n",
    "        shortage_ineq = torch.hstack(\n",
    "            (\n",
    "                -ident.repeat_interleave(n_samples, 0), \n",
    "                -ident_samples, \n",
    "                zeros_matrix\n",
    "            )\n",
    "        )  \n",
    "        \n",
    "        \n",
    "        excess_ineq = torch.hstack(\n",
    "            (\n",
    "                ident.repeat_interleave(n_samples, 0), \n",
    "                zeros_matrix, \n",
    "                -ident_samples\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        price_ineq = torch.hstack(\n",
    "            (\n",
    "                params_t['pr'], \n",
    "                zeros_array, \n",
    "                zeros_array\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        \n",
    "        self.ineqs = torch.vstack(\n",
    "            (\n",
    "                shortage_ineq, \n",
    "                excess_ineq, \n",
    "                price_ineq, \n",
    "                positive_ineq\n",
    "            )\n",
    "        ).to(dev)\n",
    "\n",
    "        \n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        \n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items*n_samples), \n",
    "                                          torch.zeros(n_items*n_samples))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_batches, n_items = y.size()\n",
    "        \n",
    "        assert self.n_items == n_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(n_batches, Q.size(0), Q.size(1))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(n_batches, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            n_batches, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), self.lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Weight parameters\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5,-4))\n",
    "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
    "        # Bias parameters\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5,-4))\n",
    "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
    "        # Prior distributions\n",
    "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
    "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
    "        if self.training or sample:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "        else:\n",
    "            weight = self.weight.mu\n",
    "            bias = self.bias.mu\n",
    "        if self.training or calculate_log_probs:\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            self.log_prior, self.log_variational_posterior = 0, 0\n",
    "\n",
    "        return F.linear(input, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super().__init__()\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.l1 = BayesianLinear(n_feat, 400)\n",
    "        self.l2 = BayesianLinear(400, 400)\n",
    "        self.l3 = BayesianLinear(400, 1)\n",
    "    \n",
    "    def forward(self, x, sample=False):\n",
    "        x = self.act1(self.l1(x, sample))\n",
    "        x = self.act1(self.l2(x, sample))\n",
    "        x = self.l3(x, sample)\n",
    "        return x\n",
    "    \n",
    "    def forward_uncertain(self, X, n_samples):\n",
    "        y = torch.zeros(n_samples, X.shape[0]).to(dev)\n",
    "        for i in range(n_samples):\n",
    "            y[i] = self(X, sample=True).reshape(-1)\n",
    "        return y\n",
    "    \n",
    "    def log_prior(self):\n",
    "        return self.l1.log_prior \\\n",
    "               + self.l2.log_prior \\\n",
    "               + self.l3.log_prior\n",
    "    \n",
    "    def log_variational_posterior(self):\n",
    "        return self.l1.log_variational_posterior \\\n",
    "               + self.l2.log_variational_posterior \\\n",
    "               + self.l3.log_variational_posterior\n",
    "    \n",
    "    def sample_elbo(self, input, target, n_samples=n_samples):\n",
    "        log_priors = torch.zeros(n_samples).to(dev)\n",
    "        log_variational_posteriors = torch.zeros(n_samples).to(dev)\n",
    "        outputs = self.forward_uncertain(input, n_samples)\n",
    "        for i in range(n_samples):\n",
    "            log_priors[i] = self.log_prior()\n",
    "            log_variational_posteriors[i] = self.log_variational_posterior()\n",
    "        log_prior = log_priors.mean()\n",
    "        log_variational_posterior = log_variational_posteriors.mean()\n",
    "        mse = F.mse_loss(outputs.mean(0), target, size_average=False)\n",
    "        loss = (log_variational_posterior - log_prior)/NUM_BATCHES + 1*mse\n",
    "        return loss#, log_prior, log_variational_posterior, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_unc = BayesianNetwork(n_feat=dx)\n",
    "opt_h_unc = torch.optim.Adam(h_unc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (24) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m validate_one_epoch(X_train, y_train, h_unc\u001b[38;5;241m.\u001b[39msample_elbo, data_train)\n\u001b[1;32m     10\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m validate_one_epoch(X_val, y_val, h_unc\u001b[38;5;241m.\u001b[39msample_elbo, data_val)\n\u001b[0;32m---> 12\u001b[0m f_train \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_one_epoch_final_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_unc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m f_val \u001b[38;5;241m=\u001b[39m validate_one_epoch_final_cost(X_val, y_val, h_unc, data_val)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#rmse_costs_sep.append(val_rmse_sep.data.item())\u001b[39;00m\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mvalidate_one_epoch_final_cost\u001b[0;34m(X, y, model, data)\u001b[0m\n\u001b[1;32m     40\u001b[0m         x_ \u001b[38;5;241m=\u001b[39m X[b]\n\u001b[1;32m     41\u001b[0m         y_ \u001b[38;5;241m=\u001b[39m y[b]\n\u001b[0;32m---> 43\u001b[0m         f_ \u001b[38;5;241m=\u001b[39m \u001b[43mcost_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     44\u001b[0m         f_sum \u001b[38;5;241m=\u001b[39m f_sum \u001b[38;5;241m+\u001b[39m f_\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f_sum\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(batches_idx)\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mcost_fn\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_fn\u001b[39m(y_pred, y):\n\u001b[0;32m---> 23\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_f_per_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     f_total \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_total\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mcalc_f_per_day\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_f_per_day\u001b[39m(y_pred, y):\n\u001b[0;32m---> 18\u001b[0m     f_per_item \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_f_por_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(f_per_item, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mcalc_f_por_item\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_f_por_item\u001b[39m(y_pred, y):\n\u001b[1;32m     12\u001b[0m     y_pred, y \u001b[38;5;241m=\u001b[39m reshape_outcomes(y_pred, y)\n\u001b[0;32m---> 13\u001b[0m     z_star \u001b[38;5;241m=\u001b[39m  \u001b[43margmin_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     f_per_item \u001b[38;5;241m=\u001b[39m cost_per_item(z_star, y)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_per_item\n",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36margmin_solver\u001b[0;34m(y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmin_solver\u001b[39m(y_pred):\n\u001b[0;32m----> 2\u001b[0m     z_star \u001b[38;5;241m=\u001b[39m \u001b[43mnewsvendor_solve_kkt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z_star\n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mSolveNewsvendorWithKKT.forward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m ineqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mineqs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    103\u001b[0m ineqs \u001b[38;5;241m=\u001b[39m ineqs\u001b[38;5;241m.\u001b[39mexpand(n_batches, ineqs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], ineqs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])       \n\u001b[0;32m--> 105\u001b[0m uncert_bound \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muncert_bound\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m determ_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterm_bound\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\n\u001b[1;32m    107\u001b[0m     n_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterm_bound\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    108\u001b[0m bound \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack((uncert_bound, determ_bound))     \n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (24) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "rmse_costs_sep = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    train_one_epoch(X_train, y_train, h_unc.sample_elbo, opt_h_unc, h_unc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_train = validate_one_epoch(X_train, y_train, h_unc.sample_elbo, data_train)\n",
    "        loss_val = validate_one_epoch(X_val, y_val, h_unc.sample_elbo, data_val)\n",
    "        \n",
    "        f_train = validate_one_epoch_final_cost(X_train, y_train, h_unc, data_train)\n",
    "        f_val = validate_one_epoch_final_cost(X_val, y_val, h_unc, data_val)\n",
    "\n",
    "        #rmse_costs_sep.append(val_rmse_sep.data.item())\n",
    "\n",
    "        print(\n",
    "              'UNCERTAIN: Loss: ', \n",
    "               'Train:', round(loss_train.data.item(), 2),\n",
    "               '\\tVal: ', round(loss_val.data.item(), 2),\n",
    "               'Cost: ', \n",
    "               'Train:', round(f_train.data.item(), 2),\n",
    "               '\\tVal: ', round(f_val.data.item(), 2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 396])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_unc.forward_uncertain(X_test, n_samples=4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([396, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_unc(X_test).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
