{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import feat_eng as fe\n",
    "import data_selector_items as dsi\n",
    "import params_newsvendor as prm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mip import Model, xsum, minimize, INTEGER, CONTINUOUS, CutType, OptimizationStatus\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 0\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of data files\n",
    "path_data = './data/'\n",
    "\n",
    "# Read historical data\n",
    "sales = pd.read_csv(path_data + 'sales_train_evaluation.csv')\n",
    "\n",
    "# Spliting the data in days\n",
    "start_tr_day = 500\n",
    "start_val_day = 1742\n",
    "start_test_day = 1842\n",
    "end_day = 1941\n",
    "\n",
    "# N of items to use (Lower will be much faster)\n",
    "# In the paper, this is d_z\n",
    "n_items = 4\n",
    "\n",
    "n_samples = 3\n",
    "\n",
    "# All useful items\n",
    "sku_ids = dsi.select_items(sales, start_tr_day)\n",
    "\n",
    "# Sample only n_items to use\n",
    "random.seed(seed_number)\n",
    "sku_ids = random.sample(sku_ids, n_items)\n",
    "sku_ids = list(set(sku_ids))\n",
    "n_items = len(sku_ids)\n",
    "\n",
    "\n",
    "# Build training and test from historical data\n",
    "data_train, data_val, data_test, feat, n_items = fe.build_data(\n",
    "    path_data, sales, sku_ids, \n",
    "    start_tr_day, start_val_day, start_test_day, end_day)\n",
    "\n",
    "data_train.fillna(0, inplace=True)\n",
    "data_val.fillna(0, inplace=True)\n",
    "data_test.fillna(0, inplace=True)\n",
    "\n",
    "dx = len(feat)\n",
    "\n",
    "# Number of batch_size samples in the SGDs\n",
    "batch_size = 32 # Number of days for combined approaches\n",
    "\n",
    "# Here we change a bit the test data in order to increase the integrality gap \n",
    "# of the optimization problem (didactic purpose). Otherwise the Continuous and \n",
    "# Discrete version would have approximately the same results.\n",
    "data_train['qty'] = data_train['qty']*np.random.normal(1, 0.07)\n",
    "data_val['qty'] = data_val['qty']*np.random.normal(1, 0.07)\n",
    "data_test['qty'] = data_test['qty']*np.random.normal(1, 0.07)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_train[feat])\n",
    "\n",
    "data_train.loc[:, feat] = scaler.transform(data_train[feat])\n",
    "data_val.loc[:, feat] = scaler.transform(data_val[feat])\n",
    "data_test.loc[:, feat] = scaler.transform(data_test[feat])\n",
    "        \n",
    "X_train = torch.tensor(np.array(data_train[feat]).astype('float32'), requires_grad= True, device=dev)\n",
    "y_train = torch.tensor(np.array(data_train['qty']).astype('float32'), requires_grad= True, device=dev)\n",
    "\n",
    "X_val = torch.tensor(np.array(data_val[feat]).astype('float32'), requires_grad= True, device=dev)\n",
    "y_val = torch.tensor(np.array(data_val['qty']).astype('float32'), requires_grad= True, device=dev)\n",
    "\n",
    "X_test = torch.tensor(np.array(data_test[feat]).astype('float32'), requires_grad= True, device=dev)\n",
    "y_test = torch.tensor(np.array(data_test['qty']).astype('float32'), requires_grad= True, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data):\n",
    "    batches_idx = []\n",
    "    n_batches = int(np.floor(data['d'].nunique() / batch_size))\n",
    "    for i in range(0, n_batches):\n",
    "        days = data['d'].unique()\n",
    "        idx = data[data['d'].isin(\n",
    "            np.random.choice(days, batch_size, replace=False))].index.tolist()\n",
    "        if len(idx) == n_items*batch_size:\n",
    "            batches_idx.append(idx)\n",
    "    return batches_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = len(generate_batches(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t, params_np = prm.get_params(n_items, is_discrete=False, \n",
    "                                     q_factor = 0.01, # Quadratic penalty factor\n",
    "                                     seed_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of deterministic parameters: tensor([147., 109.])\n"
     ]
    }
   ],
   "source": [
    "print('Example of deterministic parameters:', params_t['cs'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_item = lambda Z, Y : params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "def reshape_outcomes(y_pred, y):\n",
    "    batch_size_temp = y_pred.shape[1]//n_items\n",
    "    y_pred = torch.reshape(y_pred, (n_samples, batch_size_temp, n_items))\n",
    "    y_pred = y_pred.permute((1, 0, 2)).reshape((batch_size_temp, n_samples*n_items))\n",
    "    y = torch.reshape(y, (y.shape[0]//n_items, n_items))\n",
    "    return y_pred, y\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred, y = reshape_outcomes(y_pred, y)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f\n",
    "\n",
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total\n",
    "\n",
    "# Analytical solution to find the argmin\n",
    "# This function allows autograd (backpropagation)\n",
    "def argmin_solver(y_pred):\n",
    "    z_star = y_pred\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(X_train, y_train, loss_function, optimizer, model):\n",
    "    \n",
    "    batches_idx = generate_batches(data_train)\n",
    "    \n",
    "    for b in batches_idx:\n",
    "        x_tr = X_train[b]\n",
    "        y_tr = y_train[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #preds = model(x_tr)\n",
    "\n",
    "        train_loss = loss_function(x_tr, y_tr.reshape(-1))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(X, y, val_function, data):\n",
    "    \n",
    "    batches_idx = generate_batches(data)\n",
    "    f_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in batches_idx:\n",
    "            x_ = X[b]\n",
    "            y_ = y[b]\n",
    "    \n",
    "            f_ = val_function(x_, y_.reshape(-1))  \n",
    "            f_sum = f_sum + f_\n",
    "\n",
    "    return f_sum/len(batches_idx)\n",
    "\n",
    "\n",
    "def validate_one_epoch_final_cost(X, y, model, data):\n",
    "    \n",
    "    batches_idx = generate_batches(data)\n",
    "    f_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in batches_idx:\n",
    "            x_ = X[b]\n",
    "            y_ = y[b]\n",
    "    \n",
    "            f_ = cost_fn(model(x_, n_samples), y_.reshape(-1))  \n",
    "            f_sum = f_sum + f_\n",
    "\n",
    "    return f_sum/len(batches_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0,1)\n",
    "    \n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    \n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.rho.size()).to(dev)\n",
    "        return self.mu + self.sigma * epsilon\n",
    "    \n",
    "    def log_prob(self, input):\n",
    "        return (-math.log(math.sqrt(2 * math.pi))\n",
    "                - torch.log(self.sigma)\n",
    "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()\n",
    "\n",
    "class ScaleMixtureGaussian(object):\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
    "    \n",
    "    def log_prob(self, input):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
    "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
    "        return (torch.log(self.pi * prob1 + (1-self.pi) * prob2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = 0.5\n",
    "SIGMA_1 = torch.FloatTensor([math.exp(-0)], device=dev)\n",
    "SIGMA_2 = torch.FloatTensor([math.exp(-6)], device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t, n_samples):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "            \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items  \n",
    "        self.n_samples = n_samples\n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident_samples = torch.eye(n_items*n_samples)\n",
    "        ident3 = torch.eye(n_items + 2*n_items*n_samples)\n",
    "        zeros_matrix = torch.zeros((n_items*n_samples, n_items*n_samples))\n",
    "        zeros_array = torch.zeros(n_items*n_samples)\n",
    "        ones_array = torch.ones(n_items*n_samples)\n",
    "             \n",
    "        self.Q = torch.diag(\n",
    "            torch.hstack(\n",
    "                (\n",
    "                    params_t['q'], \n",
    "                    (1/n_samples)*params_t['qs'].repeat(n_samples), \n",
    "                    (1/n_samples)*params_t['qw'].repeat(n_samples)\n",
    "                )\n",
    "            )).to(dev)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.hstack(\n",
    "                                (\n",
    "                                    params_t['c'], \n",
    "                                    (1/n_samples)*params_t['cs'].repeat(n_samples), \n",
    "                                    (1/n_samples)*params_t['cw'].repeat(n_samples)\n",
    "                                )).to(dev)\n",
    "             \n",
    "            \n",
    "        shortage_ineq = torch.hstack(\n",
    "            (\n",
    "                -ident.repeat_interleave(n_samples, 0), \n",
    "                -ident_samples, \n",
    "                zeros_matrix\n",
    "            )\n",
    "        )  \n",
    "        \n",
    "        \n",
    "        excess_ineq = torch.hstack(\n",
    "            (\n",
    "                ident.repeat_interleave(n_samples, 0), \n",
    "                zeros_matrix, \n",
    "                -ident_samples\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        price_ineq = torch.hstack(\n",
    "            (\n",
    "                params_t['pr'], \n",
    "                zeros_array, \n",
    "                zeros_array\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        \n",
    "        self.ineqs = torch.vstack(\n",
    "            (\n",
    "                shortage_ineq, \n",
    "                excess_ineq, \n",
    "                price_ineq, \n",
    "                positive_ineq\n",
    "            )\n",
    "        ).to(dev)\n",
    "\n",
    "        \n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        \n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items*n_samples), \n",
    "                                          torch.zeros(n_items*n_samples))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        n_batches, n_samples_items = y.size()\n",
    "        \n",
    "        assert self.n_samples*self.n_items == n_samples_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(batch_size, Q.size(0), Q.size(1))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(batch_size, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            batch_size, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), self.lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Weight parameters\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5,-4))\n",
    "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
    "        # Bias parameters\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5,-4))\n",
    "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
    "        # Prior distributions\n",
    "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
    "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
    "        if self.training or sample:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "        else:\n",
    "            weight = self.weight.mu\n",
    "            bias = self.bias.mu\n",
    "        if self.training or calculate_log_probs:\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            self.log_prior, self.log_variational_posterior = 0, 0\n",
    "\n",
    "        return F.linear(input, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super().__init__()\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.l1 = BayesianLinear(n_feat, 400)\n",
    "        self.l2 = BayesianLinear(400, 400)\n",
    "        self.l3 = BayesianLinear(400, 1)\n",
    "    \n",
    "    def forward(self, x, sample=False):\n",
    "        x = self.act1(self.l1(x, sample))\n",
    "        x = self.act1(self.l2(x, sample))\n",
    "        x = self.l3(x, sample)\n",
    "        return x\n",
    "    \n",
    "    def forward_uncertain(self, X, n_samples):\n",
    "        y = torch.zeros(n_samples, X.shape[0]).to(dev)\n",
    "        for i in range(n_samples):\n",
    "            y[i] = self(X, sample=True).reshape(-1)\n",
    "        return y\n",
    "    \n",
    "    def log_prior(self):\n",
    "        return self.l1.log_prior \\\n",
    "               + self.l2.log_prior \\\n",
    "               + self.l3.log_prior\n",
    "    \n",
    "    def log_variational_posterior(self):\n",
    "        return self.l1.log_variational_posterior \\\n",
    "               + self.l2.log_variational_posterior \\\n",
    "               + self.l3.log_variational_posterior\n",
    "    \n",
    "    def sample_elbo(self, input, target, n_samples=n_samples):\n",
    "        log_priors = torch.zeros(n_samples).to(dev)\n",
    "        log_variational_posteriors = torch.zeros(n_samples).to(dev)\n",
    "        outputs = self.forward_uncertain(input, n_samples)\n",
    "        for i in range(n_samples):\n",
    "            log_priors[i] = self.log_prior()\n",
    "            log_variational_posteriors[i] = self.log_variational_posterior()\n",
    "        log_prior = log_priors.mean()\n",
    "        log_variational_posterior = log_variational_posteriors.mean()\n",
    "        mse = F.mse_loss(outputs.mean(0), target, size_average=False)\n",
    "        loss = (log_variational_posterior - log_prior)/NUM_BATCHES + 1*mse\n",
    "        return loss#, log_prior, log_variational_posterior, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_unc = BayesianNetwork(n_feat=dx)\n",
    "opt_h_unc = torch.optim.Adam(h_unc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/qpth/qp.py:83: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2910.)\n",
      "  e, _ = torch.eig(Q[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNCERTAIN: Loss:  Train: 22780.64 \tVal:  22260.46 Cost:  Train: 13030.11 \tVal:  13698.27\n",
      "UNCERTAIN: Loss:  Train: 22086.5 \tVal:  21371.37 Cost:  Train: 12990.53 \tVal:  13604.65\n",
      "UNCERTAIN: Loss:  Train: 21618.29 \tVal:  21063.25 Cost:  Train: 13060.18 \tVal:  13700.91\n",
      "UNCERTAIN: Loss:  Train: 21149.22 \tVal:  20625.6 Cost:  Train: 13040.39 \tVal:  13779.15\n",
      "UNCERTAIN: Loss:  Train: 20834.14 \tVal:  20288.6 Cost:  Train: 12964.19 \tVal:  13736.31\n",
      "UNCERTAIN: Loss:  Train: 20542.89 \tVal:  20027.49 Cost:  Train: 13084.62 \tVal:  13640.76\n",
      "UNCERTAIN: Loss:  Train: 20168.08 \tVal:  19638.23 Cost:  Train: 13061.06 \tVal:  13798.46\n",
      "UNCERTAIN: Loss:  Train: 19996.07 \tVal:  19658.63 Cost:  Train: 12987.39 \tVal:  13786.66\n",
      "UNCERTAIN: Loss:  Train: 19591.43 \tVal:  19318.96 Cost:  Train: 13058.1 \tVal:  13636.62\n",
      "UNCERTAIN: Loss:  Train: 19501.89 \tVal:  19043.26 Cost:  Train: 13058.8 \tVal:  13657.5\n",
      "UNCERTAIN: Loss:  Train: 19211.87 \tVal:  18805.1 Cost:  Train: 13064.87 \tVal:  13729.19\n",
      "UNCERTAIN: Loss:  Train: 19037.68 \tVal:  18353.84 Cost:  Train: 13032.97 \tVal:  13671.17\n",
      "UNCERTAIN: Loss:  Train: 18839.54 \tVal:  18354.81 Cost:  Train: 13015.18 \tVal:  13605.46\n",
      "UNCERTAIN: Loss:  Train: 18637.84 \tVal:  18420.22 Cost:  Train: 13007.23 \tVal:  13759.21\n",
      "UNCERTAIN: Loss:  Train: 18416.82 \tVal:  17914.63 Cost:  Train: 13011.98 \tVal:  13716.03\n",
      "UNCERTAIN: Loss:  Train: 18225.45 \tVal:  17807.44 Cost:  Train: 13018.54 \tVal:  13751.1\n",
      "UNCERTAIN: Loss:  Train: 18131.97 \tVal:  17557.77 Cost:  Train: 13047.0 \tVal:  13765.9\n",
      "UNCERTAIN: Loss:  Train: 17867.88 \tVal:  17553.46 Cost:  Train: 13054.75 \tVal:  13753.11\n",
      "UNCERTAIN: Loss:  Train: 17655.68 \tVal:  17317.47 Cost:  Train: 12978.26 \tVal:  13650.4\n",
      "UNCERTAIN: Loss:  Train: 17569.29 \tVal:  17213.55 Cost:  Train: 12998.21 \tVal:  13736.26\n",
      "UNCERTAIN: Loss:  Train: 17498.61 \tVal:  16878.6 Cost:  Train: 13026.52 \tVal:  13586.23\n",
      "UNCERTAIN: Loss:  Train: 17321.2 \tVal:  16991.92 Cost:  Train: 12985.07 \tVal:  13695.78\n",
      "UNCERTAIN: Loss:  Train: 17130.64 \tVal:  16779.78 Cost:  Train: 13066.25 \tVal:  13648.96\n",
      "UNCERTAIN: Loss:  Train: 16971.08 \tVal:  16520.98 Cost:  Train: 13063.24 \tVal:  13704.34\n",
      "UNCERTAIN: Loss:  Train: 16801.23 \tVal:  16368.6 Cost:  Train: 12980.03 \tVal:  13697.18\n",
      "UNCERTAIN: Loss:  Train: 16680.93 \tVal:  16387.34 Cost:  Train: 13057.92 \tVal:  13751.42\n",
      "UNCERTAIN: Loss:  Train: 16480.79 \tVal:  16179.46 Cost:  Train: 12983.9 \tVal:  13754.95\n",
      "UNCERTAIN: Loss:  Train: 16326.79 \tVal:  16082.82 Cost:  Train: 13017.49 \tVal:  13650.79\n",
      "UNCERTAIN: Loss:  Train: 16208.2 \tVal:  16061.36 Cost:  Train: 13019.97 \tVal:  13603.53\n",
      "UNCERTAIN: Loss:  Train: 16119.87 \tVal:  15845.49 Cost:  Train: 13021.01 \tVal:  13746.67\n",
      "UNCERTAIN: Loss:  Train: 16070.62 \tVal:  15634.34 Cost:  Train: 13041.77 \tVal:  13747.03\n",
      "UNCERTAIN: Loss:  Train: 15887.48 \tVal:  15524.21 Cost:  Train: 13008.32 \tVal:  13650.66\n",
      "UNCERTAIN: Loss:  Train: 15749.46 \tVal:  15502.37 Cost:  Train: 13090.52 \tVal:  13586.46\n",
      "UNCERTAIN: Loss:  Train: 15674.97 \tVal:  15224.39 Cost:  Train: 12986.29 \tVal:  13784.47\n",
      "UNCERTAIN: Loss:  Train: 15567.19 \tVal:  14972.93 Cost:  Train: 12998.5 \tVal:  13672.13\n",
      "UNCERTAIN: Loss:  Train: 15457.34 \tVal:  15237.33 Cost:  Train: 13064.2 \tVal:  13706.52\n",
      "UNCERTAIN: Loss:  Train: 15276.43 \tVal:  14963.02 Cost:  Train: 13081.89 \tVal:  13804.26\n",
      "UNCERTAIN: Loss:  Train: 15196.17 \tVal:  15022.48 Cost:  Train: 13067.43 \tVal:  13803.77\n",
      "UNCERTAIN: Loss:  Train: 15191.17 \tVal:  14816.9 Cost:  Train: 13080.7 \tVal:  13595.58\n",
      "UNCERTAIN: Loss:  Train: 15025.55 \tVal:  14612.55 Cost:  Train: 13039.26 \tVal:  13647.93\n",
      "UNCERTAIN: Loss:  Train: 15023.25 \tVal:  14953.87 Cost:  Train: 13022.38 \tVal:  13613.2\n",
      "UNCERTAIN: Loss:  Train: 14802.21 \tVal:  14705.25 Cost:  Train: 13036.47 \tVal:  13788.72\n",
      "UNCERTAIN: Loss:  Train: 14754.65 \tVal:  14510.78 Cost:  Train: 13024.78 \tVal:  13683.78\n",
      "UNCERTAIN: Loss:  Train: 14733.69 \tVal:  14432.44 Cost:  Train: 13069.91 \tVal:  13764.36\n",
      "UNCERTAIN: Loss:  Train: 14478.07 \tVal:  14519.41 Cost:  Train: 12988.9 \tVal:  13816.41\n",
      "UNCERTAIN: Loss:  Train: 14564.77 \tVal:  14283.21 Cost:  Train: 13053.15 \tVal:  13730.49\n",
      "UNCERTAIN: Loss:  Train: 14381.26 \tVal:  14211.22 Cost:  Train: 12992.64 \tVal:  13766.64\n",
      "UNCERTAIN: Loss:  Train: 14324.04 \tVal:  14223.03 Cost:  Train: 13028.46 \tVal:  13729.73\n",
      "UNCERTAIN: Loss:  Train: 14300.43 \tVal:  13991.77 Cost:  Train: 13035.71 \tVal:  13779.5\n",
      "UNCERTAIN: Loss:  Train: 14293.12 \tVal:  13872.16 Cost:  Train: 13046.8 \tVal:  13641.15\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "rmse_costs_sep = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    train_one_epoch(X_train, y_train, h_unc.sample_elbo, opt_h_unc, h_unc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_train = validate_one_epoch(X_train, y_train, h_unc.sample_elbo, data_train)\n",
    "        loss_val = validate_one_epoch(X_val, y_val, h_unc.sample_elbo, data_val)\n",
    "        \n",
    "        f_train = validate_one_epoch_final_cost(X_train, y_train, h_unc.forward_uncertain, data_train)\n",
    "        f_val = validate_one_epoch_final_cost(X_val, y_val, h_unc.forward_uncertain, data_val)\n",
    "\n",
    "        #rmse_costs_sep.append(val_rmse_sep.data.item())\n",
    "\n",
    "        print(\n",
    "              'UNCERTAIN: Loss: ', \n",
    "               'Train:', round(loss_train.data.item(), 2),\n",
    "               '\\tVal: ', round(loss_val.data.item(), 2),\n",
    "               'Cost: ', \n",
    "               'Train:', round(f_train.data.item(), 2),\n",
    "               '\\tVal: ', round(f_val.data.item(), 2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4852])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 396])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_unc.forward_uncertain(X_test, n_samples=4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([396, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_unc(X_test).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
