{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(3, 2, N)\n",
    "x2 = np.random.normal(0, 5, N)\n",
    "\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# Noise\n",
    "n1 = np.random.normal(0, 2, N)\n",
    "n2 = np.random.normal(0, 6, N)\n",
    "n3 = np.random.normal(0, 10, N)\n",
    "\n",
    "y1 = x1**2 + 5*x1*x2 + 3*np.abs(x2) + n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X[:800], dtype = torch.float32, requires_grad = True, device = dev)\n",
    "y1_train = torch.tensor(y1[:800],  dtype = torch.float32, requires_grad = True, device = dev)\n",
    "\n",
    "X_test = torch.tensor(X[800:],  dtype = torch.float32, requires_grad = True, device = dev)\n",
    "y1_test = torch.tensor(y1[800:],  dtype = torch.float32, requires_grad = True, device = dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, output_size,\n",
    "                 prior_mu, prior_rho,\n",
    "                 n_samples\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bias weight\n",
    "        input_size = input_size + 1\n",
    "        \n",
    "        # Defining Prior distribution (Gaussian)\n",
    "        self.prior_mu = torch.tensor(prior_mu)\n",
    "        self.prior_rho = torch.tensor(prior_rho)\n",
    "        \n",
    "        # Defining Variational class (Gaussian class)\n",
    "        self.theta_mu = nn.Parameter(\n",
    "            torch.Tensor(n_samples, input_size, output_size).uniform_(-0.2, 0.2)).float()\n",
    "        self.theta_rho = nn.Parameter(\n",
    "            torch.Tensor(n_samples, input_size, output_size).uniform_(-2,-1)).float()\n",
    "        \n",
    "        # Defining some constants\n",
    "        self.logsqrttwopi = torch.log(\n",
    "            torch.sqrt(2*torch.tensor(torch.pi)))\n",
    "        self.K = torch.tensor(1)\n",
    "        \n",
    "        # Defining number of samples for forward\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def rho_to_sigma(self, theta_rho):\n",
    "        return torch.log(1 + torch.exp(theta_rho))\n",
    "\n",
    "    def sample_weight(self, theta_mu, theta_rho):\n",
    "        w = (theta_mu \n",
    "        + self.rho_to_sigma(theta_rho)*torch.randn(theta_mu.shape))\n",
    "        return w\n",
    "\n",
    "    def log_prob_gaussian(self, x, mu, rho):\n",
    "            return (\n",
    "                - self.logsqrttwopi\n",
    "                - self.rho_to_sigma(rho)\n",
    "                - ((x - mu)**2)/(2*self.rho_to_sigma(rho)**2)\n",
    "            ).sum()\n",
    "    \n",
    "    def prior(self, w):\n",
    "        return self.log_prob_gaussian(\n",
    "            w, self.prior_mu, self.prior_rho)\n",
    "        \n",
    "    def variational(self, w, theta_mu, theta_rho):\n",
    "        return self.log_prob_gaussian(\n",
    "            w, theta_mu, theta_rho) \n",
    "    \n",
    "    def kl_divergence_layer(self):\n",
    "        theta_mu = self.theta_mu\n",
    "        theta_rho = self.theta_rho\n",
    "        w = self.sample_weight(theta_mu, theta_rho)\n",
    "        Q = self.variational(w, theta_mu, theta_rho)\n",
    "        P = self.prior(w)\n",
    "        KL = Q - P \n",
    "        return KL\n",
    "    \n",
    "    def forward(self, x_layer):\n",
    "        theta_mu = self.theta_mu\n",
    "        theta_rho = self.theta_rho\n",
    "        w = self.sample_weight(theta_mu, theta_rho)    \n",
    "        x_next_layer = torch.bmm(x_layer, w[:, :-1, :]) + w[:,-1,:].unsqueeze(1)\n",
    "        return x_next_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalNet(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self, n_samples, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear1 = VariationalLayer(input_size, 5, 0, 0.54, n_samples)\n",
    "        #self.bn = nn.BatchNorm1d(5)\n",
    "        self.linear2 = VariationalLayer(5, output_size, 0, 0.54, n_samples)\n",
    "    \n",
    "    # Perform the computation\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "        x = x.expand((self.n_samples, x.shape[1], x.shape[2]))\n",
    "        x = self.linear1(x)\n",
    "        #x = self.bn(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        #x = self.act1(x)\n",
    "        return x\n",
    "    \n",
    "    def kl_divergence_NN(self):\n",
    "        kl = self.linear1.kl_divergence_layer() + self.linear2.kl_divergence_layer()\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "h = VariationalNet(n_samples, input_size, output_size).to(dev)\n",
    "opt_h = torch.optim.Adam(h.parameters(), lr=0.01)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#X_train = torch.unsqueeze(X_train, 2)\n",
    "index = np.random.randint(0, 800, 5)\n",
    "x_h = X_train[index]\n",
    "y_h = y1_train[index]\n",
    "h.forward(x_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(324.0914, grad_fn=<MseLossBackward0>) tensor(340.0129, grad_fn=<AddBackward0>)\n",
      "tensor(346.3026, grad_fn=<MseLossBackward0>) tensor(343.7459, grad_fn=<AddBackward0>)\n",
      "tensor(1903.8341, grad_fn=<MseLossBackward0>) tensor(340.8668, grad_fn=<AddBackward0>)\n",
      "tensor(639.5687, grad_fn=<MseLossBackward0>) tensor(335.0477, grad_fn=<AddBackward0>)\n",
      "tensor(445.5335, grad_fn=<MseLossBackward0>) tensor(345.3464, grad_fn=<AddBackward0>)\n",
      "tensor(803.3212, grad_fn=<MseLossBackward0>) tensor(338.1334, grad_fn=<AddBackward0>)\n",
      "tensor(580.7350, grad_fn=<MseLossBackward0>) tensor(329.2625, grad_fn=<AddBackward0>)\n",
      "tensor(561.3204, grad_fn=<MseLossBackward0>) tensor(334.4700, grad_fn=<AddBackward0>)\n",
      "tensor(635.4660, grad_fn=<MseLossBackward0>) tensor(346.8938, grad_fn=<AddBackward0>)\n",
      "tensor(1052.6262, grad_fn=<MseLossBackward0>) tensor(350.6403, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    index = np.random.randint(0, 800, 50)\n",
    "    x_h = X_train[index]\n",
    "    y_h = y1_train[index]\n",
    "    \n",
    "    opt_h.zero_grad()\n",
    "    y_preds = h(x_h)\n",
    "    y_preds_mean = y_preds.mean(axis=0).squeeze()\n",
    "    elbo_loss = mse_loss(y_preds_mean, y_h) + h.kl_divergence_NN()\n",
    "    elbo_loss.backward()\n",
    "    opt_h.step()\n",
    "    \n",
    "    print(mse_loss(y_preds_mean, y_h), h.kl_divergence_NN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(790.9849, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(y_preds_mean, y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(202.5808, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.kl_divergence_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 102.2150,  145.6053,   20.7263,   -1.6152,  -31.5738,   10.1983,\n",
       "          24.0533,   54.6786,  -23.9765,  -69.9426,  -81.3019,  -63.6686,\n",
       "          92.4034,  131.3438,   99.1495, -111.5350,  -64.6963,  -50.1505,\n",
       "         197.3893,   64.2947,   50.1944, -111.5350, -140.7539,   68.0634,\n",
       "         -47.6772,  -27.9659,  190.0887,  -88.2236, -124.4992,    3.7649,\n",
       "         -46.4705,   40.3146,  160.9212,  -50.7410,  -12.7711,  -72.0561,\n",
       "         -42.7727,  -75.9347,  108.1896,  160.0007,   27.8732,  228.6359,\n",
       "         151.4664,  126.0074,   94.7874,  -23.6355,   72.0101,   -5.6030,\n",
       "         115.2242,   -5.5860], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  67.7424,  133.4991,   14.8095,    6.8667,  -10.0592,    1.8639,\n",
       "          17.4458,   34.9345,  -17.4515,  -60.1327,  -74.0747,  -49.9519,\n",
       "          70.7016,  130.5905,   71.5508, -148.9616,  -49.9473,  -18.4058,\n",
       "         237.8830,   51.3122,   14.6421, -148.9616, -246.1836,   55.9984,\n",
       "         -13.2758,  -34.7388,  145.3001, -127.8421, -188.8626,    5.0664,\n",
       "         -54.7836,   23.9301,  148.0581,  -41.7343,    2.5848,  -38.4266,\n",
       "         -18.1399,  -72.8139,  109.4105,  179.5030,   16.9142,  288.7208,\n",
       "         118.8489,  142.0373,   93.5721,  -16.7625,   66.8598,    3.4595,\n",
       "         110.7035,    0.8648], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.theta_mu tensor([[[ 0.8771,  0.8008,  0.5494,  0.6440,  0.4596],\n",
      "         [-0.4487,  0.7498,  0.5977,  0.2930, -0.1289],\n",
      "         [ 0.8443,  0.6756,  0.3603,  0.5161,  0.5984]],\n",
      "\n",
      "        [[ 0.7617,  0.4910,  0.6485,  0.7612,  0.8183],\n",
      "         [ 0.2891, -0.4360, -0.4808,  0.7688, -0.4957],\n",
      "         [ 0.8583,  0.6387,  0.6754,  0.6904,  0.7762]],\n",
      "\n",
      "        [[ 0.6100,  0.7565,  0.5851,  0.4265,  0.3386],\n",
      "         [ 0.1600,  0.5227, -0.3907,  0.5654,  0.0436],\n",
      "         [ 0.5872,  0.8885,  0.4619,  0.6182,  0.5223]]])\n",
      "linear1.theta_rho tensor([[[-1.5920, -1.2908, -1.5692, -0.9557, -0.8429],\n",
      "         [-0.8280, -1.4659, -1.1269, -1.2181, -1.1346],\n",
      "         [-1.5434, -1.2956, -1.3341, -1.1154, -1.1518]],\n",
      "\n",
      "        [[-1.3310, -1.8959, -1.3955, -1.7491, -1.6686],\n",
      "         [-0.7042, -1.4796, -0.7514, -1.0054, -1.0801],\n",
      "         [-1.7647, -1.1682, -1.3814, -1.2318, -1.2843]],\n",
      "\n",
      "        [[-1.1970, -1.0642, -1.5848, -1.3141, -1.0779],\n",
      "         [-1.5442, -1.5914, -1.0839, -1.3320, -0.5853],\n",
      "         [-1.7603, -1.1472, -1.5695, -1.2556, -1.0960]]])\n",
      "linear2.theta_mu tensor([[[0.9998],\n",
      "         [0.8157],\n",
      "         [0.6800],\n",
      "         [0.5571],\n",
      "         [0.5820],\n",
      "         [0.8347]],\n",
      "\n",
      "        [[0.7326],\n",
      "         [0.6313],\n",
      "         [0.7770],\n",
      "         [0.9420],\n",
      "         [0.8497],\n",
      "         [0.7466]],\n",
      "\n",
      "        [[0.5946],\n",
      "         [0.8837],\n",
      "         [0.6734],\n",
      "         [0.7248],\n",
      "         [0.5943],\n",
      "         [0.7624]]])\n",
      "linear2.theta_rho tensor([[[-1.9010],\n",
      "         [-1.5481],\n",
      "         [-1.1166],\n",
      "         [-1.6079],\n",
      "         [-1.6291],\n",
      "         [-1.2598]],\n",
      "\n",
      "        [[-1.9633],\n",
      "         [-1.8822],\n",
      "         [-1.8255],\n",
      "         [-1.8919],\n",
      "         [-1.1401],\n",
      "         [-1.3872]],\n",
      "\n",
      "        [[-1.7603],\n",
      "         [-1.8200],\n",
      "         [-1.1493],\n",
      "         [-1.6120],\n",
      "         [-1.1628],\n",
      "         [-0.9803]]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in h.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(989.9192, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = 3\n",
    "vi_obj = VariationalLayer(input_size, output_size, 0, 0.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.unsqueeze(X_train, 2)\n",
    "index = np.random.randint(0, 800, 4)\n",
    "x_h = X_train[index]\n",
    "y_h = y1_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_obj.forward(x_h, 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elbo_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43melbo_loss\u001b[49m(xh, yh, h, w)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elbo_loss' is not defined"
     ]
    }
   ],
   "source": [
    "elbo_loss(xh, yh, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.unsqueeze(X_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "variational() missing 2 required positional arguments: 'theta_mu' and 'theta_rho'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [162]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m w \u001b[38;5;241m=\u001b[39m vi_obj\u001b[38;5;241m.\u001b[39msample_weight(weight_mu, weight_rho)\n\u001b[1;32m      2\u001b[0m Q \u001b[38;5;241m=\u001b[39m vi_obj\u001b[38;5;241m.\u001b[39mvariational(w, weight_mu, weight_rho)\n\u001b[0;32m----> 3\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[43mvi_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      6\u001b[0m x_h \u001b[38;5;241m=\u001b[39m X_train[index]\n",
      "\u001b[0;31mTypeError\u001b[0m: variational() missing 2 required positional arguments: 'theta_mu' and 'theta_rho'"
     ]
    }
   ],
   "source": [
    "w = vi_obj.sample_weight(weight_mu, weight_rho)\n",
    "Q = vi_obj.variational(w, weight_mu, weight_rho)\n",
    "P = vi_obj.variational(w)\n",
    "\n",
    "index = np.random.randint(0, 800, 5)\n",
    "x_h = X_train[index]\n",
    "y_h = y1_train[index]\n",
    "l = mse_loss(h(x_h), y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x1 and 2x6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [193]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m x_h \u001b[38;5;241m=\u001b[39m X_train[index]\n\u001b[1;32m      3\u001b[0m y_h \u001b[38;5;241m=\u001b[39m y1_train[index]\n\u001b[0;32m----> 4\u001b[0m l \u001b[38;5;241m=\u001b[39m mse_loss(\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m)\u001b[49m, y_h)\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [179]\u001b[0m, in \u001b[0;36mANN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(x)\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x1 and 2x6)"
     ]
    }
   ],
   "source": [
    "index = np.random.randint(0, 800, 5)\n",
    "x_h = X_train[index]\n",
    "y_h = y1_train[index]\n",
    "l = mse_loss(h(x_h), y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 2])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [130]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [128]\u001b[0m, in \u001b[0;36mANN.forward_parallel\u001b[0;34m(self, x_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_parallel\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_samples):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 22\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:537\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:143\u001b[0m, in \u001b[0;36mreduce_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    140\u001b[0m storage \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCowardly refusing to serialize non-leaf tensor which requires_grad, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince autograd does not support crossing process boundaries.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you just want to transfer the data, call detach() on the tensor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore serializing (e.g., putting it on the queue).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m check_serializing_named_tensor(tensor)\n\u001b[1;32m    149\u001b[0m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mhooks\u001b[38;5;241m.\u001b[39mwarn_if_has_hooks(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue)."
     ]
    }
   ],
   "source": [
    "h.forward_parallel(x_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.41972735,  1.2029036 ],\n",
       "       [ 3.64036384, -1.80001503],\n",
       "       [ 1.66258872, -9.45797395],\n",
       "       ...,\n",
       "       [ 3.35859904,  2.77637386],\n",
       "       [ 2.11101038,  5.22272453],\n",
       "       [ 2.69671436,  2.78818971]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = vi_obj.prior(vi_obj.sample_weight(weight_mu, weight_rho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0, 800, 200)\n",
    "x_h = X_train[index]\n",
    "y_h = y1_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2213e+00, -6.2528e+00],\n",
       "        [ 2.7539e+00,  2.2754e+00],\n",
       "        [ 4.8483e+00, -3.6463e+00],\n",
       "        [ 4.6008e+00, -2.2948e+00],\n",
       "        [ 6.7030e+00, -2.3596e+00],\n",
       "        [ 1.8571e+00,  6.8027e-01],\n",
       "        [ 2.9115e+00, -5.4428e+00],\n",
       "        [ 2.3574e+00, -4.6242e+00],\n",
       "        [ 4.1867e+00, -5.0198e+00],\n",
       "        [ 5.5665e+00,  1.7493e+00],\n",
       "        [ 4.1867e+00, -5.0198e+00],\n",
       "        [ 1.6467e+00, -1.4431e+00],\n",
       "        [ 1.7152e+00, -2.6472e+00],\n",
       "        [ 4.8483e+00, -3.6463e+00],\n",
       "        [ 3.3220e+00,  3.2644e+00],\n",
       "        [ 1.7996e+00, -6.0863e+00],\n",
       "        [ 9.8054e-01,  3.7539e+00],\n",
       "        [ 5.2619e+00, -2.2712e+00],\n",
       "        [ 5.8081e+00,  4.0490e+00],\n",
       "        [ 5.0169e+00,  3.7023e+00],\n",
       "        [ 7.5814e-01,  3.0518e-01],\n",
       "        [ 4.2604e+00,  5.6789e+00],\n",
       "        [ 7.5471e+00, -1.5020e+00],\n",
       "        [ 6.1520e+00,  4.9272e+00],\n",
       "        [ 4.3850e+00,  2.8569e+00],\n",
       "        [ 4.5348e+00,  3.5263e+00],\n",
       "        [ 3.4467e+00, -4.1564e+00],\n",
       "        [-1.3712e+00,  3.6505e+00],\n",
       "        [ 3.8401e+00,  3.3312e+00],\n",
       "        [ 5.3668e+00, -4.8615e+00],\n",
       "        [ 2.0427e+00,  9.1059e-01],\n",
       "        [ 1.1735e+00,  7.7315e+00],\n",
       "        [-2.2564e+00,  1.1066e+00],\n",
       "        [ 3.7575e+00,  3.5623e+00],\n",
       "        [ 3.1006e+00, -2.8287e+00],\n",
       "        [ 2.5677e+00,  2.1725e+00],\n",
       "        [ 3.6404e+00, -1.8000e+00],\n",
       "        [ 4.0081e+00, -5.1534e+00],\n",
       "        [ 6.4448e+00,  2.7751e+00],\n",
       "        [ 2.0561e+00,  4.1414e-02],\n",
       "        [ 2.1654e+00,  1.3357e+00],\n",
       "        [ 6.1574e+00,  1.1348e+01],\n",
       "        [ 5.1213e+00, -1.0509e+01],\n",
       "        [ 7.2335e-01, -6.1377e+00],\n",
       "        [ 4.6861e+00,  5.6095e+00],\n",
       "        [ 3.4809e+00,  7.0722e-03],\n",
       "        [ 2.3574e+00, -4.6242e+00],\n",
       "        [ 9.9098e-01,  2.2986e+00],\n",
       "        [-1.0013e+00,  4.1154e+00],\n",
       "        [ 5.0149e+00, -5.0131e+00],\n",
       "        [ 5.7128e+00,  2.7622e+00],\n",
       "        [ 4.2478e+00, -1.1957e+00],\n",
       "        [ 4.1419e+00, -8.5059e+00],\n",
       "        [-3.1216e+00,  3.0758e+00],\n",
       "        [ 4.5348e+00,  3.5263e+00],\n",
       "        [ 7.6920e-01,  4.8152e+00],\n",
       "        [-1.5647e+00, -2.9344e+00],\n",
       "        [ 3.5575e+00,  6.9786e-01],\n",
       "        [ 1.1837e+00, -3.3843e+00],\n",
       "        [ 4.4847e+00,  2.9122e+00],\n",
       "        [ 3.7996e+00,  4.0384e-02],\n",
       "        [ 2.7150e-01,  1.5182e+00],\n",
       "        [ 1.6258e+00,  9.2874e+00],\n",
       "        [ 3.2481e+00,  7.2629e-01],\n",
       "        [ 2.3549e+00,  2.2191e+00],\n",
       "        [ 3.6404e+00, -1.8000e+00],\n",
       "        [ 2.0960e+00,  8.9925e+00],\n",
       "        [ 4.4070e+00, -5.7698e+00],\n",
       "        [ 4.0478e+00,  4.1103e-01],\n",
       "        [ 5.2619e+00, -2.2712e+00],\n",
       "        [-4.2075e-01,  2.6132e+00],\n",
       "        [ 3.7796e+00, -1.2589e+00],\n",
       "        [ 5.4074e+00,  9.6498e+00],\n",
       "        [ 3.0450e+00,  9.7262e+00],\n",
       "        [ 4.1867e+00, -6.3441e+00],\n",
       "        [ 2.3549e+00,  2.2191e+00],\n",
       "        [-2.9575e-01, -5.0211e+00],\n",
       "        [ 2.4067e+00,  5.8433e+00],\n",
       "        [ 2.6122e+00,  3.1011e+00],\n",
       "        [ 4.0457e+00, -3.0625e+00],\n",
       "        [ 1.4471e+00, -4.8653e+00],\n",
       "        [ 6.1503e+00, -5.2530e+00],\n",
       "        [ 5.0540e+00,  5.2491e+00],\n",
       "        [ 4.4573e+00, -5.1979e+00],\n",
       "        [ 4.1073e+00,  2.6293e+00],\n",
       "        [ 4.6861e+00,  5.6095e+00],\n",
       "        [ 1.2264e+00, -2.2595e+00],\n",
       "        [ 1.0373e+00, -4.0995e+00],\n",
       "        [ 6.0379e+00,  3.4194e-01],\n",
       "        [ 3.0735e+00, -5.8397e+00],\n",
       "        [ 4.6008e+00, -2.2948e+00],\n",
       "        [ 4.3111e+00,  7.0796e+00],\n",
       "        [ 3.1874e+00,  8.8398e+00],\n",
       "        [ 5.0781e+00,  1.1830e+01],\n",
       "        [ 2.3325e+00, -2.6037e+00],\n",
       "        [ 2.7069e+00, -4.2269e+00],\n",
       "        [ 6.4448e+00,  2.7751e+00],\n",
       "        [ 1.0955e+00, -5.3721e+00],\n",
       "        [ 3.1681e+00, -9.9816e-01],\n",
       "        [ 2.4690e+00,  2.5959e+00],\n",
       "        [-8.4587e-01,  7.3888e-02],\n",
       "        [ 3.2975e+00, -4.8681e+00],\n",
       "        [ 1.2946e+00,  7.7900e+00],\n",
       "        [ 4.4753e+00,  1.2577e+00],\n",
       "        [ 1.7996e+00, -6.0863e+00],\n",
       "        [ 2.0904e+00,  2.4417e-01],\n",
       "        [ 2.5805e+00, -5.6564e+00],\n",
       "        [ 4.6441e+00, -7.0014e-01],\n",
       "        [ 1.8060e+00, -5.1491e+00],\n",
       "        [ 3.2947e+00,  2.7953e+00],\n",
       "        [ 4.5230e+00,  3.3684e+00],\n",
       "        [ 1.0877e+00,  9.4003e+00],\n",
       "        [ 1.6258e+00,  9.2874e+00],\n",
       "        [ 1.1799e+00,  8.1679e+00],\n",
       "        [ 2.9100e+00, -5.3686e+00],\n",
       "        [ 4.3130e+00,  1.8640e+00],\n",
       "        [ 4.2007e-01,  8.8226e+00],\n",
       "        [ 5.5185e+00,  1.5456e+00],\n",
       "        [-1.3684e+00, -3.0266e+00],\n",
       "        [ 1.1799e+00,  8.1679e+00],\n",
       "        [ 2.0727e+00,  6.9472e+00],\n",
       "        [ 3.0737e+00,  3.3305e-01],\n",
       "        [ 5.4783e+00,  2.3917e+00],\n",
       "        [ 1.0122e+00,  9.3832e-01],\n",
       "        [ 4.4258e+00, -1.8501e+00],\n",
       "        [ 2.4579e+00,  4.1520e-01],\n",
       "        [ 4.6880e+00,  2.1946e+00],\n",
       "        [ 1.1735e+00,  7.7315e+00],\n",
       "        [ 2.6284e+00,  6.4786e+00],\n",
       "        [ 3.9744e+00,  6.2589e+00],\n",
       "        [ 4.9426e+00, -3.6281e+00],\n",
       "        [ 1.3175e+00, -4.5762e-01],\n",
       "        [ 4.5269e+00, -2.4364e+00],\n",
       "        [ 4.4573e+00, -5.1979e+00],\n",
       "        [ 2.1711e-01,  6.5833e+00],\n",
       "        [ 1.3187e+00, -1.2044e-01],\n",
       "        [-1.5647e+00, -2.9344e+00],\n",
       "        [ 4.4989e+00, -3.2355e+00],\n",
       "        [ 1.0122e+00,  9.3832e-01],\n",
       "        [ 3.1567e+00, -1.4761e-01],\n",
       "        [ 1.1714e+00,  1.0239e+01],\n",
       "        [ 2.7156e+00,  6.9025e+00],\n",
       "        [ 4.6857e+00, -6.5394e+00],\n",
       "        [ 2.1348e+00,  8.4103e-02],\n",
       "        [ 1.7051e+00,  6.8187e+00],\n",
       "        [ 3.4467e+00, -4.1564e+00],\n",
       "        [ 5.3558e+00, -7.2359e-01],\n",
       "        [ 5.1163e+00,  2.9896e+00],\n",
       "        [ 3.0702e+00, -2.0315e+00],\n",
       "        [ 1.4197e+00,  1.2029e+00],\n",
       "        [ 3.9201e+00,  8.9298e+00],\n",
       "        [ 1.7936e+00,  1.7860e+00],\n",
       "        [ 3.3532e+00,  5.8603e+00],\n",
       "        [ 4.2265e+00, -4.7608e+00],\n",
       "        [ 7.2811e-01, -3.6428e-01],\n",
       "        [ 1.5127e+00, -3.5730e+00],\n",
       "        [ 5.8625e+00,  4.5118e+00],\n",
       "        [ 4.9048e-01,  1.3626e+01],\n",
       "        [ 3.2787e+00,  1.7774e+00],\n",
       "        [ 2.3934e+00, -3.2097e+00],\n",
       "        [ 5.0204e+00,  2.5834e+00],\n",
       "        [-5.2047e-01,  9.0772e+00],\n",
       "        [ 2.1881e+00,  8.0024e+00],\n",
       "        [ 2.3147e+00,  8.4183e-02],\n",
       "        [ 4.0478e+00,  4.1103e-01],\n",
       "        [ 2.5738e+00, -2.9967e+00],\n",
       "        [ 3.4649e+00, -1.1076e+01],\n",
       "        [ 4.0303e+00,  2.0800e+00],\n",
       "        [ 3.4467e+00, -4.1564e+00],\n",
       "        [ 6.4624e+00,  1.0276e-01],\n",
       "        [ 3.8186e+00, -1.0165e+00],\n",
       "        [ 3.7575e+00,  3.5623e+00],\n",
       "        [ 3.7712e+00, -3.3082e+00],\n",
       "        [ 2.4579e+00,  4.1520e-01],\n",
       "        [-1.0394e+00, -1.3418e+00],\n",
       "        [ 3.1529e+00, -2.9057e+00],\n",
       "        [ 1.7739e+00,  1.3703e+00],\n",
       "        [ 3.2008e+00, -7.6072e+00],\n",
       "        [ 4.5230e+00,  3.3684e+00],\n",
       "        [ 1.5127e+00, -3.5730e+00],\n",
       "        [ 5.1177e+00, -3.8126e+00],\n",
       "        [ 2.9648e-01, -1.1864e+00],\n",
       "        [ 2.0427e+00,  9.1059e-01],\n",
       "        [ 5.2619e-01,  2.5396e+00],\n",
       "        [ 2.8876e+00, -4.4858e+00],\n",
       "        [ 4.4635e+00, -5.6835e+00],\n",
       "        [ 2.5821e+00, -1.5985e+00],\n",
       "        [ 1.5695e+00, -5.5184e+00],\n",
       "        [ 8.7105e-01,  2.5179e+00],\n",
       "        [ 3.2043e+00, -9.6052e-01],\n",
       "        [ 5.8081e+00,  4.0490e+00],\n",
       "        [ 2.5944e+00,  7.8854e+00],\n",
       "        [ 3.8856e+00,  4.2251e+00],\n",
       "        [ 4.4635e+00, -5.6835e+00],\n",
       "        [ 6.1503e+00, -5.2530e+00],\n",
       "        [ 1.3400e+00, -2.6819e+00],\n",
       "        [ 5.9189e+00,  4.6587e+00],\n",
       "        [ 6.3695e+00, -4.6322e+00],\n",
       "        [ 4.4385e-01,  8.5420e+00],\n",
       "        [ 1.2264e+00, -2.2595e+00]], dtype=torch.float64,\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8096.7533, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(h(x_h), y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sample_weight(weight_mu, weight_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0181, -0.4754]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.9496, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.1615, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variational_posterior(w, weight_mu, weight_rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
