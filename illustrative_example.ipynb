{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "import pdb\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Utils\n",
    "import data_generator\n",
    "from variational_layer import VariationalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "N_train = 3000\n",
    "N_valid = N - N_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_generator.data_4to4(N)\n",
    "X, y_perfect = data_generator.data_4to4(N, noise_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot if output data is 1D\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_plot = pca.transform(X)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,3), dpi=120)\n",
    "plt.scatter(X_plot, y, c='blue', alpha=0.2, s=5)\n",
    "plt.scatter(X_plot, y_perfect, c='r', alpha=0.7, s=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_i = self.X[idx]\n",
    "        y_i = self.y[idx]\n",
    "\n",
    "        return X_i, y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArtificialDataset(X, y)\n",
    "\n",
    "data_train, data_valid = torch.utils.data.random_split(\n",
    "    dataset, [N_train, N_valid])\n",
    "\n",
    "BATCH_SIZE_LOADER = 64\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=True, num_workers=mp.cpu_count())\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    data_valid, batch_size=BATCH_SIZE_LOADER,\n",
    "    shuffle=True, num_workers=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalNet(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self, n_samples, input_size, output_size, K):\n",
    "        super().__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        self.linear1 = VariationalLayer(input_size, 32, 0, 3, n_samples)\n",
    "        #self.bn = nn.BatchNorm1d(16)\n",
    "        self.linear2 = VariationalLayer(32, 16, 0, 3, n_samples)\n",
    "        self.linear3 = VariationalLayer(16, output_size, 0, 3, n_samples)\n",
    "        self.K = K\n",
    "    \n",
    "    # Perform the computation\n",
    "    def forward(self, x):\n",
    "        #pdb.set_trace()\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "        x = x.expand((self.n_samples, x.shape[1], x.shape[2]))\n",
    "        x = self.linear1(x)\n",
    "        #x = self.bn(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    def kl_divergence_NN(self):\n",
    "        kl = (\n",
    "            self.linear1.kl_divergence_layer() \n",
    "            + self.linear2.kl_divergence_layer()\n",
    "            + self.linear3.kl_divergence_layer()\n",
    "        )\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "input_size = X.shape[1]\n",
    "output_size = y.shape[1]\n",
    "\n",
    "K = 1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = VariationalNet(n_samples, input_size, output_size, K).to(dev)\n",
    "#h1 = ANN(input_size, output_size).to(dev)\n",
    "#h2 = BayesianNetwork(1).to(dev)\n",
    "opt_h = torch.optim.Adam(h.parameters(), lr=0.005)\n",
    "\n",
    "mse_loss_mean = nn.MSELoss(reduction='mean')\n",
    "mse_loss_sum = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    \n",
    "    mse_running_loss = 0.\n",
    "    kl_running_loss = 0.\n",
    "    \n",
    "    n = len(training_loader.dataset)\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        \n",
    "        x_batch, y_batch = data\n",
    "        opt_h.zero_grad()\n",
    "\n",
    "        y_preds = h(x_batch)\n",
    "        y_preds_mean = y_preds.mean(axis=0).squeeze()\n",
    "\n",
    "        mse_loss_ = mse_loss_sum(y_preds_mean, y_batch)\n",
    "        kl_loss_ = h.K*h.kl_divergence_NN()\n",
    "        \n",
    "        elbo_loss = mse_loss_ + kl_loss_\n",
    "        elbo_loss.backward()\n",
    "        \n",
    "        opt_h.step()\n",
    "        \n",
    "        mse_running_loss += mse_loss_.item()\n",
    "        kl_running_loss += kl_loss_.item()\n",
    "        \n",
    "    mse = mse_running_loss/n\n",
    "    kl = kl_running_loss/n\n",
    "\n",
    "    return mse, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------EPOCH 1------------------\n",
      "MSE LOSS \t train 627.91 valid 478.88\n",
      "KL LOSS \t train 11.93 valid 11.98\n",
      "ELBO LOSS \t train 639.84 valid 490.86\n",
      "------------------EPOCH 2------------------\n",
      "MSE LOSS \t train 480.32 valid 456.31\n",
      "KL LOSS \t train 11.56 valid 11.62\n",
      "ELBO LOSS \t train 491.88 valid 467.93\n",
      "------------------EPOCH 3------------------\n",
      "MSE LOSS \t train 460.45 valid 454.1\n",
      "KL LOSS \t train 11.21 valid 11.28\n",
      "ELBO LOSS \t train 471.66 valid 465.38\n",
      "------------------EPOCH 4------------------\n",
      "MSE LOSS \t train 454.03 valid 446.04\n",
      "KL LOSS \t train 10.89 valid 10.96\n",
      "ELBO LOSS \t train 464.92 valid 456.99\n",
      "------------------EPOCH 5------------------\n",
      "MSE LOSS \t train 451.94 valid 444.59\n",
      "KL LOSS \t train 10.57 valid 10.64\n",
      "ELBO LOSS \t train 462.51 valid 455.23\n",
      "------------------EPOCH 6------------------\n",
      "MSE LOSS \t train 450.6 valid 444.94\n",
      "KL LOSS \t train 10.27 valid 10.33\n",
      "ELBO LOSS \t train 460.87 valid 455.27\n",
      "------------------EPOCH 7------------------\n",
      "MSE LOSS \t train 450.3 valid 444.61\n",
      "KL LOSS \t train 9.97 valid 10.03\n",
      "ELBO LOSS \t train 460.28 valid 454.64\n",
      "------------------EPOCH 8------------------\n",
      "MSE LOSS \t train 450.06 valid 446.19\n",
      "KL LOSS \t train 9.68 valid 9.74\n",
      "ELBO LOSS \t train 459.74 valid 455.93\n",
      "------------------EPOCH 9------------------\n",
      "MSE LOSS \t train 449.72 valid 443.65\n",
      "KL LOSS \t train 9.4 valid 9.44\n",
      "ELBO LOSS \t train 459.12 valid 453.09\n",
      "------------------EPOCH 10------------------\n",
      "MSE LOSS \t train 451.03 valid 444.0\n",
      "KL LOSS \t train 9.12 valid 9.17\n",
      "ELBO LOSS \t train 460.15 valid 453.17\n",
      "------------------EPOCH 11------------------\n",
      "MSE LOSS \t train 449.92 valid 444.06\n",
      "KL LOSS \t train 8.85 valid 8.9\n",
      "ELBO LOSS \t train 458.77 valid 452.96\n",
      "------------------EPOCH 12------------------\n",
      "MSE LOSS \t train 448.47 valid 444.46\n",
      "KL LOSS \t train 8.59 valid 8.65\n",
      "ELBO LOSS \t train 457.06 valid 453.11\n",
      "------------------EPOCH 13------------------\n",
      "MSE LOSS \t train 448.74 valid 444.68\n",
      "KL LOSS \t train 8.36 valid 8.42\n",
      "ELBO LOSS \t train 457.09 valid 453.1\n",
      "------------------EPOCH 14------------------\n",
      "MSE LOSS \t train 448.38 valid 443.95\n",
      "KL LOSS \t train 8.14 valid 8.22\n",
      "ELBO LOSS \t train 456.51 valid 452.17\n",
      "------------------EPOCH 15------------------\n",
      "MSE LOSS \t train 448.88 valid 445.06\n",
      "KL LOSS \t train 7.95 valid 8.02\n",
      "ELBO LOSS \t train 456.83 valid 453.09\n",
      "------------------EPOCH 16------------------\n",
      "MSE LOSS \t train 449.37 valid 444.39\n",
      "KL LOSS \t train 7.78 valid 7.86\n",
      "ELBO LOSS \t train 457.15 valid 452.25\n",
      "------------------EPOCH 17------------------\n",
      "MSE LOSS \t train 448.64 valid 444.78\n",
      "KL LOSS \t train 7.62 valid 7.71\n",
      "ELBO LOSS \t train 456.26 valid 452.49\n",
      "------------------EPOCH 18------------------\n",
      "MSE LOSS \t train 448.98 valid 444.09\n",
      "KL LOSS \t train 7.48 valid 7.58\n",
      "ELBO LOSS \t train 456.46 valid 451.67\n",
      "------------------EPOCH 19------------------\n",
      "MSE LOSS \t train 447.99 valid 446.29\n",
      "KL LOSS \t train 7.36 valid 7.46\n",
      "ELBO LOSS \t train 455.35 valid 453.74\n",
      "------------------EPOCH 20------------------\n",
      "MSE LOSS \t train 448.84 valid 445.56\n",
      "KL LOSS \t train 7.25 valid 7.36\n",
      "ELBO LOSS \t train 456.09 valid 452.92\n",
      "------------------EPOCH 21------------------\n",
      "MSE LOSS \t train 449.04 valid 444.47\n",
      "KL LOSS \t train 7.17 valid 7.29\n",
      "ELBO LOSS \t train 456.21 valid 451.76\n",
      "------------------EPOCH 22------------------\n",
      "MSE LOSS \t train 448.69 valid 444.51\n",
      "KL LOSS \t train 7.1 valid 7.22\n",
      "ELBO LOSS \t train 455.79 valid 451.72\n",
      "------------------EPOCH 23------------------\n",
      "MSE LOSS \t train 448.4 valid 444.09\n",
      "KL LOSS \t train 7.03 valid 7.14\n",
      "ELBO LOSS \t train 455.43 valid 451.23\n",
      "------------------EPOCH 24------------------\n",
      "MSE LOSS \t train 447.91 valid 445.97\n",
      "KL LOSS \t train 6.95 valid 7.06\n",
      "ELBO LOSS \t train 454.86 valid 453.04\n",
      "------------------EPOCH 25------------------\n",
      "MSE LOSS \t train 448.39 valid 444.02\n",
      "KL LOSS \t train 6.88 valid 7.0\n",
      "ELBO LOSS \t train 455.27 valid 451.02\n",
      "------------------EPOCH 26------------------\n",
      "MSE LOSS \t train 448.08 valid 445.34\n",
      "KL LOSS \t train 6.82 valid 6.94\n",
      "ELBO LOSS \t train 454.91 valid 452.28\n",
      "------------------EPOCH 27------------------\n",
      "MSE LOSS \t train 447.81 valid 443.98\n",
      "KL LOSS \t train 6.76 valid 6.87\n",
      "ELBO LOSS \t train 454.57 valid 450.85\n",
      "------------------EPOCH 28------------------\n",
      "MSE LOSS \t train 448.31 valid 445.75\n",
      "KL LOSS \t train 6.7 valid 6.81\n",
      "ELBO LOSS \t train 455.01 valid 452.55\n",
      "------------------EPOCH 29------------------\n",
      "MSE LOSS \t train 449.7 valid 444.7\n",
      "KL LOSS \t train 6.64 valid 6.75\n",
      "ELBO LOSS \t train 456.34 valid 451.45\n",
      "------------------EPOCH 30------------------\n",
      "MSE LOSS \t train 448.24 valid 444.96\n",
      "KL LOSS \t train 6.59 valid 6.71\n",
      "ELBO LOSS \t train 454.83 valid 451.67\n",
      "------------------EPOCH 31------------------\n",
      "MSE LOSS \t train 448.33 valid 444.68\n",
      "KL LOSS \t train 6.54 valid 6.67\n",
      "ELBO LOSS \t train 454.87 valid 451.34\n",
      "------------------EPOCH 32------------------\n",
      "MSE LOSS \t train 447.13 valid 445.69\n",
      "KL LOSS \t train 6.51 valid 6.61\n",
      "ELBO LOSS \t train 453.64 valid 452.3\n",
      "------------------EPOCH 33------------------\n",
      "MSE LOSS \t train 447.47 valid 444.77\n",
      "KL LOSS \t train 6.46 valid 6.57\n",
      "ELBO LOSS \t train 453.92 valid 451.34\n",
      "------------------EPOCH 34------------------\n",
      "MSE LOSS \t train 447.96 valid 445.28\n",
      "KL LOSS \t train 6.42 valid 6.54\n",
      "ELBO LOSS \t train 454.38 valid 451.82\n",
      "------------------EPOCH 35------------------\n",
      "MSE LOSS \t train 447.92 valid 445.3\n",
      "KL LOSS \t train 6.38 valid 6.5\n",
      "ELBO LOSS \t train 454.3 valid 451.79\n",
      "------------------EPOCH 36------------------\n",
      "MSE LOSS \t train 447.35 valid 445.38\n",
      "KL LOSS \t train 6.34 valid 6.45\n",
      "ELBO LOSS \t train 453.69 valid 451.82\n",
      "------------------EPOCH 37------------------\n",
      "MSE LOSS \t train 447.79 valid 444.65\n",
      "KL LOSS \t train 6.29 valid 6.41\n",
      "ELBO LOSS \t train 454.09 valid 451.06\n",
      "------------------EPOCH 38------------------\n",
      "MSE LOSS \t train 448.41 valid 445.88\n",
      "KL LOSS \t train 6.26 valid 6.37\n",
      "ELBO LOSS \t train 454.67 valid 452.25\n",
      "------------------EPOCH 39------------------\n",
      "MSE LOSS \t train 447.37 valid 445.29\n",
      "KL LOSS \t train 6.22 valid 6.33\n",
      "ELBO LOSS \t train 453.6 valid 451.62\n",
      "------------------EPOCH 40------------------\n",
      "MSE LOSS \t train 448.31 valid 445.1\n",
      "KL LOSS \t train 6.19 valid 6.3\n",
      "ELBO LOSS \t train 454.5 valid 451.4\n",
      "------------------EPOCH 41------------------\n",
      "MSE LOSS \t train 447.93 valid 445.81\n",
      "KL LOSS \t train 6.16 valid 6.28\n",
      "ELBO LOSS \t train 454.09 valid 452.09\n",
      "------------------EPOCH 42------------------\n",
      "MSE LOSS \t train 448.48 valid 445.87\n",
      "KL LOSS \t train 6.12 valid 6.24\n",
      "ELBO LOSS \t train 454.6 valid 452.1\n",
      "------------------EPOCH 43------------------\n",
      "MSE LOSS \t train 447.29 valid 445.04\n",
      "KL LOSS \t train 6.09 valid 6.21\n",
      "ELBO LOSS \t train 453.39 valid 451.24\n",
      "------------------EPOCH 44------------------\n",
      "MSE LOSS \t train 447.44 valid 445.43\n",
      "KL LOSS \t train 6.07 valid 6.19\n",
      "ELBO LOSS \t train 453.52 valid 451.62\n",
      "------------------EPOCH 45------------------\n",
      "MSE LOSS \t train 447.03 valid 447.18\n",
      "KL LOSS \t train 6.04 valid 6.15\n",
      "ELBO LOSS \t train 453.06 valid 453.33\n",
      "------------------EPOCH 46------------------\n",
      "MSE LOSS \t train 447.37 valid 444.99\n",
      "KL LOSS \t train 6.01 valid 6.13\n",
      "ELBO LOSS \t train 453.38 valid 451.12\n",
      "------------------EPOCH 47------------------\n",
      "MSE LOSS \t train 446.89 valid 445.42\n",
      "KL LOSS \t train 5.98 valid 6.11\n",
      "ELBO LOSS \t train 452.87 valid 451.53\n",
      "------------------EPOCH 48------------------\n",
      "MSE LOSS \t train 447.04 valid 447.78\n",
      "KL LOSS \t train 5.97 valid 6.08\n",
      "ELBO LOSS \t train 453.01 valid 453.85\n",
      "------------------EPOCH 49------------------\n",
      "MSE LOSS \t train 447.58 valid 445.61\n",
      "KL LOSS \t train 5.94 valid 6.05\n",
      "ELBO LOSS \t train 453.52 valid 451.66\n",
      "------------------EPOCH 50------------------\n",
      "MSE LOSS \t train 447.26 valid 445.48\n",
      "KL LOSS \t train 5.92 valid 6.03\n",
      "ELBO LOSS \t train 453.18 valid 451.52\n",
      "------------------EPOCH 51------------------\n",
      "MSE LOSS \t train 447.75 valid 445.39\n",
      "KL LOSS \t train 5.89 valid 6.01\n",
      "ELBO LOSS \t train 453.65 valid 451.39\n",
      "------------------EPOCH 52------------------\n",
      "MSE LOSS \t train 446.0 valid 446.95\n",
      "KL LOSS \t train 5.88 valid 5.99\n",
      "ELBO LOSS \t train 451.88 valid 452.94\n",
      "------------------EPOCH 53------------------\n",
      "MSE LOSS \t train 447.31 valid 446.18\n",
      "KL LOSS \t train 5.85 valid 5.97\n",
      "ELBO LOSS \t train 453.16 valid 452.15\n",
      "------------------EPOCH 54------------------\n",
      "MSE LOSS \t train 447.6 valid 447.61\n",
      "KL LOSS \t train 5.83 valid 5.94\n",
      "ELBO LOSS \t train 453.42 valid 453.55\n",
      "------------------EPOCH 55------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE LOSS \t train 446.37 valid 447.0\n",
      "KL LOSS \t train 5.81 valid 5.92\n",
      "ELBO LOSS \t train 452.18 valid 452.92\n",
      "------------------EPOCH 56------------------\n",
      "MSE LOSS \t train 446.94 valid 445.92\n",
      "KL LOSS \t train 5.79 valid 5.9\n",
      "ELBO LOSS \t train 452.74 valid 451.82\n",
      "------------------EPOCH 57------------------\n",
      "MSE LOSS \t train 447.75 valid 447.29\n",
      "KL LOSS \t train 5.78 valid 5.88\n",
      "ELBO LOSS \t train 453.53 valid 453.17\n",
      "------------------EPOCH 58------------------\n",
      "MSE LOSS \t train 445.85 valid 445.45\n",
      "KL LOSS \t train 5.75 valid 5.87\n",
      "ELBO LOSS \t train 451.6 valid 451.32\n",
      "------------------EPOCH 59------------------\n",
      "MSE LOSS \t train 446.64 valid 447.45\n",
      "KL LOSS \t train 5.73 valid 5.85\n",
      "ELBO LOSS \t train 452.37 valid 453.3\n",
      "------------------EPOCH 60------------------\n",
      "MSE LOSS \t train 447.02 valid 446.14\n",
      "KL LOSS \t train 5.72 valid 5.84\n",
      "ELBO LOSS \t train 452.74 valid 451.98\n",
      "------------------EPOCH 61------------------\n",
      "MSE LOSS \t train 446.42 valid 445.49\n",
      "KL LOSS \t train 5.71 valid 5.81\n",
      "ELBO LOSS \t train 452.13 valid 451.3\n",
      "------------------EPOCH 62------------------\n",
      "MSE LOSS \t train 445.65 valid 445.73\n",
      "KL LOSS \t train 5.68 valid 5.79\n",
      "ELBO LOSS \t train 451.33 valid 451.52\n",
      "------------------EPOCH 63------------------\n",
      "MSE LOSS \t train 446.25 valid 446.01\n",
      "KL LOSS \t train 5.66 valid 5.77\n",
      "ELBO LOSS \t train 451.91 valid 451.78\n",
      "------------------EPOCH 64------------------\n",
      "MSE LOSS \t train 446.25 valid 446.02\n",
      "KL LOSS \t train 5.65 valid 5.76\n",
      "ELBO LOSS \t train 451.91 valid 451.79\n",
      "------------------EPOCH 65------------------\n",
      "MSE LOSS \t train 446.01 valid 446.67\n",
      "KL LOSS \t train 5.63 valid 5.74\n",
      "ELBO LOSS \t train 451.64 valid 452.41\n",
      "------------------EPOCH 66------------------\n",
      "MSE LOSS \t train 445.75 valid 447.51\n",
      "KL LOSS \t train 5.62 valid 5.73\n",
      "ELBO LOSS \t train 451.37 valid 453.24\n",
      "------------------EPOCH 67------------------\n",
      "MSE LOSS \t train 447.01 valid 445.57\n",
      "KL LOSS \t train 5.6 valid 5.72\n",
      "ELBO LOSS \t train 452.61 valid 451.28\n",
      "------------------EPOCH 68------------------\n",
      "MSE LOSS \t train 446.58 valid 446.96\n",
      "KL LOSS \t train 5.59 valid 5.7\n",
      "ELBO LOSS \t train 452.17 valid 452.66\n",
      "------------------EPOCH 69------------------\n",
      "MSE LOSS \t train 446.41 valid 448.37\n",
      "KL LOSS \t train 5.57 valid 5.69\n",
      "ELBO LOSS \t train 451.98 valid 454.06\n",
      "------------------EPOCH 70------------------\n",
      "MSE LOSS \t train 446.73 valid 446.63\n",
      "KL LOSS \t train 5.56 valid 5.68\n",
      "ELBO LOSS \t train 452.29 valid 452.31\n",
      "------------------EPOCH 71------------------\n",
      "MSE LOSS \t train 445.49 valid 448.59\n",
      "KL LOSS \t train 5.54 valid 5.65\n",
      "ELBO LOSS \t train 451.04 valid 454.24\n",
      "------------------EPOCH 72------------------\n",
      "MSE LOSS \t train 446.17 valid 447.63\n",
      "KL LOSS \t train 5.54 valid 5.65\n",
      "ELBO LOSS \t train 451.71 valid 453.28\n",
      "------------------EPOCH 73------------------\n",
      "MSE LOSS \t train 446.03 valid 446.57\n",
      "KL LOSS \t train 5.53 valid 5.64\n",
      "ELBO LOSS \t train 451.56 valid 452.2\n",
      "------------------EPOCH 74------------------\n",
      "MSE LOSS \t train 446.39 valid 446.22\n",
      "KL LOSS \t train 5.51 valid 5.62\n",
      "ELBO LOSS \t train 451.9 valid 451.85\n",
      "------------------EPOCH 75------------------\n",
      "MSE LOSS \t train 445.2 valid 446.32\n",
      "KL LOSS \t train 5.49 valid 5.61\n",
      "ELBO LOSS \t train 450.69 valid 451.92\n",
      "------------------EPOCH 76------------------\n",
      "MSE LOSS \t train 446.54 valid 447.99\n",
      "KL LOSS \t train 5.49 valid 5.59\n",
      "ELBO LOSS \t train 452.03 valid 453.59\n",
      "------------------EPOCH 77------------------\n",
      "MSE LOSS \t train 446.04 valid 447.18\n",
      "KL LOSS \t train 5.48 valid 5.59\n",
      "ELBO LOSS \t train 451.52 valid 452.77\n",
      "------------------EPOCH 78------------------\n",
      "MSE LOSS \t train 445.24 valid 448.73\n",
      "KL LOSS \t train 5.46 valid 5.57\n",
      "ELBO LOSS \t train 450.7 valid 454.3\n",
      "------------------EPOCH 79------------------\n",
      "MSE LOSS \t train 445.95 valid 447.71\n",
      "KL LOSS \t train 5.45 valid 5.56\n",
      "ELBO LOSS \t train 451.39 valid 453.27\n",
      "------------------EPOCH 80------------------\n",
      "MSE LOSS \t train 445.67 valid 447.64\n",
      "KL LOSS \t train 5.43 valid 5.54\n",
      "ELBO LOSS \t train 451.1 valid 453.18\n",
      "------------------EPOCH 81------------------\n",
      "MSE LOSS \t train 446.64 valid 447.2\n",
      "KL LOSS \t train 5.42 valid 5.54\n",
      "ELBO LOSS \t train 452.06 valid 452.74\n",
      "------------------EPOCH 82------------------\n",
      "MSE LOSS \t train 446.44 valid 447.94\n",
      "KL LOSS \t train 5.42 valid 5.53\n",
      "ELBO LOSS \t train 451.86 valid 453.47\n",
      "------------------EPOCH 83------------------\n",
      "MSE LOSS \t train 446.58 valid 447.81\n",
      "KL LOSS \t train 5.41 valid 5.52\n",
      "ELBO LOSS \t train 451.99 valid 453.34\n",
      "------------------EPOCH 84------------------\n",
      "MSE LOSS \t train 445.08 valid 447.84\n",
      "KL LOSS \t train 5.4 valid 5.51\n",
      "ELBO LOSS \t train 450.48 valid 453.35\n",
      "------------------EPOCH 85------------------\n",
      "MSE LOSS \t train 446.3 valid 448.33\n",
      "KL LOSS \t train 5.39 valid 5.51\n",
      "ELBO LOSS \t train 451.69 valid 453.83\n",
      "------------------EPOCH 86------------------\n",
      "MSE LOSS \t train 444.51 valid 447.19\n",
      "KL LOSS \t train 5.38 valid 5.49\n",
      "ELBO LOSS \t train 449.9 valid 452.68\n",
      "------------------EPOCH 87------------------\n",
      "MSE LOSS \t train 445.72 valid 447.43\n",
      "KL LOSS \t train 5.37 valid 5.48\n",
      "ELBO LOSS \t train 451.08 valid 452.91\n",
      "------------------EPOCH 88------------------\n",
      "MSE LOSS \t train 445.18 valid 447.68\n",
      "KL LOSS \t train 5.36 valid 5.46\n",
      "ELBO LOSS \t train 450.54 valid 453.14\n",
      "------------------EPOCH 89------------------\n",
      "MSE LOSS \t train 445.76 valid 447.71\n",
      "KL LOSS \t train 5.36 valid 5.47\n",
      "ELBO LOSS \t train 451.12 valid 453.18\n",
      "------------------EPOCH 90------------------\n",
      "MSE LOSS \t train 446.59 valid 447.47\n",
      "KL LOSS \t train 5.36 valid 5.46\n",
      "ELBO LOSS \t train 451.95 valid 452.93\n",
      "------------------EPOCH 91------------------\n",
      "MSE LOSS \t train 444.9 valid 450.56\n",
      "KL LOSS \t train 5.35 valid 5.46\n",
      "ELBO LOSS \t train 450.25 valid 456.02\n",
      "------------------EPOCH 92------------------\n",
      "MSE LOSS \t train 445.38 valid 447.63\n",
      "KL LOSS \t train 5.34 valid 5.45\n",
      "ELBO LOSS \t train 450.72 valid 453.08\n",
      "------------------EPOCH 93------------------\n",
      "MSE LOSS \t train 444.73 valid 447.89\n",
      "KL LOSS \t train 5.33 valid 5.44\n",
      "ELBO LOSS \t train 450.07 valid 453.33\n",
      "------------------EPOCH 94------------------\n",
      "MSE LOSS \t train 444.99 valid 447.53\n",
      "KL LOSS \t train 5.33 valid 5.43\n",
      "ELBO LOSS \t train 450.32 valid 452.97\n",
      "------------------EPOCH 95------------------\n",
      "MSE LOSS \t train 444.62 valid 448.69\n",
      "KL LOSS \t train 5.32 valid 5.42\n",
      "ELBO LOSS \t train 449.94 valid 454.11\n",
      "------------------EPOCH 96------------------\n",
      "MSE LOSS \t train 445.24 valid 447.48\n",
      "KL LOSS \t train 5.31 valid 5.41\n",
      "ELBO LOSS \t train 450.55 valid 452.9\n",
      "------------------EPOCH 97------------------\n",
      "MSE LOSS \t train 444.89 valid 450.45\n",
      "KL LOSS \t train 5.3 valid 5.41\n",
      "ELBO LOSS \t train 450.19 valid 455.85\n",
      "------------------EPOCH 98------------------\n",
      "MSE LOSS \t train 444.35 valid 448.53\n",
      "KL LOSS \t train 5.29 valid 5.39\n",
      "ELBO LOSS \t train 449.64 valid 453.93\n",
      "------------------EPOCH 99------------------\n",
      "MSE LOSS \t train 444.59 valid 448.95\n",
      "KL LOSS \t train 5.28 valid 5.4\n",
      "ELBO LOSS \t train 449.87 valid 454.34\n",
      "------------------EPOCH 100------------------\n",
      "MSE LOSS \t train 445.11 valid 448.75\n",
      "KL LOSS \t train 5.27 valid 5.38\n",
      "ELBO LOSS \t train 450.38 valid 454.13\n",
      "------------------EPOCH 101------------------\n",
      "MSE LOSS \t train 446.01 valid 448.82\n",
      "KL LOSS \t train 5.27 valid 5.37\n",
      "ELBO LOSS \t train 451.28 valid 454.19\n",
      "------------------EPOCH 102------------------\n",
      "MSE LOSS \t train 443.79 valid 449.36\n",
      "KL LOSS \t train 5.26 valid 5.36\n",
      "ELBO LOSS \t train 449.05 valid 454.72\n",
      "------------------EPOCH 103------------------\n",
      "MSE LOSS \t train 444.91 valid 449.1\n",
      "KL LOSS \t train 5.25 valid 5.35\n",
      "ELBO LOSS \t train 450.16 valid 454.45\n",
      "------------------EPOCH 104------------------\n",
      "MSE LOSS \t train 445.74 valid 448.25\n",
      "KL LOSS \t train 5.24 valid 5.35\n",
      "ELBO LOSS \t train 450.99 valid 453.6\n",
      "------------------EPOCH 105------------------\n",
      "MSE LOSS \t train 443.92 valid 449.33\n",
      "KL LOSS \t train 5.23 valid 5.34\n",
      "ELBO LOSS \t train 449.16 valid 454.67\n",
      "------------------EPOCH 106------------------\n",
      "MSE LOSS \t train 446.27 valid 448.36\n",
      "KL LOSS \t train 5.23 valid 5.33\n",
      "ELBO LOSS \t train 451.5 valid 453.69\n",
      "------------------EPOCH 107------------------\n",
      "MSE LOSS \t train 444.91 valid 448.52\n",
      "KL LOSS \t train 5.23 valid 5.33\n",
      "ELBO LOSS \t train 450.13 valid 453.85\n",
      "------------------EPOCH 108------------------\n",
      "MSE LOSS \t train 443.98 valid 449.15\n",
      "KL LOSS \t train 5.22 valid 5.32\n",
      "ELBO LOSS \t train 449.2 valid 454.47\n",
      "------------------EPOCH 109------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE LOSS \t train 443.44 valid 448.93\n",
      "KL LOSS \t train 5.21 valid 5.32\n",
      "ELBO LOSS \t train 448.65 valid 454.24\n",
      "------------------EPOCH 110------------------\n",
      "MSE LOSS \t train 444.56 valid 448.33\n",
      "KL LOSS \t train 5.21 valid 5.31\n",
      "ELBO LOSS \t train 449.76 valid 453.64\n",
      "------------------EPOCH 111------------------\n",
      "MSE LOSS \t train 444.7 valid 448.74\n",
      "KL LOSS \t train 5.2 valid 5.3\n",
      "ELBO LOSS \t train 449.9 valid 454.05\n",
      "------------------EPOCH 112------------------\n",
      "MSE LOSS \t train 444.05 valid 448.82\n",
      "KL LOSS \t train 5.19 valid 5.3\n",
      "ELBO LOSS \t train 449.25 valid 454.12\n",
      "------------------EPOCH 113------------------\n",
      "MSE LOSS \t train 444.23 valid 448.87\n",
      "KL LOSS \t train 5.19 valid 5.29\n",
      "ELBO LOSS \t train 449.42 valid 454.16\n",
      "------------------EPOCH 114------------------\n",
      "MSE LOSS \t train 444.98 valid 449.08\n",
      "KL LOSS \t train 5.18 valid 5.29\n",
      "ELBO LOSS \t train 450.16 valid 454.36\n",
      "------------------EPOCH 115------------------\n",
      "MSE LOSS \t train 444.17 valid 448.64\n",
      "KL LOSS \t train 5.17 valid 5.29\n",
      "ELBO LOSS \t train 449.35 valid 453.93\n",
      "------------------EPOCH 116------------------\n",
      "MSE LOSS \t train 444.19 valid 452.55\n",
      "KL LOSS \t train 5.17 valid 5.27\n",
      "ELBO LOSS \t train 449.36 valid 457.83\n",
      "------------------EPOCH 117------------------\n",
      "MSE LOSS \t train 444.27 valid 449.57\n",
      "KL LOSS \t train 5.17 valid 5.27\n",
      "ELBO LOSS \t train 449.44 valid 454.84\n",
      "------------------EPOCH 118------------------\n",
      "MSE LOSS \t train 443.32 valid 448.56\n",
      "KL LOSS \t train 5.15 valid 5.26\n",
      "ELBO LOSS \t train 448.47 valid 453.82\n",
      "------------------EPOCH 119------------------\n",
      "MSE LOSS \t train 443.64 valid 449.41\n",
      "KL LOSS \t train 5.14 valid 5.25\n",
      "ELBO LOSS \t train 448.79 valid 454.66\n",
      "------------------EPOCH 120------------------\n",
      "MSE LOSS \t train 444.0 valid 449.29\n",
      "KL LOSS \t train 5.14 valid 5.25\n",
      "ELBO LOSS \t train 449.14 valid 454.54\n",
      "------------------EPOCH 121------------------\n",
      "MSE LOSS \t train 444.53 valid 449.81\n",
      "KL LOSS \t train 5.14 valid 5.24\n",
      "ELBO LOSS \t train 449.66 valid 455.06\n",
      "------------------EPOCH 122------------------\n",
      "MSE LOSS \t train 443.92 valid 451.11\n",
      "KL LOSS \t train 5.14 valid 5.24\n",
      "ELBO LOSS \t train 449.05 valid 456.35\n",
      "------------------EPOCH 123------------------\n",
      "MSE LOSS \t train 444.81 valid 450.99\n",
      "KL LOSS \t train 5.13 valid 5.23\n",
      "ELBO LOSS \t train 449.93 valid 456.22\n",
      "------------------EPOCH 124------------------\n",
      "MSE LOSS \t train 443.73 valid 452.22\n",
      "KL LOSS \t train 5.12 valid 5.23\n",
      "ELBO LOSS \t train 448.86 valid 457.46\n",
      "------------------EPOCH 125------------------\n",
      "MSE LOSS \t train 444.03 valid 450.68\n",
      "KL LOSS \t train 5.12 valid 5.23\n",
      "ELBO LOSS \t train 449.15 valid 455.91\n",
      "------------------EPOCH 126------------------\n",
      "MSE LOSS \t train 443.93 valid 450.1\n",
      "KL LOSS \t train 5.12 valid 5.22\n",
      "ELBO LOSS \t train 449.05 valid 455.33\n",
      "------------------EPOCH 127------------------\n",
      "MSE LOSS \t train 444.21 valid 449.11\n",
      "KL LOSS \t train 5.11 valid 5.22\n",
      "ELBO LOSS \t train 449.32 valid 454.33\n",
      "------------------EPOCH 128------------------\n",
      "MSE LOSS \t train 443.59 valid 452.97\n",
      "KL LOSS \t train 5.1 valid 5.22\n",
      "ELBO LOSS \t train 448.7 valid 458.19\n",
      "------------------EPOCH 129------------------\n",
      "MSE LOSS \t train 443.48 valid 449.06\n",
      "KL LOSS \t train 5.11 valid 5.21\n",
      "ELBO LOSS \t train 448.58 valid 454.27\n",
      "------------------EPOCH 130------------------\n",
      "MSE LOSS \t train 443.54 valid 449.31\n",
      "KL LOSS \t train 5.1 valid 5.21\n",
      "ELBO LOSS \t train 448.64 valid 454.52\n",
      "------------------EPOCH 131------------------\n",
      "MSE LOSS \t train 443.05 valid 449.86\n",
      "KL LOSS \t train 5.1 valid 5.21\n",
      "ELBO LOSS \t train 448.15 valid 455.07\n",
      "------------------EPOCH 132------------------\n",
      "MSE LOSS \t train 444.38 valid 451.61\n",
      "KL LOSS \t train 5.09 valid 5.2\n",
      "ELBO LOSS \t train 449.48 valid 456.81\n",
      "------------------EPOCH 133------------------\n",
      "MSE LOSS \t train 444.31 valid 449.4\n",
      "KL LOSS \t train 5.09 valid 5.2\n",
      "ELBO LOSS \t train 449.4 valid 454.6\n",
      "------------------EPOCH 134------------------\n",
      "MSE LOSS \t train 444.64 valid 450.61\n",
      "KL LOSS \t train 5.09 valid 5.19\n",
      "ELBO LOSS \t train 449.73 valid 455.8\n",
      "------------------EPOCH 135------------------\n",
      "MSE LOSS \t train 444.05 valid 450.22\n",
      "KL LOSS \t train 5.09 valid 5.19\n",
      "ELBO LOSS \t train 449.13 valid 455.41\n",
      "------------------EPOCH 136------------------\n",
      "MSE LOSS \t train 443.44 valid 450.24\n",
      "KL LOSS \t train 5.08 valid 5.19\n",
      "ELBO LOSS \t train 448.52 valid 455.43\n",
      "------------------EPOCH 137------------------\n",
      "MSE LOSS \t train 443.06 valid 448.92\n",
      "KL LOSS \t train 5.07 valid 5.18\n",
      "ELBO LOSS \t train 448.13 valid 454.1\n",
      "------------------EPOCH 138------------------\n",
      "MSE LOSS \t train 444.35 valid 450.69\n",
      "KL LOSS \t train 5.07 valid 5.18\n",
      "ELBO LOSS \t train 449.41 valid 455.87\n",
      "------------------EPOCH 139------------------\n",
      "MSE LOSS \t train 442.97 valid 449.65\n",
      "KL LOSS \t train 5.07 valid 5.17\n",
      "ELBO LOSS \t train 448.04 valid 454.82\n",
      "------------------EPOCH 140------------------\n",
      "MSE LOSS \t train 443.59 valid 453.82\n",
      "KL LOSS \t train 5.06 valid 5.16\n",
      "ELBO LOSS \t train 448.66 valid 458.98\n",
      "------------------EPOCH 141------------------\n",
      "MSE LOSS \t train 443.18 valid 449.24\n",
      "KL LOSS \t train 5.06 valid 5.17\n",
      "ELBO LOSS \t train 448.24 valid 454.42\n",
      "------------------EPOCH 142------------------\n",
      "MSE LOSS \t train 444.13 valid 450.33\n",
      "KL LOSS \t train 5.06 valid 5.17\n",
      "ELBO LOSS \t train 449.19 valid 455.5\n",
      "------------------EPOCH 143------------------\n",
      "MSE LOSS \t train 443.01 valid 450.28\n",
      "KL LOSS \t train 5.06 valid 5.17\n",
      "ELBO LOSS \t train 448.07 valid 455.46\n",
      "------------------EPOCH 144------------------\n",
      "MSE LOSS \t train 442.94 valid 449.93\n",
      "KL LOSS \t train 5.06 valid 5.16\n",
      "ELBO LOSS \t train 447.99 valid 455.1\n",
      "------------------EPOCH 145------------------\n",
      "MSE LOSS \t train 442.9 valid 452.45\n",
      "KL LOSS \t train 5.05 valid 5.16\n",
      "ELBO LOSS \t train 447.96 valid 457.61\n",
      "------------------EPOCH 146------------------\n",
      "MSE LOSS \t train 443.77 valid 450.94\n",
      "KL LOSS \t train 5.06 valid 5.16\n",
      "ELBO LOSS \t train 448.83 valid 456.11\n",
      "------------------EPOCH 147------------------\n",
      "MSE LOSS \t train 442.8 valid 451.2\n",
      "KL LOSS \t train 5.05 valid 5.16\n",
      "ELBO LOSS \t train 447.85 valid 456.36\n",
      "------------------EPOCH 148------------------\n",
      "MSE LOSS \t train 442.9 valid 453.63\n",
      "KL LOSS \t train 5.05 valid 5.15\n",
      "ELBO LOSS \t train 447.95 valid 458.78\n",
      "------------------EPOCH 149------------------\n",
      "MSE LOSS \t train 444.11 valid 449.14\n",
      "KL LOSS \t train 5.04 valid 5.15\n",
      "ELBO LOSS \t train 449.16 valid 454.29\n",
      "------------------EPOCH 150------------------\n",
      "MSE LOSS \t train 443.43 valid 449.98\n",
      "KL LOSS \t train 5.05 valid 5.15\n",
      "ELBO LOSS \t train 448.47 valid 455.13\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 150\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('------------------EPOCH {}------------------'.format(\n",
    "        epoch_number + 1))\n",
    "\n",
    "    h.train(True)\n",
    "    avg_mse_loss, avg_kl_loss = train_one_epoch(\n",
    "        epoch_number)\n",
    "    avg_loss = avg_mse_loss + avg_kl_loss\n",
    "    \n",
    "    h.train(False)\n",
    "    mse_running_loss_v = 0.0\n",
    "    kl_running_loss_v = 0.0\n",
    "    \n",
    "    n = len(validation_loader.dataset)\n",
    "    \n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        \n",
    "        x_val_batch, y_val_batch = vdata\n",
    "\n",
    "        y_val_preds = h(x_val_batch)\n",
    "        y_val_preds_mean = y_val_preds.mean(axis=0).squeeze()\n",
    "        \n",
    "        mse_loss_ = mse_loss_sum(y_val_preds_mean, y_val_batch)\n",
    "        kl_loss_ = h.K*h.kl_divergence_NN()\n",
    "        elbo_loss = mse_loss_ + kl_loss_\n",
    "        \n",
    "        mse_running_loss_v += mse_loss_\n",
    "        kl_running_loss_v += kl_loss_\n",
    "        \n",
    "    avg_vmseloss = (mse_running_loss_v / n).item()\n",
    "    avg_vklloss = (kl_running_loss_v / n).item()\n",
    "    \n",
    "    avg_vloss = avg_vmseloss + avg_vklloss\n",
    "    \n",
    "    print('MSE LOSS \\t train {} valid {}'.format(\n",
    "        round(avg_mse_loss, 2), round(avg_vmseloss, 2)))\n",
    "    print('KL LOSS \\t train {} valid {}'.format(\n",
    "        round(avg_kl_loss, 2), round(avg_vklloss, 2)))\n",
    "    print('ELBO LOSS \\t train {} valid {}'.format(\n",
    "        round(avg_loss, 2), round(avg_vloss, 2)))\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5000, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(validation_loader.dataset.dataset.X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_curve(X_test, y_test, h):\n",
    "    xt = torch.hstack(\n",
    "    (X_test[:,0], \n",
    "     torch.tensor(np.arange(-1.4, -1, 1/500), dtype = torch.float32), \n",
    "     torch.tensor(np.arange(1, 1.4, 1/500), dtype = torch.float32)))\n",
    "    \n",
    "    plt.scatter(X_test[:,0].detach(), y_test.detach())\n",
    "    for i in range(0, 100):\n",
    "        plt.scatter(xt.detach(), \n",
    "                    h(xt.unsqueeze(1)).detach().squeeze()[0], \n",
    "                    color='r', alpha=0.01)\n",
    "        plt.ylim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA4yklEQVR4nO3deXyU5b338c8v+8ISdkggLILIJqARUNC64lKX2Erdui/Ubs9jj9rKU0+12zmeYqueqq1U29paNwpGrQq4oQiiLAk7YYcwgZAACYRM1rmeP64ZspBltntmkvzer9e8MnPPPfdcY2S+uXYxxqCUUkr5xEW7AEoppWKLBoNSSqlmNBiUUko1o8GglFKqGQ0GpZRSzWgwKKWUaiYswSAifxGRIyKyucmxviLyjojs9P7s08Zrv+Y9Z6eIfC0c5VFKKRW8cNUY/gZc0+LY/cB7xpgxwHvex82ISF/gQWA6MA14sK0AUUopFRlhCQZjzEfAsRaHbwKe895/Dsht5aVXA+8YY44ZY44D73BmwCillIqgBAevPcgYc8h7/zAwqJVzsoCiJo8Peo+dQUTmAnMB0tPTzz/nnHPCWFSllOr61q1bV2aMGdDReU4Gw2nGGCMiIa29YYxZACwAyMnJMWvXrg1L2ZRSqrsQkf3+nOfkqKQSERniLcwQ4Egr57iAYU0eD/UeU0opFSVOBsPrgG+U0deA11o5ZykwW0T6eDudZ3uPKaWUipJwDVd9EfgEGCsiB0XkW8DDwFUishO40vsYEckRkWcAjDHHgF8Ba7y3X3qPKaWUihLpjMtuax+DUkoFTkTWGWNyOjovIp3PSimlQmAMeDz2pwjExdmfDtFgUEqpWObxgNsNtbU2GBISICkJkpMdCwcNBqWUilXGwMmTUFZmQyApyQZDba2tNSQlOfK2GgxKKRWLfKGwY4d93LOnPVZdDb16QU2NY8Ggq6sqpVSsMQYqK20olJdDQ4N9fPKkrTnU1NgmJodojUEppWJNVRXs3g3HjkFiIpw6BfHxtqbg62tITHTs7bXGoJRSscTthu3boaTEhkFNja0tGGPvnzplO56Tkx0rggaDUkrFCl8olJXZTub6ett8lJxsawrHj9vj/fvb0HCIBoNSSsWCmhrYtcs2HyUl2ccnT0JqKlRUwJEj9vigQbbPwcE+Bg0GpZSKttpa2L/fNh/Fxdkv/tpaWytwuaCoyNYUBg60tYqqKvu8Q7TzWSmloqm2Fg4cgIMHbS2gvt5++RtjA6GkBDIzISMDSkvt+SK2eSklxZEiaTAopVS0tAwFEduPUF0Nhw7ZTufkZFtzOHIE+vWzNQqw8xocosGglFLRUFdnm4kOHWpcB+nYMRsGhw41TmCrq7NzGRIS7HPV1bam4OACqBoMSikVaQ0NtgbgCwBjbAfzoUP2Z319Ywd0Q4O9nTplHw8ebGsRCc59fWswKKVUJDU02OGou3c3hsLRo7Bnj+1UTkiwzUVutz03OdnWGioqYMAAO1Q1Lc3ReQwaDEopFSkNDY0h4HbbJiSXC/btswERF2ePVVfb++npNhROnYIhQ2DUKBsK6ekaDEop1el5PLZjef9+21fg8cDevbb5yLe/gq8GIWL7EerqbICcdZYNhaQk6NHDDlt1cIKbBoNSSjnN47EdyLt2wYkT9gt/507bzxAXZ2/19Y21hpSUxhrFOefAiBE2LAYMsAHhYG0BHJ7gJiJjRaSgye2EiNzd4pxLRaSiyTk/d7JMSikVUcbYMNi924ZDZSVs3GiHqSYm2iCoqbFh4dtzweOxfQ3jx8OwYfY6gwfDmDGOzV1oytEagzGmEJgCICLxgAt4tZVTVxhjrneyLEopFRVVVbamUFZmO5C3brVB0bOnrSXU1dlw8NUCPB7bTDR5MvTta/sl+vWDoUMbJ78lJXWZpqQrgN3GmP0RfE+llIqe6mrbZHT4sA2G/fvtF3vPnjYQfCGQmGgDQAR697Z9ChkZ9lh6uq0lFBfbAPF1Pvfp41g4RDIYbgNebOO5C0VkA1AM3GuM2RK5YimllAN8oVBUZG/Fxbb20DQUEhIaQyE11XYsDx/eeE5qqm162revMQxSUuy+DElJ9qcDIhIMIpIE3AjMa+Xp9cBwY0yliFwH5AFjWrnGXGAuQHZ2tnOFVUqpUFVX293Xdu+2fQnFxbYfIS3NLoPR0GBDITm5sVYwcGDjHAXfyKSiIrsMd2qqve3fb5uXxo2zK6925mAArgXWG2NKWj5hjDnR5P5bIvKUiPQ3xpS1OG8BsAAgJyfHubngSikVipoaW1MoLLTDUU+etMfT0mwIGGMDwRjbZ9Crl52j0LOnrTE0NNhrFBbaOQ5Hj9qgMcb2M1RV2dDo29exjxCpYLidNpqRRGQwUGKMMSIyDTtS6miEyqWUUuFTW2s7mjdssH/d19TYL39fX4BvVVRj7G3QIHtLSbE1ArfbznXYtMmupFpcbAOiuto2Ix04YEPE7Ybp0x37GI4Hg4ikA1cB321y7C4AY8yfgFuA74lIPeAGbjPGwdWhlFLKCbW1dkbzmjV20lp1deOeCb4JbMnJtm8hOdkupd27d+OIpFOnbPPTtm02EI4dg4ICGyxgl98GO2S1d2/b9+AQx4PBGHMK6Nfi2J+a3H8CeMLpciillGPq6mwH8bp19q/6hgZ7zMcY21ksYvsTsrPtz7g429dw5Igdxrppkw2EoiK7FHdriopg0iQ70skhOvNZKaVCUV9vw+DTTxtDoba2cRazbxVUY2yT0ejRNhSqq20T04EDsGqV7ag+ftx2Nre3bWd1tW2mKitr+5wQaTAopVSwfKGwfLlt/qmvtzffUFTfSqm1tXbU0Zgx9rmTJ23tYeNG+OQT2/Tk62j2x7FjthPaIRoMSikVjIYG26zz/vv2Z3y8DYDqalsjSEiwX/5xcbbDeMwY+7i83H6xf/KJ7VPw7ekcCN/7OUSDQSmlAuXx2L/y33mncXXU6mpbW0hLs1/aIjYc+vSxi+DV19saQVERrF0LmzfbEUzBqK+3y2o4RINBKaUC4QuFN9+0bf2+jXRqamwfQlKS7U9ITLSroQ4b1jiMde1a2zewebO9Rig2bQrP52mFBoNSqlvJy3cxf2khrnI38SI0GENWRir3XT2W3KlZ7b+4vt7+xf/mm3ZUUFKSrSm43XaiWnx8465rWVm2Cen4cVi/3s5tKCmxQ1DDYfPm8FynFRoMSqluIS/fxUOvb6Hc3TiMtME7ZcpV7ua+f20AaDsc6ursTOalS+1f/YmJdu5BXZ3tU4iPb5yjMGqUncm8dy989JEderpzZ+i1hKYcDAZH92NQSqlYkJfvYt7iTc1CoaW6BsMv3mhj/c76ejt57b337IzkhATbPFRX17j/si8ghg2z/Qv5+bZmsWOHDYdwhgLYmopDtMaglOry5i8txF3X0OF5x6taCQ7f5LV33rGjieLj7VDRmhpbK/A1H6Wl2SGp5eX2r/n9++1wVAcnojlFg0Ep1eUVl7uDe6FvmYsVK2xNwTcnwbdSqu9xfLzdP+HAARsGvlnQnZQGg1Kqy8vMSMUVaDj4Vkn97DPbaeyrKdTV2ZoC2Ilqycl2Abx9+2xN4bPP/J+oFqO0j0Ep1eVdds4AxI/z0hK9X4nV1XZpirVr7TpGDQ2NS1+npto+hLo6+xhsKHz8Mbz9dqcPBdAag1Kqi8vLd/HymiL8WbI5OTHeBsC2bfav/8OHbc2haUdzQoJd2fTkSRsYBw7ABx/Y/odIyupgaG0INBiUUl3az17dRF2Dfyv5lx89YUNhyxbbfFRdbYPBt7kO2HkJvqalQ4fg3XcdLH07usBGPUopFXF5+S5O1XY8GgkAt5upbhfsjbM1hcpKW0vwrXdkTOM+Cb51ktavd/YDtCfBua9vDQalVJfim9lcXO72q/kIALebgSU7+ep5/WzTUGWlnbsQF9e4jPaBA7b5qLLSjlKKtkmTHLu0BoNSqsvwTWTzZ87CaVVVDC7ZSa/SY+Sm94KyisaaQn29bToqK7O1iLIyu9VmLJg82bFLazAopboMfyeyAWAMUnWKIYcKST9eSe/6UvI+rSU3u5etKVRV2aajqio7Mmn1amcLH4iEBDjrLOcu79iVlVLKD02bfjL9XcyuDX7PVaitJeXwAUbt30RGxVFE6omrqmdFdQW5o6bYmoHL1Thsdd++oMrjmGuusctvOMTxYBCRfcBJoAGoN8bktHhegMeB64Aq4OvGmCj26CilIqVl04+r3M28xXY56UDDIS/fhUDH/QqnTjFw08dM27wWat3UJ6UQf8JN7149cbnr7EijAwfsAnmx0JfQ0owZtrbgGyXlgEjVGC4zxrS1Qem1wBjvbTrwR+9PpVQX11rTj7uugflLC9sMhrZqGL94Y0vHoXDyJNkr3+TKDW/SgKEmcTDJJw/Si1T2Hq1kJHWwR2x/goOrlwZt1Cg7l6JvXxg82LG3iYWmpJuAvxtjDLBaRDJEZIgxJsxLESqlYk1baxj5jrcMgcvOGcDLa4pOz0vwLZe9dv+x1hfAa6qigrNWvMYVW14hHTjFAPrXbScDKALOAupJhNXH7MijWJSeDlOm2FrDoEGOvU0kgsEAy0TEAE8bYxa0eD4L+3vxOeg91iwYRGQuMBcgOzvbudIqpSKmd2pi60thCzyQt4lF61zNmpmeX33gjFPrGkyrx5s5doxxHy7ikh2vkQHUAGMoPf30KKACGEQdVHYQMNEwaBCcdx5Mm2ZDYeLEzt3HAMwyxrhEZCDwjohsN8Z8FOhFvIGyACAnJ8fv4clKqdhV1+Bp9bgx8M/VB/yfh9Ce0lKmfvAS0/cupTdQi/3Ls6l4oC+QG473C6dzzoHx42H0aJgwwQ5RHT68cblvhzgeDMYYl/fnERF5FZgGNA0GFzCsyeOh3mNKqS6madNQ79TEdmclhyUUSkqY/u4/OO/gcnpiQyGzvfIRA+GQlQVjx8KIETYEpk61gTBwoO1wFn+WAwyNo8EgIulAnDHmpPf+bOCXLU57HfihiLyE7XSu0P4FpbqevHwX9y3cQJ3HfuW3t5taWBw6xKxlf2Py4ZWkA/W0HwoAUWtESk6GkSPtaKMRI2xNYepUOPts6NPH0eUvWuP0uw0CXrUjUkkAXjDGLBGRuwCMMX8C3sIOVd2FHa76DYfLpJSKkKY1BAhTLcAfLheXLXmGCWVrSMGGwpBIvXcgBg+2tYMhQyAz04bBBRfYWoNvI6AocDQYjDF7gDPmbXsDwXffAD9wshxKqcgLanmKcCgq4qq3nuac8gKSsWEUU6HQo4cNgWHD7G3MGMjJsR3KAwZAYmK0SxgTw1WVUl1QQMtThMu+fVz7xlOMqdpKkveQc6P9AzRggG0qysy0o4wuuACmT7fhkJ4etdpBazQYlFKOaHef5cpKeuzewFmffMaIilUk4eFI3Dh2nHseh8+dgRk8JPBO1t27uem1PzCibhcJQBLQJ8Ayh/1v9ZQUGwLZ2fY2diycf35jZ3IM1A5ao8GglHJERlrimZPOjEF27eSiZc8zpXI9ydihoh4g01PA5IICKPgLS3pew77Zl1ObPRqSks68eEs7dnDLq48zlP3EAQOJ8r7F6el2lvKIEbb/YNo021wUgaGm4aDBoJQKu7x8FxUtQ+HECTJWvs3NG1+iN3XUAwM480voEHDrySWcWLSEdZxLweXXUjluqm2bb822bdzx+qMMphgIrT8h5FFJvXrZEUW+kUXnnWdnKg8aFLGhpuGgwaCUCps7//wJK3e32PvY4yFux3YuXvI8E2s2MhhIaecavi92AS5mI9Pe38iO9wey8rwvcjznIjt802fLFr7270foTymCHQYZFRkZNgRGjWocWTRqFPTuHfGhpuHQ+UqslIpJrYZCeTn9P/43N255iT4E1hHcs8n9Hhxh/Po/cmT9H3kr6xZODM9i+KoVXMN6hoah7EHr1QtmzbK7qeXk2J9RHmoaDhoMSqmwaBYKDQ3Eb9/CpW/9g3GebQwP8do9vbcBwATXv6K/NkKvXnDxxXDRRTBzpu1U7tPH9od0kuai9mgwKKXC69gxBq94jc9vX0QGMTRc1A8djhHq2RMuuwyuuAIuucTOVu7RI+Y7kwOlwaCUCllevsuOOCrczhVv/42xtVsYhh1x1JnktvfkjTfCLbfYmsKQIV2mdtAaDQalVEjy8l3c90oB8TsK+dKbfyS9fg8jol2ocLr2WvjmN20NoV+/Llc7aI0Gg1IqJPOXFlJfWsqVK/NIqd/D2GgXKFxmzYJ777WB0Lt3p+5MDpQGg1IqJK7SE2Rv/IwBpR8zPtqFCVEekDtwIMybB3feCf37d9nmovZoMCilgpaX7yLp4F7Oz18W8PITsWgFceQuWWI3x0lOjnZxoqb71I2UUmE3P6+ASQWr6Fm/u1P3K1QCS4G8rzxl1zHqxqEAGgxKqRAcW/sZo3e8x6RoFyQER4FVZPLurU8wePyYbtWX0BZtSlJKBSwv38VDz33MhetXkMbxaBcnaC7glSFfpPi6G6B/f+67ust0nYdEg0EpFZC8fBfzXsknY80qBh3+oFN2ONtawlmsvPE7NJw9DuLjmXlWX3KnZkW7aDFBg0EpFZD5SwupO7CPCzZ8QHq0CxOgk0ARsDDn/1A5/aJmK7YWFFVErVyxxrFgEJFhwN+xCx4aYIEx5vEW51wKvAbs9R5abIz5pVNlUkqFrvjwcSYWfELvU5s61ZyFI8BrfWaz4/pb7C5qLZyqjfBuczHMyRpDPXCPMWa9iPQE1onIO8aYrS3OW2GMud7BciilwqjXge2cs/UDRka7IH46CmxiIO9c9V1qJ071b+Ofbs6xYDDGHMLuuYEx5qSIbAOygJbBoJTqBPLyXdz3zIdc+NlHpHOo2bLYwXIDqWG4TmuOAxXAi+O+zbFLLrV7Jii/RKSPQURGAFOBT1t5+kIR2QAUA/caY7a0cY25wFyA7Oxsh0qqlGpNXr6LeQsLyNi8jkzXMiaEeL0i4K8Dv8jJW26B8nKGv/AL7uAwfcNQVg9wEPg4bSYF19+KGTGyW85eDoXjwSAiPYBFwN3GmBMtnl4PDDfGVIrIddgZ6WNau44xZgGwACAnJ8c4V2KlVEvzlxZSe7CIaRs/DOkv/DpgAWex7877YKh3i52ePdn/kz/zcEE+n1v2cy4n+FrEVmBV6kz2X30d1cPHQkp7e8WptjgaDCKSiA2FfxpjFrd8vmlQGGPeEpGnRKS/MabMyXIppQJTXFLO2ILVZJSvC3p46pvAR1c9hGfKeWdOIhPBTD2P5WNf4bNNn3Lx8kWMZx9ndhGfqRJYDnw668dUnz8z6DCI00rFaU6OShLgWWCbMeb3bZwzGCgxxhgRmYadiX3UqTIppYIzsGQPEze9T0YQrz0MPDnyq1TfeGPHX9ppaVRNv4ylEy5g5e4NnPXJZ4yoWEUS7hYnxlHKRHaeO5VD587AMyQr5BnLyQk649nHyRrDTOArwCYRKfAe+39ANoAx5k/ALcD3RKQe2w91mzFGm4mUiiUnT3L2J8tJpyjg9ZCKgSdn3UPtzMsCe2GPHlROnsmGyTPZwI8DfNfgVNd5IvI+nYGTo5I+BtqtnBljngCecKoMSqkQeTzk/WkxWfveZGIQL38/bQqJMy+htpXnBDvBKVZkZjg1Pqrz0bqTUqptJSV8/PtnguoM3g1suunbJKamkJrYfNez1MR47pyRTVZGKgJkZaTSJ63DHZcdI6DrJDWhS2IopVpXW0veI8+SdDjwDXhqgUWjbsIzNJsKdx2P3jqF+UsLKS53k5mRyn1Xjz1jXaK8fBfzFm/CXRfZGcgC3DkjW9dJakKDQaloMQYOHYJ33oHPPoPUVDj/fLjiChg4MKpFy8t38fiTrzL1r88yIIjXFwClV94CcXFkZqSSOzWrwy9e3/PzlxbiKm/Z2eyMeBF+96XJGgotaDAoFQ0eD3z4IVx+eevP/+Y38L3vQZ/I74uWl+9i3j9WM3n5O/T07GN0gK8/Drwx827o04fUxPiAmmiaBkhevou7Xy5o9TwRm6uh8hijodAK7WNQKtI8Hnj11bZDAeBnP4O+feGNN6CyMnJlA+Yv2U7ClvUM3/1GcB3ODKI6ZxZZGan89xcmBf3Fmzs1iy/PyD5jBEtqYjx3Ts8Oy7wD7XBundYYlIokjwf++U/46lebHc7Dzgr2SQRyAW68ESZOhD//GaZOjciWk4d2F3FF/kekBTFm6CDw0pb37Z7JYfDr3EnkDO/bav9EzvC+zFu8EXeQw0wDrc10JxoMSkWKxwMLFtgmImBhO6fWNX1+82aOXnghg6++mdwnfgujRjm3/WRdHdOKCuhfGniHM8DIOd+Bs88Oa5Ga9j0Ul7uZv7Tw9PHcqVlM/eUyjlfVtXcJABLjhB4pCZRX1bXZAa4s6YzzyXJycszatWujXQyl/OfxwGOPwT33tBsI/pjzu9/Bl7/sTAf1zp08duEtcHQjgX5l7gT+X0lJ2MvV2mil1MT4081U/oxm6pOWyIM3TOj2QSAi64wxOR2dp30MSjnN44Hf/IaFYQgFgIX33AMjRsBLL0FF+HYdy/twG7df8x9UBBEKdcD4eY86Elbzlxae8aXvrmtoVnP47y9MOj0nIiM1kT5piafnRzx26xTyfz6724dCILQpSSkneTxw//0snD8/rJdd6HYz5/bb4dJLbUf1RRdBWlrQ18v7ZDe/+49HuXLPv4NqQvoQePpn3wn6/dtT3MbQ1eJyN3n5rmb9D4/eOkUDIAy0xqCUUzwe+NGPwh4KPguBhcuXw1VXwbe+BevWQW1ri090oKGBpx9+jtnr/8zwIMpxDCi8+TeQ7swO0G2NHOqdmsi8xZtwlbsxgKvczbzFm8jLdzlSju5Eg0EpJzQ0wFe+wsKnnnL8rRYCC196CXJy4IEHYN++wAb5b9jARa8/TH8gmK/2t+PHcff9dwTxSv/cd/XYM5bUEKDcXdduE5MKngaDin2VlZCXZ5tNROwWjbfeCqtXQ13Ho1Eirr4err2WhS+8ENG3XQjkzZ8PkyfDU09BmR/bmhQWwt1304s6+gfxnkXA1xc9S+60EUG82j9N+xCg48X32mp6Uv7TUUkqdh0/Dv/1X/DII8CZY/0BEs+7ktzFz8DwYBpBHFBfDzNmsHDduqgV4fQciJwc+OlPYfZs6NXrzBP37iXvC99gf8GHfm2I05o5jz8OP/pRxLbOnPnw+x0ul5GVkcrK+9uZPNiN+TsqSTufVWwxBnbuJO+yWykqLmBwB6fXrX+XR0aMYNmkb/H9p35C7qzwjqEPSH09nH02C/fuDfilc1o8DmX00unwXLsW5syBa6+Fn/yEvKShzP/oAK5yN/EuF9fk/YExlZsDHoHkk0gCfOUrEd1PuaPagE5aCw9tSlKxY/9+uP568saOpc6PUPAZDnxn07O8cfEU8v72lu30jbT6eujXL+BQmEOTULjrrmbHQ9E0WPLefpunL7uMly7+EiWr14DLxcXLngspFKqA3Ddfi/haTu0tYRHqEhyqkTYlqdiwZQt/nTiRHiFe5hCQ/c3/S+5v/xP69QtHyTpWXw+JiQH/ld/sy3/fPtsctmcP3Hmn7T+h9eazUJRgJ6KNAIaGcJ2VZPCY+1DQ+ysHq6PJbqp9OsGtC8rLdzHz4fcZef+bzHz4/S4zLO+BPy7jx2EIBYAhQN1fHucPo2bAxo3O1x68oRCoZqHgcjX2kYwaBe++C4sWAbavINTaQ1ODgFmEFgplwKV/eyHioQBnTmbTWoIzHK8xiMg1wONAPPCMMebhFs8nA38HzgeOArcaY/a1d83uWGPoqn8p3fn7ZbjvuZrbHbh2DfDlV16xbew9whE7LTQJhUBqC82+6NtbQqKoCH79a7u+UoDv4aTEa+4k99/PQXx8xyermBITNQYRiQeeBK4FxgO3i0jLiZXfAo4bY0YDjwL/42SZOquOlgXojB54dSOb7/mGI6EAkAws/NKX7Nj+gwfDe/FwhMKxY+0vITFsGDz+OLz3HowYwRzsiKNomgPk/vURDYUuzummpGnALmPMHmNMLfAScFOLc24CnvPe/xdwhUgEhzl0Eu0tC9AZ5eW7eOEXf+Z+ih1/r4WPPw633QYbNoSnaSkcoXDypH8dtykpdt+GVavgV7+yw1CjZA7AX/8Kg/0dFqA6K6eDIQs7B8bnoPdYq+cYY+qBCuCMXkMRmSsia0VkbWlpqUPFjV1tjcborBuNzH/i33xjwxMRGy+dt3IlTJkCixeHtvFNqKGQkABud+BNW0OGwE9+AqtWMWfWrMBeGwaJABMmwBe/GPH3VpHXaTqfjTELjDE5xpicAQOC2YW2c2ttWYBOO2b76FHG/+X7nBPBtzy9v8GcOfDgg8E1LVVXBxUKp5t/hg6FqqrgO22TkuDCCyEvjzlPPhnRZqVcgOeeg549I/iuKlqcDgYXMKzJ46HeY62eIyIJQG9sJ7RqosuMxqipIe+HDzGbKMw1wPuF/vvf26al9evtmkb+OHECUoOrneUCnHeeHZIaxAimM/TrB9/9LrkbNjDntts4EPoVO/bQQ3D++ZF4JxUDHB2V5P2i3wFcgQ2ANcAdxpgtTc75ATDJGHOXiNwGfMEY86X2rtsdRyV1CcaQ9/vnOXzvV89sK4yw0007L74In/98+38JHz0K/RtXEgq4CWn2bFiyxJkZwhUV5P3qaTb+7qf0xQ5HDbfEAeeQu/szrS10ATExKsnbZ/BDYCmwDXjFGLNFRH4pIjd6T3sW6Cciu4D/AO53skwqig4fJv/e/xP1UAA7cQyA22+H//zPtpuWDh0KLRRuuw2WLnVu2Yjevcmdfx/nvvwBa/rmsC/Ml08EclfkaSh0MzrzWUVGTQ088QQv3Htv1IdcNnW65jBlCjz7rF2Z1DcUc98+GDny9LkBh8IPfgBPPBGOYvrn1CnyHvk7Gx76Kb05GfRyFz5zAJYvh899LvSyqZgQEzUGpcAOTb187pM8cO9PQg6F5cDj1/6cxEtvDsu4/tNf9gUFtg39lVfsUNKtW0+HQh5BhMIDD0Q2FADS08l98HtMfnUJqwZeyB7gZJCXSgT45z81FLoprTEoR+Xlu5j37EdMf/FJZh1bSe8gr3MMGPS1H5L7yIONTTt798J3vmMngBH6zODWlp4Iav2j+fPh3ntDLE2I3G7y/vAyG346j14cDmgJjEQg91e/gnnzdCJbF+NvjUGDQTlq5q+X4Xnz31yz+g9B7SUMdtG38Y+9QO73bzlzVI/HY5eXvuQSO+KJ0Bedm0PwITPn6adh7twQSxBGxcW2iezRR+H48TP++5zeu6Gpr38dnnwypD2kVWzSYFAxYcytj3HDK//L2ewNqtM5cexMct9/BTI72Erm1Cl4/vnTS1dHel2hk8A3n3/erowaa2pqbHj+5jfw9tvtn3vZZXYBvwgvp60iQ/sYVPRVVTF716f0DjYUhk4kt+DdjkMB7Eb03/0uHDgAffqEdUXSjriAby5eHJuhAJCcDDNnwj/+YWsCI0e2Pkrqjjvs8F0NhW5Pd3BTztm4keEbVwS1xPMcgO2rA58lPGwYHD4MixYx5447HK857AF+umQJXH21w+8UBt6JcVx+Obz0EmzaZAN10CCYNMmuQtsNVxVQZ9KmJOWIvOVb+eAbP6bPvmVMCPC1VcDX1q+HqVNDK8SxYzB5MnkHD4Z1sxufMuB7H35o+zc6m+pqKC+3P1NSICMjKvsrqMjSPZ+7uLx8Fw+9voVyt/3K65OWyIM3TCB3ahZ5+S7mLy2kuNxNZkYq9109NqJLZ+StK+KR/3qOmfs+DTgUAL72wAOhhwJA375QVERuXh7cfHNYaw9zAD75BGbMCONVIyglRVdJVW3SGkMnlJfv4r6FG6jzNP/dxQnEC9S1WIYo0hv6XHzPi4z7x1Nkln4ccDDMAbuCabiHSZaU2CaUrVvDM6x140bb/KJUJ6Kdz13Y/KWFZ4QCgMecGQoQ4Q196uth1cf0LV0bcChsBaiocGbs/KBB9st8xYqgJ8bN8d44eFBDQXVp2pTUSTRtHgqmjhexDX1cLiZvXklPqgN+6eSH/wK9ejlQKK/4eJg1C+rryX39dfjCF4D2h7Y2G92UlARlZbpukOryNBg6gdb2ew6U0xv6PJC3iRdX7Gb0+69xfuUGzg7w9c8CS37ydQdK1or4eLj5Ztvxunw5c669FjpqUn3oIfjpT7WDVnULGgydQGv7PQfKyQ19HsjbxPOrD5B0cA/jCj5gcoCvrwZyfrvIuRVI25KcbIeZlpXBypV2Ytf27XZF1V69YPx4O+Hrqqtg+HC7+5pS3YD+nx7j8vJduMLQDOTrY2ivAzqY0Uy+UKC6mgkFn9CH/QF3XK0HHr3vCwG+Koz69oUbbrA3pZQGQyzzNSGFg6vczY9fLuDulwuIF6HBGLK8X/4A8xZvxN2k59pV7j793m2Fw+lQAFL2FzJ6x9sBr4fkAh4tKQn48yilnKPBEMPC0YTUlK8VvcHbnu4qd7c67NXHXdfA3S8XMH9p4Rm1h7x81+lQoKqKqZ+tYAynAi7T3XfdBQMHBvw6pZRzNBhiWCRGErUVCk35ag9r9x/jg+2luMrdNO0NSNu9mTHFSwh0hZ1tYJeoVkrFFJ3HEMOcHkkUCHddA8+vPnC6v+N0nJw4wQUfLWViENd8c+QXoUePcBVRKRUmjgSDiMwXke0islFEXhWRjDbO2ycim0SkQES671TmNtx39VgS4yM8UicQHg99t61nVOWnBFrKVcC8F37rRKmUUiFyqsbwDjDRGHMusAOY1865lxljpvgzTbs78a2FVNcQu0uWxJUd4YLlrwS1Ac/n/uev5M4YFfYyKaVC50gwGGOWGWPqvQ9XQ1ArL3dbefku7vvXhtML5MWk2lpGfLqcqRwI+KWvArn3fjX8ZVJKhUUkOp+/CbzcxnMGWCYiBnjaGLOgrYuIyFxgLkB2dnbYCxlLfvFGEDWF8nIyCpYz5dPFjOQY/YF4oAK7jHUpfXBlTmf3uedTedakkNv2k/ZsZ+bW5wPegKcU+NJL70Ocdm8pFauCDgYReRdobd3enxljXvOe8zOgHvhnG5eZZYxxichA4B0R2W6M+ai1E72hsQDs6qrBlrszOF4VQE2hpIRBbzzH9Uc/Yiyc0dbf+MV9HIqXcLR4CSXA1rgZFF4yk/KxU+1a/IEoL2fGa3/m3MBeBUAhaXz/S5cG8UqlVKQEHQzGmCvbe15Evg5cD1xh2ljb2xjj8v48IiKvAtOAVoNBNVFTQ/KBnWQvfppr2cuwAF7az3sb71kNy1ezazlspi+FM26nbMIF0L9/+xc4cYJRzz7KDewNuNiFwONlByK/9IVSKiCONCWJyDXAT4DPGWOq2jgnHYgzxpz03p8N/NKJ8nQ2GamJrfcvnDhBrw0rmL7qz5xLfavVtUCNBkZzDFY/CavhKHAA2MEF7LjiMk6cPcWuG1RfT8LObcx8/ddcH8RENoDnBuXyQL9gdn9WSkWSU30MTwDJ2OYhgNXGmLtEJBN4xhhzHTAIeNX7fALwgjFmiUPl6VSunzykcVYxQFkZ/T9YxA173iAL6O3ge/tqFFNZA++toe49OIHtr8gI4bovAwN/+L0wlFAp5TRHgsEYM7qN48XAdd77eyDghTi7vLx8F4vWuexf6K79ZC16juvq1pNNdKapJ0LAHcwtlQNrP/8wj30+mI0+lVKRpktixJj5b2+j5uhRBr62kM8XvxbUHIFY8zgjYOLEiO47rZQKngZDrGhogBMnqF69kilvvcJNDVtJi3aZwmAFUPmDh8mKoeU9lFLt02CItro6OHoU1q6FN9/k5jf+wmBqu0Qo1ANvXngP0qOHoxsFKaXCS4Mhkjweu51keTkcPgy7d9tAWL8eCgqgrIyJQAzPdw7I7+iHZ+Yl3DkjW5uRlOpEuk8wNDTAqVNw/DhUVNgvaI93YxoRezOmce/fQI/5JCRAejqkptrrnzoFVVW2ZnDyJJSWwv79sHEjrFtnw6GJXNrfnL6zeAko+94feOyO8zUUlOpkukcwNDTYQCgttV/OR4/CkSNQW9v4JW9M8y/5QI6B3WA+Ls6O+a+uhspKe05Cgn2v4mIbCLt3w44dUF9/Zjm7iCXAuq8sgF69NBSU6oS6RzDU1kJNjb1vjP0CT021x3xf9g0N9kvc42msFfh7DOxxEdi1y4bQqVM2gMrKbG1hxw67yXwXtwhY/eWnITMz2kVRSgWpewRDQ8OZX+QJCZCYaJ8D+1x8fPPXtXfMGBssJ07Y2sHJk7bvoKTENh35+hC6iSpgAWfhmvsQ9LF7uelIJKU6p+4RDL5mHl8zENimnLq65jUGaDyn5TFff0F5eWMH8rFj9ueJE+By2VDohrYB/xj/Lequud6GLZCaGK8jkZTqpLpHMCQlQXKyvS9iv+Td7uadxr6agW856Pp6+4V/8qStEfg6rSsqbADs29fYPNVNNWA7mQtufQJGjDh9PCsjlfuuHqv9C0p1Ut0jGOLjbfNGUpL9gk9JgX79mo9Kqq+3f/2XlTXvqC4rgwMHbB9BaWlUP0Ys+RB4d8aPqJ7+Ofvf0ysjNZGV918evYIppULWPYIBbE0gPR3S0mzHaE2N/eu/vNze9u+3I4cOH4ZNm2wQlJfb/oLa2igXPjacwK6J/mnOj6ie2TwQfCpiedc5pZRfukcwGGObjnwTy/bvt01BZWX255YtcPCgbTLqQs1DR4F1pJA/9auUnXdR23stnDhBr8I1jH3/bSaynVTADexgDIUzZvu3T4NXpnY4K9XpdY9gqKmxAbB6NaxaZW/btkW7VI45Cuwmidcvu5eayec39q+0pVcvTlxwBWsuuII1Ibyvdjgr1TV0j2A4ftwuPfHuu7B1K+zcGe0SOaIUu5zGv4bfRtEVV8OAARF77z5piTx4wwTtcFaqC+gewXD0KGzfbmcgu1xdbtbxEexGOhs4h/euu5P6cZPsPI0ISktK0FBQqovoHsFQXQ1FRbBmjR2C2kVUYieWVQMvjvs2ZZdcChkZUSlLcbk7Ku+rlAq/7hEM9fW2KakLhUIxdu/U1ck5rLz2VjxjxjbOwYgC7XRWqutw7JtERB4SEZeIFHhv17Vx3jUiUigiu0TkfkcKc+iQbUrqAo4AZUAVCTxx3g9Z8e178YwdF9FQkBaPtdNZqa7F6RrDo8aYR9p6UkTigSeBq4CDwBoRed0YszWspSgsDOvloqEEMNhf2LKMS8m/Zg4MHx709QS4c0Y2i9a5cNc1BPRag53dXFzuJlNnOSvV5US7KWkasMsYswdARF4CbgLCGwxlZWG9XKRUYNMyBdu5XEEGi2Z+G/f50+3qsEHyhcKvcyeRM7wv97yygQbf4oJ+yMpI1dnNSnVhTgfDD0Xkq8Ba4B5jzPEWz2cBRU0eHwSmt3YhEZkLzAXIzs4OrBSnTgV2fhTlATuBIkZxin4MoIrSlCR2jBjHzmmXwZAh7b5esH/RtxQvgseYM/7Cz52axY9fLvC7fNpspFTXF1IwiMi7wOBWnvoZ8EfgV9jvqV8BvwO+Gex7GWMWAAsAcnJy/P/zFjrVbObPBk5iW/ZkqpN7EZ9QzZ70TI4MHkZt7wF2rad29ElL5PPnDjmjeSg1MZ7//sKkNpt7MjNScbUyqihehNunD+OD7aXabKRUNxJSMBhjrvTnPBH5M/DvVp5yAcOaPB7qPRZe0rK7NAbl5sLFF7PhtYPUJiVQ0yOVwwOGU9urn19zEr7sbRoCyBnel/lLC/3+Mr/v6rHMW7wpoDBRSnVdjjUlicgQY4xvy7Kbgc2tnLYGGCMiI7GBcBtwR9gLE6t9DIMGwec+B1OmQHY2pKbSp38Fm1P7UNE3E5Oa5leoZaQmng4FsM1DgXyh+84NJEyUUl2Xk30MvxWRKdimpH3AdwFEJBN4xhhznTGmXkR+CCzF9q/+xRizJewlqa4O+yVDMmoUXHklnH22Xek1Odku9JeZyS3fH8Ony/ZhxL/hp6mJ8Tx044SQixRomCilui7HgsEY85U2jhcD1zV5/BbwllPlAGKn83naNJg504ZB//7Qs6c9npEBo0fD4MHkJiXxi5WHOF7l3/LV2tyjlAq3aA9XjYxo1xhmz4YLLoCBA20IpKXZGkKPHnYuQlaW3SvCO0mt3M9QyMpI1VBQSoVd9wiGaEhJgcsvh8mT7Re/bwe5ujro3RtGjoTBg21ItJi13NYooaYEcJW7mfnw+9ofoJQKq+4RDN4N6iNi6FCYPh3GjbOB0Lev/eJvaLDLYJ91lm1GSk5us2P5vqvHcncHcwt843Vd5W7mLd4EoOGglAqL7hEM6enOv8fkyTB1qu1YHjLE1go8HhtKw4bBiBG2GcmPoae5U7M6DIam3HUNzF9aqMGglAqL7hEMfm5LGZQZM+xw0zFjbB9CSopdzbVHDxsIQ4faTub4eOfKgC57rZQKn+4RDD16hPd6PXvC+efDhAk2EPr1s4EAtrkoO9uGRGpq0Kuexgl4ApjfrcteK6XCpXsEQ1paeK4zeLCtHUyYYPsKBgywzUVxcbZ2MHx4YydziLOtAwkFXb9IKRVO3SMYQliJFLAdyaNH20DIzrZNUx6PDZzMTBsIvXqFrbkoL9/V5mJ4TQnoLGWlVNh1j2AYNqzjc1pKTLRBMHo0jB9vh5emp9vRRf362WsOGmSPhbn/YP7Swg5DQZe+Vko5pXsEw/DhcM45/u3i1r+/bSYaM6bxlpBgm4sGD7ajjvr2DUtzUVs66kjWpiOllJO6RzAMGwY5OXDkCBw71vo52dl2SOno0fY2YoT94u/d2wZCdra978dw01BlpCW2uSRGljYdKaUc1j2CISPDrlFUUwObN0NpKbjdtrkoM9M2CY0aBRMn2jkIHo9tLhoxwoaCA81F7WlrM7XUxDhtPlJKOa57BENamm0SamiwtYHSUigpsY+HDYNJk2wQJCban6NHdzg72UkV7tZrC+46D3n5Lq0tKKUc1T2CIT3d1gzi4+3ooT59bFD072/nH6Sk2IloI0bY5yK5hEYr2lsrSWc4K6Wc1j2CISXF1gTq620/wbhxcPy4bTLKyorY7GR/tbdWks5wVko5rXsEQ3x84+zkigqorbU1iN69Q5qd7JTcqVn84o0trXZA6wxnpZTTYusb0Um+ZqRhw+xw1MzMZnsgxJoHb5hAamLzGowOU1VKRUL3qDF0QroPs1IqWhwJBhF5GfD9aZsBlBtjprRy3j7gJNAA1BtjcpwoT2el+zArpaLBkWAwxtzquy8ivwMq2jn9MmNMmRPlUEopFThHm5JERIAvATorSymlOgmne14vBkqMMTvbeN4Ay0RknYjMdbgsSiml/BB0jUFE3gUGt/LUz4wxr3nv3w682M5lZhljXCIyEHhHRLYbYz5q4/3mAnMBsrOzgy22UkqpDohpa2GeUC8skgC4gPONMQf9OP8hoNIY80hH5+bk5Ji1a9eGXkillOpGRGSdP4N8nGxKuhLY3lYoiEi6iPT03QdmA5sdLI9SSik/OBkMt9GiGUlEMkXkLe/DQcDHIrIB+Ax40xizxMHyKKWU8oNjo5KMMV9v5VgxcJ33/h5gslPvr5RSKjixuR6EUkqpqNFgUEop1YwGg1JKqWY0GJRSSjWjwaCUUqoZDQallFLNaDAopZRqRoNBKaVUMxoMSimlmtFgUEop1YwGg1JKqWY0GJRSSjWjwaCUUqoZDQallFLNaDAopZRqRoNBKaVUMxoMSimlmtFgUEop1YwGg1JKqWZCCgYRmSMiW0TEIyI5LZ6bJyK7RKRQRK5u4/UjReRT73kvi0hSKOVRSikVulBrDJuBLwAfNT0oIuOB24AJwDXAUyIS38rr/wd41BgzGjgOfCvE8iillApRSMFgjNlmjCls5ambgJeMMTXGmL3ALmBa0xNERIDLgX95Dz0H5IZSHqWUUqFLcOi6WcDqJo8Peo811Q8oN8bUt3POaSIyF5jrfVgpIk0DqT9QFlKJY1NX/VzQdT9bV/1c0HU/W3f6XMP9eWGHwSAi7wKDW3nqZ8aY1/x5k3AwxiwAFrT2nIisNcbktPZcZ9ZVPxd03c/WVT8XdN3Ppp/rTB0GgzHmyiCu6wKGNXk81HusqaNAhogkeGsNrZ2jlFIqwpwarvo6cJuIJIvISGAM8FnTE4wxBvgAuMV76GtAxGogSimlWhfqcNWbReQgcCHwpogsBTDGbAFeAbYCS4AfGGMavK95S0QyvZf4KfAfIrIL2+fwbJBFabWJqQvoqp8Luu5n66qfC7ruZ9PP1YLYP9yVUkopS2c+K6WUakaDQSmlVDOdMhjaW4qjxXn7RGSTiBSIyNpIljEYAXyua7xLjewSkfsjWcZgiUhfEXlHRHZ6f/Zp47wG7++rQERej3Q5/dXR78A78OJl7/OfisiIKBQzYH58rq+LSGmT39G3o1HOQInIX0TkiIhsbuN5EZH/9X7ujSJyXqTLGCw/PtulIlLR5Hf28w4vaozpdDdgHDAWWA7ktHPePqB/tMsbzs8FxAO7gVFAErABGB/tsvvx2X4L3O+9fz/wP22cVxntsvrxWTr8HQDfB/7kvX8b8HK0yx2mz/V14IlolzWIz3YJcB6wuY3nrwPeBgSYAXwa7TKH8bNdCvw7kGt2yhqDaXspjk7Nz881DdhljNljjKkFXsIuQRLrbsIuewKdf/kTf34HTT/vv4ArvMvAxLLO+v9Wh4wxHwHH2jnlJuDvxlqNnWM1JDKlC40fny1gnTIYAmCAZSKyzrukRleQBRQ1edzuUiIxZJAx5pD3/mFgUBvnpYjIWhFZLSK5kSlawPz5HZw+x9gJnBXYIdmxzN//t77obW75l4gMa+X5zqiz/rvy14UiskFE3haRCR2d7NRaSSEL01Ics4wxLhEZCLwjItu96Ro1sbLEiBPa+2xNHxhjjIi0NU56uPd3Ngp4X0Q2GWN2h7usKmhvAC8aY2pE5LvYWtHlUS6Tat967L+rShG5DsjDTjpuU8wGgwluKY6W13B5fx4RkVexVeWoBkMYPpc/y41ERXufTURKRGSIMeaQt4p+pI1r+H5ne0RkOTAV2+4dS/z5HfjOOSgiCUBv7DIwsazDz2WMafoZnsH2HXUFMfvvKlTGmBNN7r8lIk+JSH9jTJsLB3bZpiQRSReRnr77wGzs/hGd3RpgjNhNjpKwHZsxO3qnidexy55AG8ufiEgfEUn23u8PzMTOno81/vwOmn7eW4D3jbcnMIZ1+LlatLvfCGyLYPmc9DrwVe/opBlARZOmz05NRAb7+rdEZBr2e7/9P1Ki3aMeZC/8zdg2wBqgBFjqPZ4JvOW9Pwo7qmIDsAXbVBP1sof6ubyPrwN2YP+SjvnP5S1zP+A9YCfwLtDXezwHeMZ7/yJgk/d3tgn4VrTL3c7nOeN3APwSuNF7PwVYiN2L5DNgVLTLHKbP9d/ef08bsGudnRPtMvv5uV4EDgF13n9j3wLuAu7yPi/Ak97PvYl2RjvG2s2Pz/bDJr+z1cBFHV1Tl8RQSinVTJdtSlJKKRUcDQallFLNaDAopZRqRoNBKaVUMxoMSimlmtFgUEop1YwGg1JKqWb+Px6teITTq9f6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_test_curve(validation_loader.dataset.dataset.X,\n",
    "               validation_loader.dataset.dataset.y,\n",
    "               h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = h(validation_loader.dataset.dataset.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5000, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_OP = 8\n",
    "\n",
    "sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.randint(0, Y_pred.shape[1], (BATCH_SIZE_OP,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "qy_00 = Y_pred[sample, batch, 0]\n",
    "qy_01 = Y_pred[sample, batch, 1]\n",
    "qy_10 = Y_pred[sample, batch, 1]\n",
    "qy_11 = Y_pred[sample, batch, 2]\n",
    "\n",
    "cy_0 = Y_pred[sample, batch, 0]\n",
    "cy_1 = Y_pred[sample, batch, 2]\n",
    "\n",
    "Qy = torch.zeros((BATCH_SIZE_OP, 2, 2))\n",
    "cy = torch.zeros((BATCH_SIZE_OP, 2))\n",
    "\n",
    "Qy[:,0,0] = qy_00\n",
    "Qy[:,0,1] = qy_01\n",
    "Qy[:,1,0] = qy_10\n",
    "Qy[:,1,1] = qy_11\n",
    "\n",
    "cy[:,0] = cy_0\n",
    "cy[:,1] = cy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.5101, 11.8222],\n",
       "        [ 9.3594, 10.2748],\n",
       "        [ 8.5177, 14.6552],\n",
       "        [ 8.1419, 13.0726],\n",
       "        [ 7.9186, 10.3805],\n",
       "        [ 7.7878,  9.2600],\n",
       "        [12.7521,  9.3533],\n",
       "        [11.0066, 10.7209]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qy[,,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((BATCH_SIZE_OP, 2, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic Programming (Q, c, A, b):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
